{
  
    
        "post0": {
            "title": "Data Classes Pytorch And Fast_ai",
            "content": "Pytorch Dataset - A collection of tuples of X(independent features) and Y(dependent or target) for each item. Say there are 100 such tuples. . Pytorch DataLoader – Pass the above dataset and a batch size to Dataloader. Dataloader returns an iterator of multiple batches of tuples of X and Y. . For the above dataset and batch size =16, DataLoader will return 7 batches of 16 tuples each in the first 6 batches and remaining 4 tuples in the 7th batch. (16*6=96 and remaining 4) . FastAI Datasets – A training set collection and a validation set collection of tuples of X(independent features) and Y(dependent or target) for each item. This is an iterator as well. . FastAI DataLoaders – This is a FastAI class which can take multiple Pytorch Dataloaders as input. Generally a training pytorch dataloader and validation pytorch dataloader are passed as inputs to this class. . Fast AI DataBlock – This API is used to create the FastAI Dataloaders. Below is an example. . “dls” is the dataloaders class that we created using the Datablock API. . This is like a template/blueprint which we defined first. . | The bold text is where we create the FastAI Dataloaders using the blueprint defined and passing a dataframe as input. . | splitter_function is a function we are passing as input for splitting the dataframe into train and valid. . | get_x_function and get_y_function are functions we pass as inputs for getting the X and Y from the dataframe. . | Fast AI Docs has more details. . | show_batch() can be used to see if the Datablock that we defined is correct. For example, if we missed the Item Transformation of RandomResizeCrop and the input images are of different sizes, then the show_batch() is going to fail and provide a meaningful message. . | . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), . splitter=splitter_funtion, . get_x=get_x_function, . get_y=get_y_funtion, . item_tfms = RandomResizedCrop(128, min_scale=0.35)) . dls = dblock.dataloaders(df) . dls.show_batch(nrows=2, ncols=3) .",
            "url": "https://mrg-ai.github.io/blog/2020/10/10/Data-Classes-Pytorch-and-Fast_AI.html",
            "relUrl": "/2020/10/10/Data-Classes-Pytorch-and-Fast_AI.html",
            "date": " • Oct 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Machine Learning End To End Flow Summary",
            "content": "The blog assumes the reader knows some basic Machine Learning terms. . This Summary post should be read along with the below blog post . https://mrg-ai.github.io/blog/2020/08/08/ML-End-To-End-Flow-CAHousingDataset.html . Get the data . This could be via files or from a database. Try to get the data into a pandas dataframe. . | That is the best format to do further data exploration. . | Use head(), info(), describe() methods to look at the data and column/column type information . | Using matplotlib visualize the data via histograms for numerical columns . | For Categorical columns, find the unique values using value_counts() . | . Split the data into Training and Test . Splitting the data can be random using a random seed using sklearns test_train_split class . | Get the labels into a different dataframe for using during prediction. . | However, in reality we should split using some unique column in the data (if available) . | We can create such column if that is possible by using some combination of existing columns. . | Sometimes, we may also have to do a stratified data split i.e. data should be taken from the different “strata” of the data. For example – Low Income Grp, High Income Grp etc. . StratifiedShuffleSplit class can be used along with some column which indicates the “strata” | . | . Visualize and Data Cleanup of the training data . Use matplotlib and look at the training data in more detail. . | Create new attributes (new dataframe columns) from existing columns if possible . | Date columns can definitely be used to create new Day, Month, Quarter, Year etc (DatePart function) . | Clean the data for NULLs, Blanks etc through any of the below methods. Each method has its own implications and should be considered appropriately. . Drop such rows . | Use sklearn SimpleImputer to impute such NULLs with Median or Mean values . | . | NOTE – SimpleImputer works only on Numerical columns and CategoricalEncoders work on Categorical columns. We generally create different dataframes for Numericals and Categoricals. There is another Class which can handle both together and we will look at it in next section. . | Handle the Categorical values i.e. convert them to Numbers . Ordinal Encoder Class for Ordinal (they have a relationship between them) Categories like Low, Medium, High . | One Hot Encoder Class for categories which are Nominal (unrelated to one another) like List of State Names (CA, AZ, NY etc.) . | We can also create Custom Transformers to do some custom transformations. An example below. Using BaseEstimator and TransformerMixin classes we can get many existing sklearn methods in the Custom Transformer. . | . | . from sklearn.base import BaseEstimator, TransformerMixin . rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 . class CombinedAttributesAdder(BaseEstimator, TransformerMixin): . def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs . self.add_bedrooms_per_room = add_bedrooms_per_room . def fit(self, X, y=None): . return self # nothing else to do . def transform(self, X): . rooms_per_household = X[:, rooms_ix] / X[:, households_ix] . population_per_household = X[:, population_ix] / X[:, households_ix] . if self.add_bedrooms_per_room: . bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] . return np.c_[X, rooms_per_household, population_per_household, . bedrooms_per_room] . else: . return np.c_[X, rooms_per_household, population_per_household] . attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) . housing_extra_attribs = attr_adder.transform(housing.values) . Feature Scaling . This is required because ML algorithms require the different numerical features to be on similar scales. . | Therefore, it is always a good practice to Scale the Numerical features. . | Min-Max Scaling (Normalization) can be achieved with MinMaxScaler Class. This will cause issues when the data has many outliers. . | Standardization can be achieved through StandardScaler Class. Less affected by outliers. . | . Pipelines . Since there are multiple steps we do as part of Data preprocessing, we should create a pipeline to do these transformations one after the another. . | The output of one becomes input to another and so on. . | A pipeline for numerical attributes can look like below . | . num_pipeline = Pipeline([ . (‘imputer’, SimpleImputer(strategy=“median”)), . (‘attribs_adder’, CombinedAttributesAdder()), . (‘std_scaler’, StandardScaler()), . ]) . We can get the different transformations done for numerical columns as below using the pipeline defined above . Dataframe_Transformed = num_pipeline.fit_transform(Dataframe[numerical attribute list]) | . | We can create a Full Pipeline for all attributes at once also. This would be ideal instead of having separate pipelines for Numericals and Categoricals. . | Below is such an example. We can use ColumnTransformer Class. Remainder keyword is to tell that any columns not covered in the num pipeline or cat pipeline should be passed through. . | . from sklearn.compose import ColumnTransformer . num_attribs = list(dataframe_num) . cat_attribs = list(dataframe_cat) . num_attribs . [‘List of Numeric Attributes’] . cat_attribs . [‘List of Categorical Attributes’] . full_pipeline = ColumnTransformer([ . (“num”, num_pipeline, num_attribs), . (“cat”, OneHotEncoder(), cat_attribs), . ], remainder=‘passthrough’) . Dataframe_AllCols_Prepared = full_pipeline.fit_transform(Dataframe_AllCols) . | The data is now ready to be trained. . | . Evaluate Different Models . Evaluate the different models . | Calculate the cross validations score using K Fold Cross Validation . | Pick the model which best suits the data. . | Save the picked model using joblib.dump(model, ‘&lt;model_name&gt;.pkl’) as a pkl file. . | This can be later reloaded back using joblib.load() . | . Finetune the Model . Finetune the models hyperparameters to finetune the model . | Using GridSearchCV or RandomizedSearchCV classes, we can get the best hyperparameters for the model. . | . Deploy the Model . The model can be deployed on Cloud or can be exposed through a REST API and the model’s predict function can be used to evaluate the output by giving the necessary inputs. . | Note that the test input or any new input has to be transformed using the same data preprocessing pipeline before the model can predict the output using that input. . | . X_test_prepared = full_pipeline.transform(X_test) . final_predictions = final_model.predict(X_test_prepared) .",
            "url": "https://mrg-ai.github.io/blog/2020/09/07/Machine-Learning-End-to-End-Flow-Summary.html",
            "relUrl": "/2020/09/07/Machine-Learning-End-to-End-Flow-Summary.html",
            "date": " • Sep 7, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "ML End to End Project Flow for CA Housing Data",
            "content": "Initial Data Exploration and Data Splits . Load the data from housings.csv file uploaded to Colab. The original dataset is from https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.tgz . import pandas as pd import numpy as np . housing = pd.read_csv(&#39;housing.csv&#39;) . head() function shows the first five rows . housing.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | . info() function shows description of the data, in particular the total number of rows, each attribute&#8217;s type, and the number of nonnull values. . Note that the total bedrooms has many NULL values. These have to be taken care of later. . housing.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 longitude 20640 non-null float64 1 latitude 20640 non-null float64 2 housing_median_age 20640 non-null float64 3 total_rooms 20640 non-null float64 4 total_bedrooms 20433 non-null float64 5 population 20640 non-null float64 6 households 20640 non-null float64 7 median_income 20640 non-null float64 8 median_house_value 20640 non-null float64 9 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB . lets find the unique values of the categorical column . housing[&quot;ocean_proximity&quot;].value_counts() . &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 . # lets find some information on numerical columns . housing.describe() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . count 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20433.000000 | 20640.000000 | 20640.000000 | 20640.000000 | 20640.000000 | . mean -119.569704 | 35.631861 | 28.639486 | 2635.763081 | 537.870553 | 1425.476744 | 499.539680 | 3.870671 | 206855.816909 | . std 2.003532 | 2.135952 | 12.585558 | 2181.615252 | 421.385070 | 1132.462122 | 382.329753 | 1.899822 | 115395.615874 | . min -124.350000 | 32.540000 | 1.000000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | 0.499900 | 14999.000000 | . 25% -121.800000 | 33.930000 | 18.000000 | 1447.750000 | 296.000000 | 787.000000 | 280.000000 | 2.563400 | 119600.000000 | . 50% -118.490000 | 34.260000 | 29.000000 | 2127.000000 | 435.000000 | 1166.000000 | 409.000000 | 3.534800 | 179700.000000 | . 75% -118.010000 | 37.710000 | 37.000000 | 3148.000000 | 647.000000 | 1725.000000 | 605.000000 | 4.743250 | 264725.000000 | . max -114.310000 | 41.950000 | 52.000000 | 39320.000000 | 6445.000000 | 35682.000000 | 6082.000000 | 15.000100 | 500001.000000 | . Make sure the mean, median, percentiles are all observed and if required any data modifications have to be done. . To vizualize the numerical columns we can use matplotlib and see the histograms . import matplotlib.pyplot as plt housing.hist(bins=50,figsize=(20,15)) plt.show() . Observe the different values and see if they make sense. If they don&#39;t make sense, then go back and check if its a data error or else if the data values are on a different scale. In the above case, look at the median income and it becomes evident it is probably in so many 100Ks i.e. 200K, 400K and so on.. . Some of the data columns seem to have been capped. Look at housing median value and housing median age. See the last bar. That indicates the values may have been capped. Anything above 52(for age) and 500000k (for value) is capped to those values. This may need to be fixed by either removing such data or collecting proper values without capping. . Scale of different columns are all over the place. We will need to bring them all to same scale. This is Feature Scaling.. . Also, the data distributions are not like a bell curve. It is &quot;tail heavy&quot; in many cases. If possible they will have to be transformed to make the data distribution to be like a bell curve. . Dont do any more data analysis, otherwise you run the risk of &quot;data snooping&quot; bias. Create a test set asap . Split the data into train and test . Lets define a function to split train and test . def split_train_test_random(data, test_ratio): np.random.seed(42) shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] . call the function above and split the data randomly. Since we used a seed, it will always split the same set of records even if we rerun the code. . random_train, random_test = split_train_test_random(housing, 0.2) . random_train.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 14196 -117.03 | 32.71 | 33.0 | 3126.0 | 627.0 | 2300.0 | 623.0 | 3.2596 | 103000.0 | NEAR OCEAN | . 8267 -118.16 | 33.77 | 49.0 | 3382.0 | 787.0 | 1314.0 | 756.0 | 3.8125 | 382100.0 | NEAR OCEAN | . 17445 -120.48 | 34.66 | 4.0 | 1897.0 | 331.0 | 915.0 | 336.0 | 4.1563 | 172600.0 | NEAR OCEAN | . 14265 -117.11 | 32.69 | 36.0 | 1421.0 | 367.0 | 1418.0 | 355.0 | 1.9425 | 93400.0 | NEAR OCEAN | . 2271 -119.80 | 36.78 | 43.0 | 2382.0 | 431.0 | 874.0 | 380.0 | 3.5542 | 96500.0 | INLAND | . random_test.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 20046 -119.01 | 36.06 | 25.0 | 1505.0 | NaN | 1392.0 | 359.0 | 1.6812 | 47700.0 | INLAND | . 3024 -119.46 | 35.14 | 30.0 | 2943.0 | NaN | 1565.0 | 584.0 | 2.5313 | 45800.0 | INLAND | . 15663 -122.44 | 37.80 | 52.0 | 3830.0 | NaN | 1310.0 | 963.0 | 3.4801 | 500001.0 | NEAR BAY | . 20484 -118.72 | 34.28 | 17.0 | 3051.0 | NaN | 1705.0 | 495.0 | 5.7376 | 218600.0 | &lt;1H OCEAN | . 9814 -121.93 | 36.62 | 34.0 | 2351.0 | NaN | 1063.0 | 428.0 | 3.7250 | 278000.0 | NEAR OCEAN | . The problem with above approach is that when/if the data is updated with new rows, then the split will change and we do not want that. . If possible, we need to find some unique identifier in the data and use that information to do the splits . Hashing is one such approach . from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32 def split_train_test_by_hash_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set] . The above functions are to create the ids and then split based on ids . However, in the current dataset, we do not have any such unique identifier . We can use row_id of the dataframe, but that is not a good approach since new data can possibly come in between the set of rows and not necessarily get appended always . A better approach is to use Latitude and Longitude in case of this dataset since that will remain unique for a larger span of time. . Below, I am creating a new dataframe and adding a new column and copying the data from housing df . housing_with_id = housing.assign(id=np.nan) . housing_with_id.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity id . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | NaN | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | NaN | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | NaN | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | NaN | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | NaN | . housing_with_id[&quot;id&quot;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] . housing_with_id.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity id . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | -122192.12 | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | -122182.14 | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | -122202.15 | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | -122212.15 | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | -122212.15 | . Note - There are other ways to add a column to dataframe. Example below. . housing_with_id_2 = housing . housing_with_id_2[&#39;id&#39;] = housing[&quot;longitude&quot;] * 1000 + housing[&quot;latitude&quot;] . housing_with_id_2.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity id . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | -122192.12 | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | -122182.14 | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | -122202.15 | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | -122212.15 | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | -122212.15 | . hash_train_set, hash_test_set = split_train_test_by_hash_id(housing_with_id, 0.2, &quot;id&quot;) . hash_train_set.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity id . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | -122192.12 | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | -122182.14 | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | -122202.15 | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | -122212.15 | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | -122212.15 | . Sklearn has libraries for splitting. Below is the most common one. . from sklearn.model_selection import train_test_split . train_set, test_set = train_test_split(data, test_size=0.2, random_state=42) . . All the above splits were done randomly. . But, many times, we have to pick test data from different examples in the data. . For example, in this dataset, we have to make sure that test set also contains low median income rows. We cannot be sure of it if the split is random. . In such cases, we create new columns which can create a Category column which creates such groups in the data. . pd.cut is useful for this. . https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html . housing[&quot;income_cat&quot;] = pd.cut(housing[&quot;median_income&quot;], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]) . housing.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity id income_cat . 0 -122.23 | 37.88 | 41.0 | 880.0 | 129.0 | 322.0 | 126.0 | 8.3252 | 452600.0 | NEAR BAY | -122192.12 | 5 | . 1 -122.22 | 37.86 | 21.0 | 7099.0 | 1106.0 | 2401.0 | 1138.0 | 8.3014 | 358500.0 | NEAR BAY | -122182.14 | 5 | . 2 -122.24 | 37.85 | 52.0 | 1467.0 | 190.0 | 496.0 | 177.0 | 7.2574 | 352100.0 | NEAR BAY | -122202.15 | 5 | . 3 -122.25 | 37.85 | 52.0 | 1274.0 | 235.0 | 558.0 | 219.0 | 5.6431 | 341300.0 | NEAR BAY | -122212.15 | 4 | . 4 -122.25 | 37.85 | 52.0 | 1627.0 | 280.0 | 565.0 | 259.0 | 3.8462 | 342200.0 | NEAR BAY | -122212.15 | 3 | . housing[&quot;income_cat&quot;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f330fdb2278&gt; . As we can see in the histogram, there are very few rows of data for low income category and a random split can create a bias for that. . We have used the &quot;strata&quot; of the incomes and we will use this info to split data by taking samples from each strata . from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing[&quot;income_cat&quot;]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] . strat_test_set[&quot;income_cat&quot;].value_counts() / len(strat_test_set) . 3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64 . strat_train_set[&quot;income_cat&quot;].value_counts() / len(strat_test_set) . 3 1.402374 2 1.275436 4 0.705184 5 0.457607 1 0.159399 Name: income_cat, dtype: float64 . housing[&quot;income_cat&quot;].value_counts() / len(housing) . 3 0.350581 2 0.318847 4 0.176308 5 0.114438 1 0.039826 Name: income_cat, dtype: float64 . As seen above, the data distribution is good across all income groups . We now remove the income category since it is no longer needed. . for set_ in (strat_train_set, strat_test_set): set_.drop(&quot;income_cat&quot;, axis=1, inplace=True) . Data vizualizations after the data split . housing_train = strat_train_set.copy() . made a copy of the train data and calling it housing_train . Lets look at some viz of the data. . Geographical data is first since we have it.. . housing_train.plot(kind=&#39;scatter&#39;, x= &#39;longitude&#39;, y=&#39;latitude&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f33103332b0&gt; . alpha parameter is used to change the transparency. . The darker regions indicate that there are overlapping scatter plots in those regions which in turn indicates data density. . housing_train.plot(kind=&#39;scatter&#39;, x= &#39;longitude&#39;, y=&#39;latitude&#39;, alpha=0.1) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3310324f98&gt; . Lets plot other columns . s option is the size of circles . c is for color . cmap is for color map. jet color map is used. . housing_train.plot(kind=&quot;scatter&quot;, x=&quot;longitude&quot;, y=&quot;latitude&quot;, alpha=0.4, s=housing_train[&quot;population&quot;]/100, label=&quot;population&quot;, figsize=(10,7), c=&quot;median_house_value&quot;, cmap=plt.get_cmap(&quot;jet&quot;), colorbar=True, ) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f3310960b00&gt; . We can also look for correlations between different columns. . This works if the dataset is small . housing_train.drop(&quot;id&quot;, axis=1, inplace=True) . We will drop the id column since it is no longer needed. . corr_matrix = housing_train.corr() . print(corr_matrix) . longitude latitude ... median_income median_house_value longitude 1.000000 -0.924478 ... -0.019583 -0.047432 latitude -0.924478 1.000000 ... -0.075205 -0.142724 housing_median_age -0.105848 0.005766 ... -0.111360 0.114110 total_rooms 0.048871 -0.039184 ... 0.200087 0.135097 total_bedrooms 0.076598 -0.072419 ... -0.009740 0.047689 population 0.108030 -0.115222 ... 0.002380 -0.026920 households 0.063070 -0.077647 ... 0.010781 0.064506 median_income -0.019583 -0.075205 ... 1.000000 0.687160 median_house_value -0.047432 -0.142724 ... 0.687160 1.000000 [9 rows x 9 columns] . corr_matrix[&#39;median_house_value&#39;] . longitude -0.047432 latitude -0.142724 housing_median_age 0.114110 total_rooms 0.135097 total_bedrooms 0.047689 population -0.026920 households 0.064506 median_income 0.687160 median_house_value 1.000000 Name: median_house_value, dtype: float64 . housing_train.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity . 17606 -121.89 | 37.29 | 38.0 | 1568.0 | 351.0 | 710.0 | 339.0 | 2.7042 | 286600.0 | &lt;1H OCEAN | . 18632 -121.93 | 37.05 | 14.0 | 679.0 | 108.0 | 306.0 | 113.0 | 6.4214 | 340600.0 | &lt;1H OCEAN | . 14650 -117.20 | 32.77 | 31.0 | 1952.0 | 471.0 | 936.0 | 462.0 | 2.8621 | 196900.0 | NEAR OCEAN | . 3230 -119.61 | 36.31 | 25.0 | 1847.0 | 371.0 | 1460.0 | 353.0 | 1.8839 | 46300.0 | INLAND | . 3555 -118.59 | 34.23 | 17.0 | 6592.0 | 1525.0 | 4459.0 | 1463.0 | 3.0347 | 254500.0 | &lt;1H OCEAN | . Another thing to look out is whether we can create new attributes from existing attributes. . Sometimes new attributes that we create will correlate much better to the dependent variable that we want to predict. . Most of the times, date columns can be split to make new columns like Year, Month, Day, Week, MonthEnd, QuarterEnd etc. . In this dataset we do not have any Date attribute though. . Below is datepart function taken from fastai library . This function converts a column of df from a datetime64 to many columns containing the information from the date. . This applies changes inplace. . Parameters: -- df: A pandas data frame. df gain several new columns. fldname: A string that is the name of the date column you wish to expand. If it is not a datetime64 series, it will be converted to one with pd.to_datetime. drop: If true then the original date column will be removed. time: If true time features: Hour, Minute, Second will be added. Examples: . df = pd.DataFrame({ &#39;A&#39; :pd.to_datetime([&#39;3/11/2000&#39;, &#39;3/12/2000&#39;, &#39;3/13/2000&#39;], infer_datetime_format=False) }) &gt;&gt;&gt; df . A 0 2000-03-11 1 2000-03-12 2 2000-03-13 &gt;&gt;&gt; add_datepart(df, &#39;A&#39;) &gt;&gt;&gt; df AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed 0 2000 3 10 11 5 71 False False False False False False 952732800 1 2000 3 10 12 6 72 False False False False False False 952819200 2 2000 3 11 13 0 73 False False False False False False 952905600 . def add_datepart(df, fldname, drop=True, time=False): fld = df[fldname] if not np.issubdtype(fld.dtype, np.datetime64): df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True) targ_pre = re.sub(&#39;[Dd]ate$&#39;, &#39;&#39;, fldname) attr = [&#39;Year&#39;, &#39;Month&#39;, &#39;Week&#39;, &#39;Day&#39;, &#39;Dayofweek&#39;, &#39;Dayofyear&#39;, &#39;Is_month_end&#39;, &#39;Is_month_start&#39;, &#39;Is_quarter_end&#39;, &#39;Is_quarter_start&#39;, &#39;Is_year_end&#39;, &#39;Is_year_start&#39;] if time: attr = attr + [&#39;Hour&#39;, &#39;Minute&#39;, &#39;Second&#39;] for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower()) df[targ_pre + &#39;Elapsed&#39;] = fld.astype(np.int64) // 10 ** 9 if drop: df.drop(fldname, axis=1, inplace=True) . Data Cleanup and transformations . Before we start with cleanup, lets copy the training data set after removing the independent variable. . The independent variable is copied to a label dataframe. . housing_labels = housing_train[&quot;median_house_value&quot;].copy() . The labels will be useful later when we run the ML models and evaluate the outputs and compare with these labels. . We will drop these labels from training data since we already saved it in housing_labels . housing_train = housing_train.drop(&quot;median_house_value&quot;, axis=1) . We will also need to do Data Cleanup. . Typical cleanups are Null cleanups . We can either remove such rows, remove those features or add some default value i.e. dropna, drop, fillna respectively . Sklearn provides an Imputer class for this. Generally we fill the nulls with median or mean value. . The strategy that you pass to the imputer should be based on that decision . from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=&quot;median&quot;) . SimpleImputer Class works on DFs which only have numbers. So, we drop the categorical column for now . housing_num = housing_train.drop(&quot;ocean_proximity&quot;, axis=1) . We can see below that housing_num only has number columns . housing_num.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income . 17606 -121.89 | 37.29 | 38.0 | 1568.0 | 351.0 | 710.0 | 339.0 | 2.7042 | . 18632 -121.93 | 37.05 | 14.0 | 679.0 | 108.0 | 306.0 | 113.0 | 6.4214 | . 14650 -117.20 | 32.77 | 31.0 | 1952.0 | 471.0 | 936.0 | 462.0 | 2.8621 | . 3230 -119.61 | 36.31 | 25.0 | 1847.0 | 371.0 | 1460.0 | 353.0 | 1.8839 | . 3555 -118.59 | 34.23 | 17.0 | 6592.0 | 1525.0 | 4459.0 | 1463.0 | 3.0347 | . imputer.fit(housing_num) . SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy=&#39;median&#39;, verbose=0) . housing_num.median().values . array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) . X = imputer.transform(housing_num) . type(X) . numpy.ndarray . X . array([[-121.89 , 37.29 , 38. , ..., 710. , 339. , 2.7042], [-121.93 , 37.05 , 14. , ..., 306. , 113. , 6.4214], [-117.2 , 32.77 , 31. , ..., 936. , 462. , 2.8621], ..., [-116.4 , 34.09 , 9. , ..., 2098. , 765. , 3.2723], [-118.01 , 33.82 , 31. , ..., 1356. , 356. , 4.0625], [-122.45 , 37.77 , 52. , ..., 1269. , 639. , 3.575 ]]) . We now have to convert this back to Dataframe . housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) . housing_tr.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income . 17606 -121.89 | 37.29 | 38.0 | 1568.0 | 351.0 | 710.0 | 339.0 | 2.7042 | . 18632 -121.93 | 37.05 | 14.0 | 679.0 | 108.0 | 306.0 | 113.0 | 6.4214 | . 14650 -117.20 | 32.77 | 31.0 | 1952.0 | 471.0 | 936.0 | 462.0 | 2.8621 | . 3230 -119.61 | 36.31 | 25.0 | 1847.0 | 371.0 | 1460.0 | 353.0 | 1.8839 | . 3555 -118.59 | 34.23 | 17.0 | 6592.0 | 1525.0 | 4459.0 | 1463.0 | 3.0347 | . type(housing_tr) . pandas.core.frame.DataFrame . We can check if all the NULL values are filled up. . housing_tr.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 16512 entries, 17606 to 15775 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 longitude 16512 non-null float64 1 latitude 16512 non-null float64 2 housing_median_age 16512 non-null float64 3 total_rooms 16512 non-null float64 4 total_bedrooms 16512 non-null float64 5 population 16512 non-null float64 6 households 16512 non-null float64 7 median_income 16512 non-null float64 dtypes: float64(8) memory usage: 1.1 MB . Lets deal with the categorical column now. . Handle Categorical Attributes . housing_cat = housing_train[[&quot;ocean_proximity&quot;]] . type(housing_cat) . pandas.core.frame.DataFrame . Below 2 cells are to just show why we needed double square brackets in above cells when we created housing_cat . housing_cat2= housing_train[&#39;ocean_proximity&#39;] . type(housing_cat2) . pandas.core.series.Series . We now have to convert the categorical values to numbers since most ML algorithms can use only numbers . There are many classes in sklearn which we can use for this. These are transformer classes. . Ordinal Encoder | One Hot Encoder | Custom Transformer (user has to write it) | from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) . type(housing_cat_encoded) . numpy.ndarray . print(housing_cat_encoded.tolist()) . [[0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [3.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [0.0], [3.0], [0.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [4.0], [3.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [4.0], [0.0], [4.0], [3.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [1.0], [3.0], [0.0], [4.0], [4.0], [3.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [3.0], [4.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [1.0], [3.0], [1.0], [0.0], [4.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [3.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [3.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [4.0], [1.0], [4.0], [1.0], [1.0], [4.0], [4.0], [0.0], [4.0], [0.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [3.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [3.0], [1.0], [4.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [1.0], [4.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [3.0], [1.0], [0.0], [1.0], [3.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [3.0], [4.0], [4.0], [4.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [4.0], [1.0], [1.0], [4.0], [1.0], [0.0], [4.0], [0.0], [1.0], [3.0], [3.0], [0.0], [1.0], [4.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0], [3.0], [0.0], [0.0], [1.0], [0.0], [3.0], [3.0], [4.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [3.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [3.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [4.0], [4.0], [1.0], [3.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [4.0], [0.0], [3.0], [0.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [4.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [3.0], [4.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [1.0], [3.0], [0.0], [3.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [3.0], [1.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [3.0], [0.0], [4.0], [1.0], [1.0], [1.0], [4.0], [4.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [3.0], [1.0], [1.0], [3.0], [1.0], [3.0], [3.0], [1.0], [0.0], [1.0], [4.0], [1.0], [4.0], [1.0], [3.0], [1.0], [4.0], [1.0], [4.0], [4.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [3.0], [1.0], [0.0], [3.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [4.0], [0.0], [3.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [4.0], [1.0], [0.0], [3.0], [4.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [3.0], [4.0], [1.0], [3.0], [1.0], [0.0], [4.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [3.0], [1.0], [0.0], [3.0], [3.0], [1.0], [0.0], [3.0], [4.0], [1.0], [3.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [3.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [1.0], [0.0], [3.0], [1.0], [4.0], [0.0], [1.0], [1.0], [4.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [0.0], [3.0], [3.0], [3.0], [1.0], [1.0], [0.0], [3.0], [1.0], [3.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [4.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [4.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [4.0], [3.0], [4.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [4.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [3.0], [3.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [4.0], [3.0], [0.0], [3.0], [4.0], [3.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [4.0], [1.0], [3.0], [1.0], [3.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [3.0], [1.0], [4.0], [4.0], [4.0], [1.0], [1.0], [1.0], [4.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [4.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [1.0], [3.0], [4.0], [3.0], [3.0], [1.0], [1.0], [3.0], [3.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [3.0], [4.0], [1.0], [4.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [4.0], [3.0], [1.0], [1.0], [3.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [4.0], [0.0], [3.0], [4.0], [1.0], [0.0], [4.0], [0.0], [1.0], [3.0], [3.0], [3.0], [0.0], [0.0], [1.0], [4.0], [3.0], [4.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [3.0], [0.0], [3.0], [4.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [3.0], [1.0], [3.0], [0.0], [1.0], [1.0], [3.0], [1.0], [4.0], [4.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [0.0], [3.0], [1.0], [3.0], [0.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [4.0], [1.0], [3.0], [3.0], [1.0], [3.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [4.0], [1.0], [0.0], [3.0], [0.0], [3.0], [3.0], [4.0], [1.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [3.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [3.0], [4.0], [0.0], [3.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [4.0], [4.0], [0.0], [0.0], [4.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [3.0], [4.0], [4.0], [3.0], [4.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [3.0], [4.0], [1.0], [1.0], [4.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [4.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [3.0], [3.0], [1.0], [1.0], [4.0], [3.0], [4.0], [0.0], [1.0], [3.0], [4.0], [4.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [3.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [4.0], [0.0], [4.0], [1.0], [1.0], [0.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [3.0], [3.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [4.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [4.0], [4.0], [1.0], [0.0], [3.0], [0.0], [3.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [3.0], [3.0], [0.0], [0.0], [4.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [3.0], [1.0], [3.0], [4.0], [4.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [3.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [4.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [3.0], [1.0], [0.0], [3.0], [4.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [1.0], [1.0], [0.0], [3.0], [1.0], [1.0], [0.0], [4.0], [4.0], [1.0], [1.0], [1.0], [0.0], [3.0], [1.0], [4.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [3.0], [3.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [3.0], [3.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [4.0], [1.0], [4.0], [1.0], [3.0], [4.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [3.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [3.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [3.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [4.0], [1.0], [1.0], [4.0], [1.0], [4.0], [1.0], [1.0], [4.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [1.0], [1.0], [3.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [3.0], [3.0], [3.0], [1.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [3.0], [4.0], [4.0], [4.0], [1.0], [4.0], [1.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [4.0], [0.0], [3.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [3.0], [1.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [3.0], [1.0], [4.0], [4.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [4.0], [3.0], [0.0], [1.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [3.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [3.0], [3.0], [0.0], [4.0], [1.0], [3.0], [0.0], [1.0], [3.0], [0.0], [3.0], [4.0], [1.0], [3.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [4.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [4.0], [4.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [3.0], [4.0], [4.0], [4.0], [0.0], [1.0], [4.0], [4.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [3.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [4.0], [3.0], [1.0], [4.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [3.0], [1.0], [1.0], [3.0], [1.0], [3.0], [0.0], [3.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [0.0], [4.0], [0.0], [3.0], [3.0], [0.0], [4.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [4.0], [0.0], [4.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [3.0], [3.0], [3.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [3.0], [3.0], [3.0], [0.0], [1.0], [4.0], [1.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [3.0], [0.0], [3.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [4.0], [1.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [3.0], [3.0], [4.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [3.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [3.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [4.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [4.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [4.0], [4.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [4.0], [4.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [3.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [3.0], [1.0], [1.0], [0.0], [4.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [4.0], [3.0], [0.0], [4.0], [4.0], [1.0], [3.0], [3.0], [4.0], [1.0], [0.0], [4.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [3.0], [3.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [3.0], [0.0], [1.0], [3.0], [0.0], [3.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [3.0], [1.0], [0.0], [0.0], [4.0], [0.0], [4.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [3.0], [1.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [3.0], [0.0], [1.0], [4.0], [0.0], [1.0], [3.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [1.0], [3.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [3.0], [4.0], [4.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [3.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [4.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [4.0], [0.0], [3.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [4.0], [0.0], [4.0], [0.0], [4.0], [0.0], [4.0], [3.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [4.0], [4.0], [4.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [4.0], [3.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [0.0], [3.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [3.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [4.0], [0.0], [3.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [3.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [3.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [4.0], [3.0], [4.0], [0.0], [3.0], [1.0], [1.0], [1.0], [4.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [3.0], [0.0], [4.0], [4.0], [4.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [0.0], [4.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [4.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [4.0], [0.0], [0.0], [4.0], [1.0], [3.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [4.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [3.0], [1.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [3.0], [0.0], [3.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [3.0], [1.0], [0.0], [3.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [4.0], [3.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [4.0], [4.0], [1.0], [4.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [4.0], [3.0], [3.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [4.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [4.0], [3.0], [4.0], [0.0], [4.0], [3.0], [3.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [3.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [4.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [3.0], [4.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [3.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [1.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [3.0], [3.0], [3.0], [0.0], [0.0], [3.0], [3.0], [0.0], [4.0], [0.0], [4.0], [3.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [1.0], [4.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0], [4.0], [1.0], [4.0], [0.0], [0.0], [0.0], [4.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [3.0], [4.0], [3.0], [1.0], [0.0], [0.0], [3.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [4.0], [4.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [1.0], [1.0], [3.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [3.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [0.0], [3.0], [1.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [3.0], [0.0], [4.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [3.0], [4.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [4.0], [1.0], [0.0], [4.0], [4.0], [3.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [3.0], [0.0], [3.0], [1.0], [4.0], [4.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [4.0], [0.0], [1.0], [0.0], [1.0], [4.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [3.0], [3.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [3.0], [1.0], [3.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [1.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [4.0], [1.0], [1.0], [3.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [4.0], [3.0], [0.0], [0.0], [3.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [4.0], [3.0], [3.0], [0.0], [0.0], [4.0], [1.0], [3.0], [1.0], [0.0], [1.0], [3.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [3.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [4.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [1.0], [1.0], [3.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [4.0], [3.0], [0.0], [4.0], [3.0], [1.0], [0.0], [1.0], [0.0], [4.0], [3.0], [0.0], [4.0], [4.0], [3.0], [3.0], [1.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [1.0], [3.0], [0.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [3.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [0.0], [3.0], [4.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [3.0], [0.0], [3.0], [3.0], [1.0], [1.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [4.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [3.0], [3.0], [4.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [3.0], [1.0], [4.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [1.0], [4.0], [3.0], [0.0], [0.0], [4.0], [4.0], [0.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [3.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [4.0], [4.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [3.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [4.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [4.0], [1.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [3.0], [3.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [1.0], [1.0], [3.0], [3.0], [0.0], [0.0], [3.0], [0.0], [1.0], [4.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [1.0], [4.0], [4.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [0.0], [4.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [4.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [3.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [3.0], [4.0], [1.0], [3.0], [3.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [4.0], [4.0], [1.0], [0.0], [0.0], [3.0], [1.0], [3.0], [0.0], [4.0], [0.0], [4.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [4.0], [3.0], [0.0], [4.0], [3.0], [3.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [4.0], [3.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [4.0], [1.0], [4.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [4.0], [4.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [4.0], [3.0], [0.0], [0.0], [4.0], [0.0], [4.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [4.0], [1.0], [3.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [4.0], [3.0], [0.0], [4.0], [1.0], [3.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [3.0], [3.0], [1.0], [4.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [3.0], [4.0], [4.0], [1.0], [0.0], [4.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [0.0], [1.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [3.0], [1.0], [4.0], [0.0], [0.0], [4.0], [4.0], [1.0], [3.0], [1.0], [1.0], [4.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [4.0], [3.0], [3.0], [3.0], [4.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [3.0], [1.0], [0.0], [1.0], [4.0], [0.0], [3.0], [0.0], [0.0], [1.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [4.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [4.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [3.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [4.0], [3.0], [3.0], [1.0], [1.0], [0.0], [1.0], [3.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [4.0], [4.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [3.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [4.0], [1.0], [4.0], [3.0], [0.0], [0.0], [3.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [3.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [3.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [4.0], [3.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [3.0], [4.0], [3.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [3.0], [1.0], [4.0], [0.0], [4.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [4.0], [1.0], [4.0], [1.0], [0.0], [0.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [2.0], [4.0], [1.0], [3.0], [3.0], [3.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [3.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [3.0], [4.0], [1.0], [1.0], [4.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [1.0], [4.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [4.0], [4.0], [1.0], [1.0], [3.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [3.0], [3.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [3.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [3.0], [1.0], [3.0], [0.0], [3.0], [0.0], [1.0], [4.0], [3.0], [3.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [4.0], [1.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [0.0], [4.0], [1.0], [0.0], [0.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [3.0], [1.0], [4.0], [0.0], [0.0], [3.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [3.0], [1.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [4.0], [3.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [3.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [3.0], [4.0], [1.0], [4.0], [4.0], [1.0], [0.0], [4.0], [4.0], [1.0], [0.0], [0.0], [3.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [1.0], [1.0], [4.0], [0.0], [4.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [3.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [3.0], [0.0], [3.0], [4.0], [1.0], [3.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [4.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [3.0], [1.0], [4.0], [1.0], [1.0], [1.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [4.0], [4.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [3.0], [0.0], [1.0], [3.0], [0.0], [4.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [4.0], [3.0], [4.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [0.0], [3.0], [3.0], [0.0], [0.0], [3.0], [3.0], [1.0], [1.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [4.0], [4.0], [1.0], [0.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [3.0], [1.0], [4.0], [0.0], [1.0], [0.0], [4.0], [4.0], [0.0], [4.0], [1.0], [1.0], [4.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [4.0], [1.0], [3.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [4.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [3.0], [3.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [3.0], [1.0], [4.0], [3.0], [0.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [3.0], [3.0], [0.0], [0.0], [3.0], [1.0], [3.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [3.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [3.0], [3.0], [1.0], [3.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [4.0], [4.0], [4.0], [4.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [3.0], [4.0], [0.0], [1.0], [0.0], [4.0], [1.0], [4.0], [3.0], [0.0], [1.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [1.0], [3.0], [1.0], [3.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [4.0], [4.0], [3.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [0.0], [3.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [3.0], [3.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [4.0], [4.0], [3.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [3.0], [4.0], [1.0], [1.0], [3.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [1.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [4.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [3.0], [1.0], [1.0], [1.0], [4.0], [3.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [4.0], [3.0], [0.0], [0.0], [3.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [4.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [3.0], [3.0], [0.0], [3.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [4.0], [4.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [4.0], [0.0], [1.0], [4.0], [4.0], [3.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [3.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [4.0], [1.0], [4.0], [0.0], [0.0], [4.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [4.0], [3.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [1.0], [1.0], [3.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [4.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [4.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [3.0], [3.0], [1.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [4.0], [3.0], [1.0], [1.0], [3.0], [1.0], [4.0], [1.0], [1.0], [4.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [3.0], [4.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [3.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [4.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [4.0], [4.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [4.0], [3.0], [1.0], [0.0], [3.0], [0.0], [0.0], [3.0], [4.0], [4.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [4.0], [3.0], [1.0], [0.0], [1.0], [0.0], [3.0], [3.0], [3.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [4.0], [0.0], [0.0], [3.0], [4.0], [1.0], [0.0], [3.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [4.0], [3.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [3.0], [3.0], [4.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [4.0], [0.0], [3.0], [4.0], [3.0], [4.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [4.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [4.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [4.0], [1.0], [0.0], [1.0], [1.0], [4.0], [3.0], [3.0], [1.0], [0.0], [0.0], [0.0], [3.0], [3.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [4.0], [4.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [3.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [3.0], [0.0], [3.0], [1.0], [1.0], [0.0], [4.0], [4.0], [3.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [3.0], [3.0], [3.0], [1.0], [1.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [3.0], [3.0], [3.0], [1.0], [1.0], [4.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [4.0], [3.0], [0.0], [3.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [3.0], [4.0], [4.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [3.0], [4.0], [0.0], [4.0], [3.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [1.0], [4.0], [1.0], [0.0], [1.0], [3.0], [4.0], [4.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [3.0], [3.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [4.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [3.0], [3.0], [4.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [3.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [3.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [4.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [3.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [3.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [3.0], [3.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [3.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [3.0], [4.0], [1.0], [0.0], [0.0], [3.0], [1.0], [1.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [3.0], [3.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [4.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [0.0], [1.0], [1.0], [1.0], [3.0], [3.0], [1.0], [0.0], [1.0], [4.0], [0.0], [3.0], [4.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [1.0], [0.0], [4.0], [4.0], [1.0], [3.0], [0.0], [1.0], [1.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [0.0], [4.0], [4.0], [4.0], [3.0], [1.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [3.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [3.0], [1.0], [3.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [3.0], [0.0], [3.0], [1.0], [3.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [4.0], [0.0], [0.0], [3.0], [4.0], [3.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [4.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [3.0], [4.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [4.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [4.0], [1.0], [4.0], [3.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [3.0], [0.0], [1.0], [1.0], [4.0], [4.0], [3.0], [4.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [3.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [3.0], [4.0], [3.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [4.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [4.0], [3.0], [3.0], [0.0], [0.0], [1.0], [3.0], [3.0], [3.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [4.0], [4.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [4.0], [1.0], [0.0], [4.0], [4.0], [1.0], [1.0], [4.0], [4.0], [3.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [3.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [1.0], [1.0], [4.0], [3.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [4.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [3.0], [4.0], [4.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [3.0], [1.0], [0.0], [3.0], [3.0], [1.0], [4.0], [4.0], [4.0], [0.0], [4.0], [1.0], [1.0], [4.0], [1.0], [4.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [3.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [3.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [3.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [4.0], [0.0], [0.0], [1.0], [4.0], [4.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [1.0], [3.0], [3.0], [1.0], [4.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [4.0], [4.0], [1.0], [4.0], [4.0], [0.0], [4.0], [1.0], [0.0], [1.0], [3.0], [1.0], [3.0], [4.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [3.0], [3.0], [4.0], [1.0], [4.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [0.0], [1.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0], [3.0], [0.0], [3.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [4.0], [1.0], [0.0], [0.0], [3.0], [3.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [4.0], [1.0], [4.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [1.0], [0.0], [4.0], [3.0], [1.0], [3.0], [3.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [4.0], [1.0], [4.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [3.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [4.0], [1.0], [3.0], [3.0], [0.0], [4.0], [1.0], [0.0], [3.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [4.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [3.0], [1.0], [3.0], [4.0], [0.0], [3.0], [0.0], [4.0], [4.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [3.0], [4.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [4.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [4.0], [4.0], [0.0], [1.0], [3.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [4.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [3.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [4.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [3.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [4.0], [4.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [3.0], [3.0], [1.0], [0.0], [4.0], [1.0], [3.0], [1.0], [0.0], [4.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [3.0], [4.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [3.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [3.0], [1.0], [4.0], [0.0], [0.0], [0.0], [4.0], [3.0], [4.0], [3.0], [1.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [3.0], [1.0], [4.0], [0.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [4.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [3.0], [0.0], [0.0], [4.0], [4.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [1.0], [3.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [3.0], [3.0], [0.0], [4.0], [0.0], [4.0], [1.0], [3.0], [1.0], [1.0], [4.0], [3.0], [1.0], [1.0], [1.0], [1.0], [0.0], [3.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [4.0], [1.0], [1.0], [4.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [3.0], [3.0], [1.0], [1.0], [4.0], [3.0], [1.0], [0.0], [4.0], [3.0], [4.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [3.0], [0.0], [3.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [4.0], [3.0], [3.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [4.0], [0.0], [4.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [3.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [3.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [3.0], [1.0], [4.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [0.0], [3.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [3.0], [0.0], [3.0], [0.0], [1.0], [4.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [1.0], [4.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [4.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [3.0], [1.0], [3.0], [1.0], [4.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [4.0], [4.0], [0.0], [0.0], [3.0], [4.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [3.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [4.0], [1.0], [3.0], [4.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [3.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [3.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [3.0], [3.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [4.0], [3.0], [1.0], [3.0], [1.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [3.0], [3.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [3.0], [0.0], [3.0], [1.0], [4.0], [1.0], [1.0], [0.0], [4.0], [1.0], [1.0], [0.0], [1.0], [4.0], [4.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [4.0], [1.0], [1.0], [3.0], [3.0], [4.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [4.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [4.0], [0.0], [3.0], [1.0], [3.0], [4.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [3.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [1.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [3.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [4.0], [3.0], [0.0], [1.0], [0.0], [4.0], [3.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [4.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [4.0], [3.0], [4.0], [1.0], [0.0], [4.0], [3.0], [1.0], [0.0], [0.0], [3.0], [4.0], [0.0], [3.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [4.0], [3.0], [0.0], [1.0], [4.0], [1.0], [1.0], [4.0], [3.0], [0.0], [4.0], [3.0], [3.0], [0.0], [4.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [4.0], [0.0], [4.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [0.0], [4.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [3.0], [4.0], [1.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [4.0], [4.0], [1.0], [4.0], [1.0], [4.0], [1.0], [3.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [3.0], [1.0], [3.0], [3.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [4.0], [4.0], [3.0], [4.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [4.0], [3.0], [3.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [3.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [3.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [4.0], [1.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [3.0], [1.0], [0.0], [4.0], [0.0], [4.0], [3.0], [3.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [1.0], [3.0], [1.0], [1.0], [4.0], [0.0], [3.0], [1.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [1.0], [3.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [3.0], [4.0], [3.0], [0.0], [1.0], [0.0], [3.0], [1.0], [1.0], [1.0], [3.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [3.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [4.0], [4.0], [0.0], [3.0], [4.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [0.0], [3.0], [1.0], [1.0], [4.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [4.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [3.0], [4.0], [3.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [1.0], [4.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [3.0], [0.0], [4.0], [4.0], [4.0], [3.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [3.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [4.0], [4.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [4.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [4.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [4.0], [0.0], [1.0], [3.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [4.0], [4.0], [4.0], [1.0], [3.0], [4.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [4.0], [1.0], [4.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [3.0], [4.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [3.0], [3.0], [0.0], [3.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [4.0], [3.0], [1.0], [1.0], [0.0], [3.0], [0.0], [3.0], [1.0], [1.0], [0.0], [3.0], [1.0], [0.0], [3.0], [4.0], [3.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [4.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [4.0], [0.0], [3.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [3.0], [3.0], [1.0], [1.0], [4.0], [0.0], [4.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [4.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [4.0], [4.0], [4.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [3.0], [3.0], [4.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [3.0], [0.0], [4.0], [1.0], [1.0], [4.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [3.0], [0.0], [3.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [4.0], [4.0], [3.0], [4.0], [0.0], [0.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [3.0], [3.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [3.0], [1.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [4.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [4.0], [3.0], [3.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [3.0], [3.0], [0.0], [3.0], [1.0], [0.0], [1.0], [3.0], [0.0], [4.0], [0.0], [4.0], [0.0], [3.0], [1.0], [3.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [4.0], [0.0], [3.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [4.0], [3.0], [4.0], [0.0], [3.0], [1.0], [4.0], [1.0], [0.0], [3.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [3.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [3.0], [3.0], [3.0], [0.0], [4.0], [0.0], [4.0], [1.0], [3.0], [1.0], [3.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [3.0], [0.0], [4.0], [1.0], [4.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [4.0], [3.0], [4.0], [1.0], [3.0], [3.0], [0.0], [1.0], [3.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [3.0], [4.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [4.0], [4.0], [1.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [3.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [3.0], [3.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [3.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [3.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [4.0], [4.0], [1.0], [0.0], [3.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [4.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [4.0], [4.0], [1.0], [1.0], [1.0], [3.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [3.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [3.0], [1.0], [1.0], [3.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [3.0], [4.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [3.0], [4.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [4.0], [1.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [4.0], [0.0], [0.0], [4.0], [3.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [3.0], [0.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [4.0], [4.0], [0.0], [4.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [4.0], [3.0], [0.0], [1.0], [1.0], [1.0], [4.0], [3.0], [0.0], [3.0], [3.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [3.0], [4.0], [0.0], [0.0], [1.0], [0.0], [3.0], [3.0], [3.0], [0.0], [1.0], [1.0], [4.0], [3.0], [1.0], [1.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [0.0], [3.0], [3.0], [1.0], [4.0], [4.0], [3.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [4.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [3.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [0.0], [1.0], [3.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [1.0], [3.0], [3.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [3.0], [1.0], [3.0], [1.0], [4.0], [4.0], [4.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [1.0], [3.0], [3.0], [0.0], [3.0], [0.0], [3.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [4.0], [0.0], [1.0], [1.0], [3.0], [4.0], [0.0], [4.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [4.0], [4.0], [1.0], [3.0], [1.0], [1.0], [4.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [3.0], [4.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [3.0], [3.0], [1.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [3.0], [3.0], [4.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [3.0], [0.0], [3.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [4.0], [1.0], [3.0], [0.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [3.0], [4.0], [0.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [4.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [1.0], [0.0], [1.0], [4.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [3.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [3.0], [3.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [1.0], [1.0], [1.0], [3.0], [1.0], [4.0], [1.0], [3.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [4.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [0.0], [3.0], [1.0], [3.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [3.0], [4.0], [4.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [3.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [3.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [3.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [1.0], [4.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [3.0], [0.0], [1.0], [4.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [1.0], [3.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [4.0], [1.0], [4.0], [3.0], [3.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [4.0], [1.0], [0.0], [3.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [3.0], [0.0], [1.0], [3.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [4.0], [3.0], [0.0], [0.0], [4.0], [1.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [3.0], [0.0], [1.0], [0.0], [3.0], [1.0], [4.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [4.0], [0.0], [1.0], [3.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [3.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [1.0], [0.0], [0.0], [3.0], [4.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [0.0], [3.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [4.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [0.0], [3.0], [3.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [3.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [3.0], [3.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [3.0], [0.0], [4.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [3.0], [0.0], [3.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [4.0], [4.0], [1.0], [1.0], [0.0], [0.0], [1.0], [3.0], [0.0], [1.0], [2.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [3.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [4.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [4.0], [4.0], [1.0], [0.0], [1.0], [0.0], [3.0], [4.0], [1.0], [4.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [1.0], [4.0], [0.0], [3.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [3.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [1.0], [4.0], [1.0], [0.0], [4.0], [0.0], [3.0], [0.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [4.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [4.0], [0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [3.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [3.0], [3.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [4.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [4.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [3.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [3.0], [3.0], [1.0], [3.0], [1.0], [0.0], [0.0], [4.0], [3.0], [1.0], [0.0], [4.0], [4.0], [1.0], [4.0], [3.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [3.0], [4.0], [1.0], [0.0], [1.0], [0.0], [3.0], [1.0], [1.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [3.0], [1.0], [0.0], [1.0], [4.0], [1.0], [4.0], [4.0], [3.0], [0.0], [4.0], [4.0], [0.0], [4.0], [1.0], [0.0], [1.0], [1.0], [4.0], [3.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [4.0], [1.0], [3.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [4.0], [4.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [3.0], [4.0], [4.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [3.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [1.0], [0.0], [1.0], [4.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [1.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [4.0], [1.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [3.0], [4.0], [0.0], [4.0], [1.0], [1.0], [1.0], [4.0], [3.0], [0.0], [1.0], [0.0], [1.0], [0.0], [4.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [3.0], [0.0], [1.0], [4.0], [1.0], [0.0], [3.0], [0.0], [3.0], [3.0], [0.0], [1.0], [3.0], [0.0], [3.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [4.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [1.0], [3.0], [4.0], [0.0], [4.0], [3.0], [0.0], [4.0], [4.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [3.0], [0.0], [3.0], [1.0], [3.0], [1.0], [4.0], [4.0], [0.0], [1.0], [4.0], [1.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0], [1.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [1.0], [4.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [3.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [4.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [1.0], [3.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [4.0], [4.0], [0.0], [3.0], [1.0], [0.0], [1.0], [3.0], [0.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [0.0], [4.0], [1.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [3.0], [3.0], [0.0], [0.0], [4.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [4.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [4.0], [1.0], [0.0], [0.0], [3.0], [0.0], [1.0], [0.0], [0.0], [1.0], [0.0], [4.0], [3.0], [1.0], [1.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [1.0], [3.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [4.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [4.0], [4.0], [0.0], [1.0], [0.0], [4.0], [4.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [3.0], [1.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [1.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [3.0], [0.0], [4.0], [3.0], [1.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [3.0], [4.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [4.0], [0.0], [1.0], [1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [4.0], [3.0], [0.0], [1.0], [3.0], [1.0], [0.0], [4.0], [1.0], [0.0], [0.0], [4.0], [0.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [0.0], [1.0], [3.0], [3.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [1.0], [1.0], [4.0], [0.0], [0.0], [0.0], [0.0], [3.0], [0.0], [0.0], [1.0], [1.0], [1.0], [0.0], [1.0], [4.0], [0.0], [4.0], [1.0], [4.0], [1.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [3.0], [0.0], [0.0], [4.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [1.0], [4.0], [3.0], [4.0], [4.0], [4.0], [1.0], [0.0], [1.0], [0.0], [1.0], [1.0], [0.0], [0.0], [0.0], [4.0], [4.0], [0.0], [1.0], [0.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [1.0], [4.0], [1.0], [0.0], [0.0], [1.0], [1.0], [4.0], [3.0], [0.0], [3.0], [1.0], [1.0], [0.0], [1.0], [3.0], [1.0], [0.0], [0.0], [0.0], [4.0], [0.0], [1.0], [1.0], [0.0], [1.0], [1.0], [1.0], [0.0], [3.0]] . ordinal_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . This encoding may not be apt for this ocean proximity data. . We see the class assigned values like 0,1,2,3,4 for the 5 different values i.e. OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN . But we know that 4 is not greater than 3,2 or 1. . In fact, 4 is almost same as 1 i.e. Near Ocean is similar to Ocean. . This encoding may not result in a good prediction. . In such cases, we use One Hot Encoding.. . This just creates a sequence of 0s ans 1s for each Categorical value as shown below. Below is just an example to get an idea. . &#39;OCEAN&#39; - 00000 . &#39;INLAND&#39; - 01000 . &#39;ISLAND&#39; - 00100 . &#39;NEAR BAY&#39; - 00010 . &#39;NEAR OCEAN&#39; - 00001 . from sklearn.preprocessing import OneHotEncoder cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat) . housing_cat_1hot . &lt;16512x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 16512 stored elements in Compressed Sparse Row format&gt; . housing_cat_1hot.toarray() . array([[1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 0., 1.], ..., [0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 1., 0.]]) . cat_encoder.categories_ . [array([&#39;&lt;1H OCEAN&#39;, &#39;INLAND&#39;, &#39;ISLAND&#39;, &#39;NEAR BAY&#39;, &#39;NEAR OCEAN&#39;], dtype=object)] . Custom Transformers . Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. You will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all you need to do is create a class and implement three methods: fit() (returning self), transform(), and fit_transform(). . You can get the last one for free by simply adding TransformerMixin as a base class. If you add BaseEstimator as a base class (and avoid *args and **kargs in your constructor), you will also get two extra methods (get_params() and set_params()) that will be useful for automatic hyperparameter tuning. . For example, here is a small transformer class that adds the combined attributes we discussed earlier: . np.c_ is nothing but column stacking. Similarly we can also do np.r_for row stacking.. . from sklearn.base import BaseEstimator, TransformerMixin rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household] attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs = attr_adder.transform(housing.values) . In this example the transformer has one hyperparameter, add_bedrooms_per_room, set to True by default (it is often helpful to provide sensible defaults). This hyperparameter will allow you to easily find out whether adding this attribute helps the Machine Learning algorithms or not. More generally, you can add a hyperparameter to gate any data preparation step that you are not 100% sure about. The more you automate these data preparation steps, the more combinations you can automatically try out, making it much more likely that you will find a great combination (and saving you a lot of time). . Feature Scaling . Feature Scaling is nothing but making the numerical columns be in similar scale. . If there are numbers for one feature ranging from 0 to 1000 and for other feature ranging from -0.01 to .001 then the ML algorithms do not work well. . That is when feature scaling comes into picture to make the different numbers adhere to similar scale. . Two main approaches - . Min Max Scaling or Normalization . A min and max value could be given and all numbers will be changed to fit within that range. . | Generally the default range is 0 to 1 . | Causes issues when data has outliers | MinMaxScaler Class | . Standardization . Subtract the mean and Divide by Std Dev. | Less Affected by outliers | StandardScaler Class | . NOTE - Any transformation should be used to fit tothe training data only, not to the full dataset (including the test set). . Then they will be used to transform the training set and the test set (and any new data). . Pipelines . Like we saw in previous sections, there are many transformations that we did. . They are done one after the other and the outputs of one transformation generally goes as input to next transformation. . This can be lined up using a sklearn Pipeline class . We have used SimpleImputer to handle NULLs . We have created a Column Adder class to add new columns . We have to do Feature Scaling after the above two . from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([ (&#39;imputer&#39;, SimpleImputer(strategy=&quot;median&quot;)), (&#39;attribs_adder&#39;, CombinedAttributesAdder()), (&#39;std_scaler&#39;, StandardScaler()), ]) . housing_num_tr = num_pipeline.fit_transform(housing_num) . type(housing_num_tr) . numpy.ndarray . The Pipeline constructor takes a list of name/estimator pairs defining a sequence of steps. All but the last estimator must be transformers (i.e., they must have a fit_transform() method). The names can be anything you like (as long as they are unique and don’t contain double underscores, __); they will come in handy later for hyperparameter tuning. . When you call the pipeline’s fit() method, it calls fit_transform() sequentially on all transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it calls the fit() method. . The pipeline exposes the same methods as the final estimator. In this example, the last estimator is a StandardScaler, which is a transformer, so the pipeline has a transform() method that applies all the transforms to the data in sequence (and of course also a fit_transform() method, which is the one we used). . Column Transformer . So far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column. . In version 0.20, Scikit-Learn introduced the ColumnTransformer for this purpose. Without this sklearn-pandas or some other third party library has to be used for achieving this. . from sklearn.compose import ColumnTransformer num_attribs = list(housing_num) cat_attribs = list(housing_cat) . num_attribs . [&#39;longitude&#39;, &#39;latitude&#39;, &#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;] . cat_attribs . [&#39;ocean_proximity&#39;] . full_pipeline = ColumnTransformer([ (&quot;num&quot;, num_pipeline, num_attribs), (&quot;cat&quot;, OneHotEncoder(), cat_attribs), ], remainder=&#39;passthrough&#39;) . We pass the numerical pipeline (of 3 transformers) that we created earlier for Numerical Attributes and the single transformer for Categorical Attributes . Also note the remainder keyword. . If there are any columns that are not taken care of via the numerical pipeline or categorical pipeline, they will be dropped by default. . The &#39;passthrough&#39; keyword tells the column transformer to pass the remainder columns as is. . In this case, there are none. . We can now pass the training dataset to this full pipeline. . housing_train.head() . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 17606 -121.89 | 37.29 | 38.0 | 1568.0 | 351.0 | 710.0 | 339.0 | 2.7042 | &lt;1H OCEAN | . 18632 -121.93 | 37.05 | 14.0 | 679.0 | 108.0 | 306.0 | 113.0 | 6.4214 | &lt;1H OCEAN | . 14650 -117.20 | 32.77 | 31.0 | 1952.0 | 471.0 | 936.0 | 462.0 | 2.8621 | NEAR OCEAN | . 3230 -119.61 | 36.31 | 25.0 | 1847.0 | 371.0 | 1460.0 | 353.0 | 1.8839 | INLAND | . 3555 -118.59 | 34.23 | 17.0 | 6592.0 | 1525.0 | 4459.0 | 1463.0 | 3.0347 | &lt;1H OCEAN | . housing_train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 16512 entries, 17606 to 15775 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 longitude 16512 non-null float64 1 latitude 16512 non-null float64 2 housing_median_age 16512 non-null float64 3 total_rooms 16512 non-null float64 4 total_bedrooms 16354 non-null float64 5 population 16512 non-null float64 6 households 16512 non-null float64 7 median_income 16512 non-null float64 8 ocean_proximity 16512 non-null object dtypes: float64(8), object(1) memory usage: 1.3+ MB . Note that the housing train dataframe above is plain data without any transformations or data cleanup (see nulls in total_bedrooms) whatsoever . All that will be taken care of by the single line below since we built the pipeline to handle data cleanup, column additions, feature scaling, encoding categoricals . housing_prepared = full_pipeline.fit_transform(housing_train) . type(housing_prepared) . numpy.ndarray . housing_prepared . array([[-1.15604281, 0.77194962, 0.74333089, ..., 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, ..., 0. , 0. , 1. ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0. , 0. , 0. ], [ 0.78221312, -0.85106801, 0.18664186, ..., 0. , 0. , 0. ], [-1.43579109, 0.99645926, 1.85670895, ..., 0. , 1. , 0. ]]) . Train the ML model using the data . Data exploration is done. . Data split for train/test is done . Data vizualations were done to help with the above two. . All the required data transformations are complete. . The data is now ready to be passed to ML algorithms . Evaluate atleast 2-3 Models . Linear Regression ML Model . housing_prepared . array([[-1.15604281, 0.77194962, 0.74333089, ..., 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , ..., 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, ..., 0. , 0. , 1. ], ..., [ 1.58648943, -0.72478134, -1.56295222, ..., 0. , 0. , 0. ], [ 0.78221312, -0.85106801, 0.18664186, ..., 0. , 0. , 0. ], [-1.43579109, 0.99645926, 1.85670895, ..., 0. , 1. , 0. ]]) . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) . Lets pick some training data and get the predictions . some_data = housing_train.iloc[:5] . some_data . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity . 17606 -121.89 | 37.29 | 38.0 | 1568.0 | 351.0 | 710.0 | 339.0 | 2.7042 | &lt;1H OCEAN | . 18632 -121.93 | 37.05 | 14.0 | 679.0 | 108.0 | 306.0 | 113.0 | 6.4214 | &lt;1H OCEAN | . 14650 -117.20 | 32.77 | 31.0 | 1952.0 | 471.0 | 936.0 | 462.0 | 2.8621 | NEAR OCEAN | . 3230 -119.61 | 36.31 | 25.0 | 1847.0 | 371.0 | 1460.0 | 353.0 | 1.8839 | INLAND | . 3555 -118.59 | 34.23 | 17.0 | 6592.0 | 1525.0 | 4459.0 | 1463.0 | 3.0347 | &lt;1H OCEAN | . some_data_prepared = full_pipeline.transform(some_data) . some_data_prepared . array([[-1.15604281, 0.77194962, 0.74333089, -0.49323393, -0.44543821, -0.63621141, -0.42069842, -0.61493744, -0.31205452, -0.08649871, 0.15531753, 1. , 0. , 0. , 0. , 0. ], [-1.17602483, 0.6596948 , -1.1653172 , -0.90896655, -1.0369278 , -0.99833135, -1.02222705, 1.33645936, 0.21768338, -0.03353391, -0.83628902, 1. , 0. , 0. , 0. , 0. ], [ 1.18684903, -1.34218285, 0.18664186, -0.31365989, -0.15334458, -0.43363936, -0.0933178 , -0.5320456 , -0.46531516, -0.09240499, 0.4222004 , 0. , 0. , 0. , 0. , 1. ], [-0.01706767, 0.31357576, -0.29052016, -0.36276217, -0.39675594, 0.03604096, -0.38343559, -1.04556555, -0.07966124, 0.08973561, -0.19645314, 0. , 1. , 0. , 0. , 0. ], [ 0.49247384, -0.65929936, -0.92673619, 1.85619316, 2.41221109, 2.72415407, 2.57097492, -0.44143679, -0.35783383, -0.00419445, 0.2699277 , 1. , 0. , 0. , 0. , 0. ]]) . print(&quot;Predictions:&quot;, lin_reg.predict(some_data_prepared)) . Predictions: [210644.60459286 317768.80697211 210956.43331178 59218.98886849 189747.55849879] . print(&quot;Original Values:&quot;, list(housing_labels.iloc[:5])) . Original Values: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] . Okay.. Its not bad.. . Lets predict for entire dataset and also calculate the error . housing_predictions = lin_reg.predict(housing_prepared) . from sklearn.metrics import mean_squared_error lin_mse = mean_squared_error(housing_labels, housing_predictions) print(np.sqrt(lin_mse)) # This is nothing but RMSE . 68628.19819848923 . A difference of $68K for a prediction is not a good number. . It could be because the model is very simple.. Its a simple Linear Regression Model. . The model is underfitting the data. The features need to enhanced or a different model needs to be chosen. . Lets try Decision Tree . Decision Tree Model . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels) . DecisionTreeRegressor(ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;, random_state=None, splitter=&#39;best&#39;) . housing_predictions = tree_reg.predict(housing_prepared) . from sklearn.metrics import mean_squared_error tree_mse = mean_squared_error(housing_labels, housing_predictions) print(np.sqrt(tree_mse)) # This is nothing but RMSE from Decision Tree Regressor . 0.0 . A difference of $0 for a prediction vs original is also not a good number. . It is because the model is overfitting the data. . How do we confirm this? We can evaluate the model on validation set and the error is going to be very high . Another option is Scikit-Learn’s K-fold cross-validation feature . This class (from sklearn.model_selection import cross_val_score) splits the training set into K distinct subsets called folds, then it trains and evaluates the Decision Tree model K times, picking a different fold for evaluation every time and training on the other K-1 folds . from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) . We have run the cross validation above with K=10 . The scoring of the cross validation feature of sklearn is actually negative. . Therefore we do a sqrt of -scores below. . tree_rmse_scores = np.sqrt(-scores) . Below is a small function to display scores mean, std dev and list of scores for each cross validation. . def display_scores(scores): print(&quot;Mean Score : &quot;, scores.mean(), &quot; nStd Dev : &quot; ,scores.std(), &quot; nAll Scores : &quot; ,scores) . display_scores(tree_rmse_scores) . Mean Score : 70666.74616904806 Std Dev : 2928.322738055112 All Scores : [69327.01708558 65486.39211857 71358.25563341 69091.37509104 70570.20267046 75529.94622521 69895.20650652 70660.14247357 75843.74719231 68905.17669382] . As expected, the Decision Tree is not accurate in its predictions. . In fact, it seems to be worse than Linear Regression Model. . Lets try Random Forests algorithm next . Random Forest Model . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor() forest_reg.fit(housing_prepared, housing_labels) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . housing_predictions = forest_reg.predict(housing_prepared) . forest_rmse = np.sqrt(mean_squared_error(housing_labels, housing_predictions)) . display_scores(forest_rmse) . Mean Score : 18680.294240259147 Std Dev : 0.0 All Scores : 18680.294240259147 . Looks perfect..!!? No underfitting or overfitting ?? . Lets try the cross validation score again . scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=&quot;neg_mean_squared_error&quot;, cv=10) . display_scores(np.sqrt(-scores)) . Mean Score : 50150.018373763225 Std Dev : 1902.0697041387534 All Scores : [49557.6095063 47584.54435547 49605.349788 52325.13724488 49586.9889247 53154.87424699 48800.48987508 47880.32844243 52958.68645964 50046.17489414] . Save the Model . We can save the model with joblib library or pickle library. . Below is an example of joblib library. . import joblib . joblib.dump(forest_reg, &#39;forest_reg_model.pkl&#39;) . [&#39;forest_reg_model.pkl&#39;] . ls . forest_reg_model.pkl housing.csv sample_data/ . forest_reg_reloaded = joblib.load(&#39;forest_reg_model.pkl&#39;) . Finetune the Model . We can either manually finetune by changing the various hyperparameters (parameters passed to ML algorithm) OR use sklearn classes to try out various hyperparameters and see which ones are best . GridSearchCV and RandomizedSearchCV are the classes . Another way to finetune is to use ensemble method (similar to Random Forest) of using multiple algorithms . Below is an example for GridSearch. . Details need to be added later once Random Forest Deep Dive is done. . from sklearn.model_selection import GridSearchCV param_grid = [ {&#39;n_estimators&#39;: [3, 10, 30], &#39;max_features&#39;: [2, 4, 6, 8]}, {&#39;bootstrap&#39;: [False], &#39;n_estimators&#39;: [3, 10], &#39;max_features&#39;: [2, 3, 4]}, ] forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, return_train_score=True) grid_search.fit(housing_prepared, housing_labels) . GridSearchCV(cv=5, error_score=nan, estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid=&#39;deprecated&#39;, n_jobs=None, param_grid=[{&#39;max_features&#39;: [2, 4, 6, 8], &#39;n_estimators&#39;: [3, 10, 30]}, {&#39;bootstrap&#39;: [False], &#39;max_features&#39;: [2, 3, 4], &#39;n_estimators&#39;: [3, 10]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=True, scoring=&#39;neg_mean_squared_error&#39;, verbose=0) . We can get the best values for the hyperparameters as shown below . grid_search.best_params_ . {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} . We can also get the best estimator directly . grid_search.best_estimator_ . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=6, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . feature_importances = grid_search.best_estimator_.feature_importances_ . feature_importances . array([7.55720671e-02, 6.39878625e-02, 4.24072059e-02, 1.82928273e-02, 1.68924417e-02, 1.75601900e-02, 1.66881781e-02, 3.03268232e-01, 6.31565549e-02, 1.08958622e-01, 8.44196144e-02, 8.53515062e-03, 1.73063945e-01, 8.08024120e-05, 2.96250425e-03, 4.15380176e-03]) . You can select this model and test it on the test set . final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(&quot;median_house_value&quot;, axis=1) y_test = strat_test_set[&quot;median_house_value&quot;].copy() X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared) final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) . /usr/local/lib/python3.6/dist-packages/sklearn/compose/_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24. FutureWarning) . print(final_rmse) . 48760.26530172545 . Model can be deployed on web and predict method can be used to evaluate and get the model&#39;s output for new data. .",
            "url": "https://mrg-ai.github.io/blog/2020/08/08/ML-End-To-End-Flow-CAHousingDataset.html",
            "relUrl": "/2020/08/08/ML-End-To-End-Flow-CAHousingDataset.html",
            "date": " • Aug 8, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Machine Learning Intro",
            "content": "This post does not explain what ML is and the usual stuff. Rather it mentions some basic details about ML. This is based on Aurelion’s ML book. . Types of ML . Based on how they are classified, below are some classifications . Whether or not they are trained with human supervision . Supervised . | Unsupervised . | Semi supervised . | Reinforcement Learning . | . | Whether or not they can learn incrementally on the fly . Online . | batch learning . | . | Whether they work by simply comparing new data points to known data points, or instead by detecting patterns in the training data and building a predictive model, much like scientists do . instance-based . | model-based learning . | . | . Common Supervised Learning Algorithms . k-Nearest Neighbors . | Linear Regression . | Logistic Regression . | Support Vector Machines (SVMs) . | Decision Trees and Random Forests . | Neural networks . | . Common Unsupervised Learning Algorithms . Clustering . K-Means . | DBSCAN . | Hierarchical Cluster Analysis (HCA) . | . | Anomaly detection and novelty detection . One-class SVM . | Isolation Forest . | . | Visualization and dimensionality reduction . Principal Component Analysis (PCA) . | Kernel PCA . | Locally Linear Embedding (LLE) . | t-Distributed Stochastic Neighbor Embedding (t-SNE) . | . | Association rule learning . Apriori . | Eclat . | . | . Most semi supervised learning algorithms are combinations of unsupervised and supervised algorithms . For example, deep belief networks (DBNs) are based on unsupervised components called restricted Boltzmann machines (RBMs) stacked on top of one another. RBMs are trained sequentially in an unsupervised manner, and then the whole system is fine-tuned using supervised learning techniques. . Reinforcement Learning Algorithms . There are no common algorithms. These work on Reward and Penalties and the learning happens over time by running it on multiple real-life examples . Algorithms which play Chess or Go are an example . | Programs used in Robots are another example . | . Batch and Online Learning algorithms . As the name says there are algorithms which have to be trained offline i.e. Batch Algorithms and algorithms which can learn on the fly i.e. Online algorithms . Instance vs Model algorithms . Depending on whether the algorithm uses learned instances to predict for new inputs like say Classification Algorithms (k-NearestNeighbors for example) or uses a Model like say Regression Algorithms where you have a line/plane. . Challenges in ML . Bad Data . | Bad Model . | . What to do with the Bad data or Bad model? . Feature Selection . | Feature Extraction . | Regularization . | Hyperparameters . | . Data Load . Below function will be useful to get data from online datasets. . import os . import tarfile . import urllib . DOWNLOAD_ROOT = “https://raw.githubusercontent.com/ageron/handson-ml2/master/” . HOUSING_PATH = os.path.join(“datasets”, “housing”) . HOUSING_URL = DOWNLOAD_ROOT + “datasets/housing/housing.tgz” . def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): . os.makedirs(housing_path, exist_ok=True) . tgz_path = os.path.join(housing_path, “housing.tgz”) . urllib.request.urlretrieve(housing_url, tgz_path) . housing_tgz = tarfile.open(tgz_path) . housing_tgz.extractall(path=housing_path) . housing_tgz.close() . Look at the next few posts on ML End to End process for more information. .",
            "url": "https://mrg-ai.github.io/blog/2020/05/05/Machine-Learning-Intro.html",
            "relUrl": "/2020/05/05/Machine-Learning-Intro.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastai Deep Learning Notes",
            "content": "Deep Learning Notes . . My Notes based on 2019 FastAI course. The code mentioned below will not work on fastai latest version. However the general information mentioned below is valid. . General stuff . Jupyter notebook – Installed through Anaconda. But there are various other ways. . | Google Collab - https://colab.research.google.com/notebooks/welcome.ipynb - recent=true . | Use GPU while running the code. . | GPUs are good at running similar code (in this case mathematical models) multiple times and hence are necessary. CPUs can’t handle or are slow. . | Pytorch, TensorFlow, FastAI, numpy, pandas, matplotlib etc are all Python libraries. Some are deep learning specific and some are math specific. . | Data is stored on google compute VM (Collab’s backend) by default. However we can store on Google Drive and access it from within python code. . | Custom Data can be used and external links (google drive, dropbox etc) will be needed for any proper usage of programs (see lesson 2 notebook in google drive of &lt;emailmanjunathrg@gmail.com)&gt; . | Visual Studio Code can be used to browse through fastai or pytorch classes and understand the library code. . | Render can be used to deploy web apps; Google Compute Engine is another option. . | Further reading - Different types of Models (Resnet, Inception, VGGNet, AlexNet etc) . | . Below lines of code are needed when running notebooks using FastAI. Few are for Google Drive, Library Reloads, Ignore Pytorch related warnings, plotting inline in the notebook . !curl -s https://course.fast.ai/setup/colab  |  bash | . %reload_ext autoreload . %autoreload 2 . %matplotlib inline . import warnings . warnings.filterwarnings(“ignore”, category=UserWarning, module=”torch.nn.functional”) . from google.colab import drive . drive.mount(‘/content/gdrive’, force_remount=True) . root_dir = ”/content/gdrive/My Drive/” . base_dir = root_dir + ’fastai-v3/’ . Below is a javascript to download URLs from google search images into a csv file. I think it should work with any google search result. . Press CtrlShiftJ in Windows/Linux and CmdOptJ in Mac and paste below and enter. A csv file should get downloaded. . urls = Array.from(document.querySelectorAll(‘.rg_di .rg_meta’)).map(el=&gt;JSON.parse(el.textContent).ou); window.open(‘data:text/csv;charset=utf-8,’ + escape(urls.join(‘ n’))); . General Process for Training a Model . Below is a general flow for train a model. This is very general and at a high level. . There can be lots of variations and other steps in between and after. . Get the data with data (like images) and labels. Example command below . Labels can be present in various ways and below command/process will need to be changed accordingly. . data = ImageDataBunch.from_folder(path, train=”.”, valid_pct=0.2, .         ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats) . Train the model using a CNN (convolutional neural network) . learn = cnn_learner(data, models.resnet34, metrics=error_rate) . Fit the data to the curve correctly using appropriate number of epochs . learn.fit_one_cycle(4) . You can interpret the data after this. You can plot confusion matrix or most confused output also. . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix() . Once you think the model has learnt, you can export the model so that it can be used in an application. This will create a file named ‘export.pkl’ in the directory where we were working that contains everything we need to deploy our model (the model, the weights but also some metadata like the classes or the transforms/normalization used). . learn.export() . This export file can be used to deploy on Render.com or Google Compute etc. . Example - . https://github.com/mrg-ai/SouthIndianSnackClassifier . ** DataBlock API** . In the previous section, we used ImageDataBunch.from_folder method to get the “data” which was then passed to a Learner. This was a factory method from FastAI library. It does quite a few things in the backend and also makes some decisions. . We cannot use Factory methods all the time. We will go through the steps that happen in these type of Factory methods and understand the flow. Then we can use those classes and we can have more control. This will also help in understanding what happens to the data before we send it to a learner. Some of these are Pytorch Classes i.e. Dataset, Dataloader and Databunch is a FastAI class . Dataset – This is the first step in getting the data. An object (like image) and its label(s) form a dataset. . Dataloader – A dataset is not enough to train a model. Batches of datasets need to be sent to the model. For creating these batches, we use a Dataloader. . Databunch – It still isn’t enough to train a model, because we’ve got no way to validate the model. If all we have is a training set, then we have no way to know how we’re doing. We need a separate set of held out data, a validation set, to see how we’re getting along. We might also use a test set. . To get the data in a format that we can send to Learner - We use a fastai class called a DataBunch. A DataBunch is something which binds together a training data loader and a validation data loader. . This object can then be sent to a Learner and we can start fitting the data using a proper learning rate, number of epochs, appropriate model etc. . Below is an example for an Image Dataset. . . In this example, ImageFileList.fromfolder creates Datasets using the files which are in a folder with name as “train-jpg” and the files with a suffix of .jpg. The information about the labels are obtained from a csv file and hence it uses .label_from_csv to which we pass the csv file name. . The data is split randomly (80:20 ratio) for training and validation datasets since we do not have them separately in this example. If we do, we should not use this class. We should use .split_by_folder if they are available in different folders. . Then we convert them into Pytorch Datasets using the .datasets . They are transformed using certain transformation rules. . Finally they are converted into a dataloader and eventually a databunch using the .databunch . Below is another example. This is an image example where images of 3 and 7 are stored in folders called 3 and 7. . path = untar_data(URLs.MNIST_TINY) . tfms = get_transforms(do_flip=False) . path.ls() . [PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/valid’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/models’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/train’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/test’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/labels.csv’)] . (path/’train’).ls() . [PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/train/3’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/train/7’)] . data = (ImageFileList.from_folder(path) #Where to find the data? -&gt; in path and its subfolders . .label_from_folder() #How to label? -&gt; depending on the folder of the filenames . .split_by_folder() #How to split in train/valid? -&gt; use the folders . .add_test_folder() #Optionally add a test set . .datasets() #How to convert to datasets? . .transform(tfms, size=224) #Data augmentation? -&gt; use tfms with a size of 224 . .databunch()) #Finally? -&gt; use the defaults for conversion to ImageDataBunch . What kind of data set is this going to be? . It’s going to come from a list of image files which are in some folder. . They’re labeled according to the folder name that they’re in. . We’re going to split it into train and validation according to the folder that they’re in (train and valid). . You can optionally add a test set. We’re going to be talking more about test sets later in the course. . We’ll convert those into PyTorch datasets. . We will then transform them using this set of transforms (tfms), and we’re going to transform into size 224. . Then we’re going to convert them into a data bunch. . In each of these stages inside the parentheses, there are various parameters that you can pass to and customize how that all works. But in the case of something like this MNIST dataset, all the defaults pretty much work and hence no customizations are done. . Multi Label Dataset example . Movie Genres, Satellite Image Descriptions are some examples of Multi label datasets. . Each image can have multiple labels and the labels can repeat across images. . The same approach followed for single label example will work here also. Only thing that will change is the parameters or arguments passed to various classes that we call. For more details - . https://colab.research.google.com/github/fastai/course-v3/blob/master/nbs/dl1/lesson3-planet.ipynb - scrollTo=_bRElObaCKsr . Segmentation . In very simple words, normal image classification for every single pixel in every single image is what is called segmentation. Self Driving cars software use this a lot to differentiate between different things that the car sensors see. . It is also used in medical science for scans, images etc. . For segmentation, we don’t use a convolutional neural network. We can, but actually an architecture called U-Net turns out to be better. Learner.create_unet will be used rather than a cnn. . Look at camvid example in fastai or google drive - https://colab.research.google.com/drive/1O6zJfhQVjnAFMZhi5KG4yl9xrvFHpR5w . Some Learning Rate related notes - . Learning Rate Annealing - idea of decreasing the learning rate during training is an older one. . Recent idea is to keep increasing the learning rate initially and then decreasing so that the model learns quickly or reaches the correct point on the graph quickly. . Classification Problem for Text - NLP (natural language processing) . Text classification is different from images. Images are generally RGB layers and each pixel can be represented by a number and it is easier than it is for Texts. . Texts have to be converted to Numbers before we do any deep learning on it. . The two processes are Tokenization and Numericalization. . Idea for a NLP program - Pass a Hashtag to the program or app. . The App should give a bar graph showing how many of the tweets using the hashtag are Positive and how many are negative. . NLP uses RNN instead of CNN. RNN is Recurrent Neural Network. . Use a Wikipedia Dataset pretrained model to transfer learn for your group/corpus of data -Example – IMDB Movie Review Dataset (which has positive/negative labels). This model predicts the next word in a sentence because we train for language model labels and not on positive/negative labels. However, that is not much useful for a classification problem. We want to classify as Positive or Negative Sentiment. . Language Model Learner has a part called Encoder which stores information about what the model has understood about the statement it was input. Also, there is something called vocab which contains the tokes and numbers for each word. . These will be used further to predict the sentiment in the text classifier model. . We will use this IMBD trained Language Model Learner data’s vocab along with the original IMDB data (positive and negative labels) to create a classification model which can predict the labels for input data. . Tabular Data . Dataframes (panda library) are used to read tabular data from csv, RDBMS, Hadoop etc. . Generally, for Tabular data, Random Forests, Gradient Descent and other algorithms are used, but NNs can also be used. . Note – Go through the Machine Learning course for other non-NN algorithms. . http://course18.fast.ai/lessonsml1/lesson3.html . First 3 chapters give a good understanding of Random Forests. Notes about that below. Apart from RF (even that can be replaced with NNs), other algorithms can all be replaced with NNs. . Some reading - https://www.dataquest.io/blog/top-10-machine-learning-algorithms-for-beginners/ . https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab . Collab Filtering . This is for Recommendation systems like Amazons or Netflix where they suggest that you might like this because you bought this/saw this. . Theory Behind NNs . Dataloaders -&gt; [ Linear Function (Matrix Multiplication) -&gt; Activation Function] -&gt; Next layer … . Back propagation is nothing but . parameters minus= learning rate * parameters.gradient_descent(loss function) i.e. . parameters = parameters minus learning rate * parameters.gradient_descent(loss function) . . Revision of GD (Gradient Descent) (Lesson 2 from the middle of the video) . Yi= a1Xi1+a2Xi2+constant and let’s assume Xi2=1 for simplicity. . If the above equation is executed for different “i” it will become a matrix multiplication. . . Using pytorch above equation can be written as below. (X first column is Random numbers) . . Tensor is a regular sized (non-ragged) array. It can be rectangular (2D) or a Cube (3D) or more dimensions. But it is always a regular size. . An example – Image is a Rank 3 tensor since it has 3 layers (RGB). . If we plot the y function from screenshot above . . If we want to fit a line through these points without knowing the coefficients 3 and 2 i.e. Tensor A, we start with some random values and we can plot the line. . We can move around the line by using the derivative of the Loss (in this case MSE) and see how Y changes. . MSE – (y_hat(prediction)-y(actual))**2.mean() . Loss is nothing but the distance of the line from the dots. If we reduce the loss, the line will match the dots and go through them thereby keeping the loss at a minimum. . . The gradient/derivative is used along with learning rate to change the value of the co-efficients and bring the line closer to where we want. . . Weight Decay – All learners have a weight decay parameters in fastai and a value of 0.1 should help in general. . W- Weights/Parameters . L(x,w) – Loss function . M(x,w) – y_hat . The box is same as . parameters = parameters minus learning rate * parameters.gradient_descent(loss function) . . Momentum – The update to the parameters/weights is not just based on the derivative, instead 1/10 of it is the derivative and 90% of it is just the same direction that we went last time. This is exponentially weighted moving average of the step. . Generally, a momentum of 0.9 is used with SGD if we want to use momentum. . RMSProp – This is similar to Momentum but it is exponentially weighted moving average of the gradient squared. . Adam is Momentum + RMSProp – https://github.com/hiromis/notes/blob/master/Lesson5.md . Cross Entropy Loss – Loss used for Single Label Multi class classification problem . SoftMax Activation function – Activation function used in Single Label Multi class classification problems. . RESNET – Residual Net. The input from previous layer is added to this layer’s output. In other words the inputs skips this convolution (skip connection). Added is a + here. This is the basic theory of Resnet. . DenseNet – Same as resnet but instead of a +, a concat is used to concatenate previous layer’s input and its output and that is passed to this layer. It is memory intensive but works well for smaller datasets. . UNet – As the name says, it is in a U shape. It can be thought of Resnet from halfpoint onwards. . Example - First layer output is skipping all layers and directly going as input to last layer. . . Nearest Neighbor interpolation . Bi Linear interpolation – In layman terms - Techniques to increase the resolution sizes of image inputs. . CNN – . The matrix multiplications in a NN are replaced with a special Matrix multiplication in case of CNNs and they are called Convolutions. . Conv Kernel multiplied by Input matrix to get one single number. Kernel Matrix can be considered to be a matrix of weights which are tied at certain places and also is sparse. . By doing this, we achieve identifying different parts of an image and later it can all be tied together to identify the whole image. . . For example – a 3x3 matrix multiplied to another 3x3 gives a 3x3 in normal matrix multiplication. However, convolution will only give a single value. . Therefore, it is generally padded with zeroes to as shown below. . Note : This is 2D example, However images are generally 3D and the same idea extends to that as well. You will have more conv kernels and so on. For images, it could be 3 conv kernels to start with along with some kind of padding to increase the size of the Tensor. However practically, there are more kernels used even during start. . . However, we don’t use Matrix multiplication since its slow and the matrix is anyways sparse with many zeroes. . There are functions which do this called Convolution functions. . Stride 2 Convolution – Same as above, but skip one layer of cells or columns in matrix when moving to next section. After the left top corner is done, you skip to top right corner in above example instead of the middle 3 columns. This reduces the height and width of output matrix but we generally increase the kernels and the depth (channels) actually increases. . Average Pooling – After multiple layers, we will have more number of channels and some small height and width. What we need is Category of the image in a Image classifier. There could be say 5 categories. To get to this point, we do various things and one of the first things is Average pooling. It basically takes average of each channel. If the final layer gave an output of say 20 layers, we take average of each layer and get one value for each layer. These are stacked in a column vector of size 20. This is still more than the categories we expect. . This is passed through a linear layer to get the 5 categories that we want. One of the categories in this vector should have a high probability value based on which the prediction can be obtained. . ResNet – See above. . DenseNet – See above. . UNET – See above. . UNET gives an output image of same size as input. The down sampling reduces the image size for sure and we know that. For increasing the size by final layer, it does a Stride ½ convolution. . Apart from padding on the perimeter, it also adds padding in between as shown below. . The 2x2 blue colored pixels are original image. Remaining are all padding. . . This is slightly older where all the padded pixels are basically zeros or white pixels. . Below is what is done now to do up-sampling. This is called Nearest Neighbor interpolation. . Bilinear Interpolation is similar but takes a weighted average of the surrounding neighbors. . Techniques like above are used in up sampling path of the UNET. However the down sampling would have reduced the size and up sampling only from that will be not useful. . A skip connection or identity connection is added in up direction where the inputs of the down sampling path are added as inputs. See UNET diagram above. . . GAN . . RNN . Inputs – Get the inputs as Tensors . | Weights/Parameters/Coefficients – Multiply with Random weights or Pretrained weights. This is Linear computation i.e. Linear layer or Affine Layer. . | Activations – The output of Affine function are also Activations. But they are Linear activations. Pass the output of previous step through a non linear activation function like ReLU or Softmax. This is the non-linearity in the NeuralNet. . Activation Function – ReLU, Softmax, Sigmoid and many others. | . | Output – The output is obtained. . | Layers – Each of these is a layer . | Loss – Compare the output with actual output and calculate the Loss. MSE, RMSE, Log(MSE) etc are some of the Loss functions. . | . L(x,w) = MSE(m(x,w),y) +WD* Sum(W**2) . Since we calculate the gradient of a Loss function, that calculates the gradient of the WD*(Sum(W**2)) . Adding the +WD* Sum(W**2) to Loss function is called L2 regularization. . The gradient of WD*(Sum(W**2)) which is used in Back propagation (params = params-LR*gradient (Loss function) i.e. 2*WD*W (generalized to WD*W) is called Weight Decay. . Back propagation is nothing but parameters = parameters minus learning rate * parameters.gradient_descent(loss function) . | One Hot Encoded Matrix – This is done to preprocess the input data that is fed to a neural net. This is helpful to pass the data into NN in a common format of Matrices. One matrix of One Hot Encoded Matrix and the other input Random Weights Matrix. . | Embedding – Array Lookup or Matrix Multiply by One hot encoded matrix. . | Latent features or Latent Factors – Insights that NNs can give. Embeddings also give latent features in Collab learning. . | N_factors in collab problems - Size of the embedding matrix. . | Weight Decay - Weight Decay is a type of Regularization. Weight decay is a number used to multiply the parameters during Loss calculation. We should use many parameters and along with that use weight decay. General value should be 0.1 . | . Parameters at time/epoch T = parameters at epoch T-1 – LR*derivative (loss with respect to parameters at T-1) . Adam – Momentum+RMS Prop . Momentum - parameters = parameters minus learning rate * [ x% (parameters.gradient_descent(loss function)) + (100-x)% (previous derivative)] . Or it can also be written as Step T = alpha*gradient + (1-alpha)*Step T-1 . This second part of 1-alpha is called Exponentially weighted moving average of past gradients. Step T-1 inturn uses Step T-2. So (1-alpha) gets multiplied multiple times. | . | RMS Prop - Exponentially weighted moving average of past gradient square and use that to divide as shown below . | . | . parameters = parameters minus learning rate *{ [ x% (parameters.gradient_descent(loss function)) + (100-x)% (previous derivative)]}/{ Exponentially weighted moving average of past gradient square} . Metric . | User Bias and Item Bias – Terms in Collab which highlight the biases that the user (like customer id, user_id) can have and the biases of the Item (like movie (popular movie) or product (star product like Apple devices)) . | Cross-Entropy – Loss function where we want a single value to be selected as output rather than probability of being close to the output. That is why it’s used in Single Label Multi class classification problems. . Loss cannot be MSE for learners which predict individual classes. MSE is more suited for finding a number on a graph kinda problems. . | Cross Entropy is a Loss function which provides low loss when the prediction is correct and its confident and high loss for wrong predictions with high confidence. . | . | Softmax – For Cross Entropy Loss function to work correctly and give positive probability and to be sure that sum of probabilities for possible values to be less than 1, the activation function to be used along with this Loss function is Softmax function. . | Dropout – Dropout is a type of regularization. Some activations are thrown out in each mini batch. We keep lot of parameters but throw away some activation randomly in each mini batch so that it does not overfit. . | BatchNorm – Generally for continuous variables. Used for most cases. This is basically to scale up the output. For example if the range of values we expect is 1 to 5, but the NN gives -1 to 1, using batch norm layer would help to normalize it.. . Yhat = f(w0, w1, w2…. Wn, X)*g + b . | Loss = Sum (y-yhat)**2 . | . | WeightNorm is another new normalization used in Fastai. Recent thing.. . | Data Augmentation - Modify model inputs during training in order to effectively increase data size. For examples - Images can be flipped or warped or perspective changes etc to basically convert one image into multiple images which look different from one another. . | Label_Cls option in Tabular - Used to pass options like Output variable to be considered a Float, Take Log of it while creating labels for it etc.. . | Tabular Architecture - Mostly Embeddings of many layers. . | Data Augmentation . | Fine Tuning . | Layer Deletion and Random weights . | Freezing and Unfreezing . | .",
            "url": "https://mrg-ai.github.io/blog/2020/01/12/FastAI-Deep-Learning-Notes.html",
            "relUrl": "/2020/01/12/FastAI-Deep-Learning-Notes.html",
            "date": " • Jan 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Linear Regression using Pytorch from Scratch",
            "content": "We will start with Numpy and then look at Pytorch . import numpy as np . np.random.seed(42) x = np.random.rand(100, 1) # Below equation is y = a + bx + noise where a = 1 and b = 2 y = 1 + 2 * x + .1 * np.random.randn(100, 1) # Shuffles the indices idx = np.arange(100) np.random.shuffle(idx) # Uses first 80 random indices for train train_idx = idx[:80] # Uses the remaining indices for validation val_idx = idx[80:] # Generates train and validation sets x_train, y_train = x[train_idx], y[train_idx] x_val, y_val = x[val_idx], y[val_idx] . x_train[:5], y_train[:5] . (array([[0.77127035], [0.06355835], [0.86310343], [0.02541913], [0.73199394]]), array([[2.47453822], [1.19277206], [2.9127843 ], [1.07850733], [2.47316396]])) . print(type(x_train)) . &lt;class &#39;numpy.ndarray&#39;&gt; . Above code cells are to create the data as Numpy arrays. This is the same data that we created in LR model with Numpy. https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy . We will convert the numpy arrays into Tensors in below cells so that we can use them with Pytorch. . !pip install torchviz . Collecting torchviz Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB) |████████████████████████████████| 51kB 2.9MB/s Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.3.1) Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-&gt;torchviz) (1.17.4) Building wheels for collected packages: torchviz Building wheel for torchviz (setup.py) ... done Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3520 sha256=28d2000b9a7b828cc19400716528eb1a55cf9aadeea8860bf65c75189b590f79 Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667 Successfully built torchviz Installing collected packages: torchviz Successfully installed torchviz-0.0.1 . import torch import torch.optim as optim import torch.nn as nn from torchviz import make_dot #Setting the Device to CPU or GPU based on the availability device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; . print(device) #Since we are using GPU here, devide shows as cuda. Otherwise it would show as cpu. . cuda . # For using with Pytorch, we need to have PyTorch&#39;s Tensors # x_train_tensor = torch.from_numpy(x_train).float().to(device) y_train_tensor = torch.from_numpy(y_train).float().to(device) . print(type(x_train), type(x_train_tensor), x_train_tensor.type()) . &lt;class &#39;numpy.ndarray&#39;&gt; &lt;class &#39;torch.Tensor&#39;&gt; torch.cuda.FloatTensor . We can see x_train_tensor is now a Tensor and .type() shows that it is a Tensor which is on GPU and is of Float datatype . # This is similar to what we did in Numpy. # We set REQUIRES_GRAD = TRUE because we want to calculate partial derivatives or gradients on these parameters a = torch.randn(1, requires_grad=True, dtype=torch.float) b = torch.randn(1, requires_grad=True, dtype=torch.float) print(a, b) . tensor([-0.0125], requires_grad=True) tensor([-0.2016], requires_grad=True) . This looks good. These are Tensors and can be used with Pytorch. Is it ?? . a.type() . &#39;torch.FloatTensor&#39; . This is a Float Tensor but resides on CPU. To use GPU, we need to push it to GPU. . a = a.to(device) b = b.to(device) . a.type() . &#39;torch.cuda.FloatTensor&#39; . b.type() . &#39;torch.cuda.FloatTensor&#39; . Okay. Now we have pushed them to GPU. . a, b . (tensor([-0.0125], device=&#39;cuda:0&#39;, grad_fn=&lt;CopyBackwards&gt;), tensor([-0.2016], device=&#39;cuda:0&#39;, grad_fn=&lt;CopyBackwards&gt;)) . What happened to requires_grad=True !? Looks like we lost it when we moved to GPU. So, this is not the right approach to create Tensors on GPU which require gradient descent . a = torch.randn(1, dtype=torch.float).to(device) b = torch.randn(1, dtype=torch.float).to(device) . a.requires_grad_() b.requires_grad_() a, b . (tensor([-0.4192], device=&#39;cuda:0&#39;, requires_grad=True), tensor([-0.1465], device=&#39;cuda:0&#39;, requires_grad=True)) . However the best way is to create the Tensors right away on GPU instead of all this moving around. . The output of this and above cells is essentially the same, but this is a best practice and straightforward. . torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) a, b . (tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True), tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)) . Okay.. Now we have a and b Tensors on GPU.. What next ? Start Forward Pass -&gt; calculate Loss -&gt; calculate gradients -&gt; do backward pass and repeat it for number of epochs ?? . lr = 1e-1 n_epochs = 1000 # set a learning rate and number of epochs . In this next section we will run a for loop and calculate the predictions, error and MSE loss similar to what we did before in LR with Numpy exercise. . However we will not calculate the gradient by hand. Rather we will use Pytorch&#39;s autograd function and its capabilities. Autograd is PyTorch’s automatic differentiation package and it will take care of calculating Partial Derivatives, Chain rule of derivatives where we multiply different partial derivatives to get a final derivative and so on. . It will also keep accumulating the gradient for each change of that object. We have set a and b to requires_grad=True. Therefore gradients will be calculated for them by Pytorch. . for epoch in range(n_epochs): yhat = a + b * x_train_tensor error = y_train_tensor - yhat loss = (error ** 2).mean() # Lets print a and b before the backward pass. print(&quot;epoch number =&quot;, epoch) print(&quot;a and b before backward pass&quot; , a,b) # We will not do this now like we did in Numpy notebook # - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb # a_grad = -2 * error.mean() # b_grad = -2 * (x_tensor * error).mean() # We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. loss.backward() # Let&#39;s check the computed gradients... type(a.grad) print(&quot;a&#39;s gradient = &quot; , a.grad) print(&quot;b&#39;s gradient = &quot; , b.grad) # Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function) a = a - lr * a.grad b = b - lr * b.grad print(&quot;a and b after backward pass&quot; , a,b) . epoch number = 0 a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-3.3881], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.9439], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;) tensor([0.3335], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;) epoch number = 1 a and b before backward pass tensor([0.5328], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;) tensor([0.3335], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;) a&#39;s gradient = None b&#39;s gradient = None . TypeError Traceback (most recent call last) &lt;ipython-input-42-ee387fc7ae7e&gt; in &lt;module&gt;() 22 # Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function) 23 &gt; 24 a = a - lr * a.grad 25 b = b - lr * b.grad 26 print(&#34;a and b after backward pass&#34; , a,b) TypeError: unsupported operand type(s) for *: &#39;float&#39; and &#39;NoneType&#39; . Well.. It did not work. a.grad and b.grad became None after epoch 0 and the parameter update statement fails for epoch 1. What happened ? . epoch number = 0 a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) . See what happened after backward pass during epoch 0. The requires_grad=True vanished again after a and b were reassigned new values. This is similar to what happened before when we moved a and b from CPU to GPU. See here. Link to the cell - https://colab.research.google.com/drive/1OBbzaMpf33M8Fc-Z1yY0jqFKLScvYgco#scrollTo=mp9lvYynHw3U . a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, grad_fn=) tensor([0.3335], device=&#39;cuda:0&#39;, grad_fn=)&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; print(a, b) . tensor([0.5328], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;) tensor([0.3335], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;) . . torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) a, b . (tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True), tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)) . print(a, b) . tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) . . for epoch in range(n_epochs): yhat = a + b * x_train_tensor error = y_train_tensor - yhat loss = (error ** 2).mean() # Lets print a and b before the backward pass. print(&quot;epoch number =&quot;, epoch) print(&quot;a and b before backward pass&quot; , a,b) # We will not do this now like we did in Numpy notebook # - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb # a_grad = -2 * error.mean() # b_grad = -2 * (x_tensor * error).mean() # We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. loss.backward() # Let&#39;s check the computed gradients... type(a.grad) print(&quot;a&#39;s gradient = &quot; , a.grad) print(&quot;b&#39;s gradient = &quot; , b.grad) # Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function) a-= lr * a.grad b.sub_(lr * b.grad) print(&quot;a and b after backward pass&quot; , a,b) . epoch number = 0 a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-3.3881], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.9439], device=&#39;cuda:0&#39;) . RuntimeError Traceback (most recent call last) &lt;ipython-input-51-83082fa8169f&gt; in &lt;module&gt;() 22 # Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function) 23 &gt; 24 a-= lr * a.grad 25 b.sub_(lr * b.grad) 26 print(&#34;a and b after backward pass&#34; , a,b) RuntimeError: a leaf Variable that requires grad has been used in an in-place operation. . We used the inplace value substitution instead of assigning to a and b. There are two ways to do it and both are shown above i.e. a-= or b.sub_ . It does not even finish epoch 0. It fails when assigning the value. This is because of Pytorch&#39;s Dynamic Computational Graph Basically since required_grad is set to True for a and b, pytorch is calculating gradients and storing them for each change to a and b. . Since we are trying to change the same object using its own value, it throws an error. We need to basically somehow tell Pytorch not to calculate gradient for a and b when we are doing this parameter update step. . . torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) a, b . (tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True), tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)) . print(a, b) . tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) . Lets try something else now. We will tell Pytorch not to compute gradients when we are doing the parameter update. . for epoch in range(n_epochs): yhat = a + b * x_train_tensor error = y_train_tensor - yhat loss = (error ** 2).mean() # Lets print a and b before the backward pass for certain epochs only. if(epoch%100==0): print(&quot;epoch number =&quot;, epoch) print(&quot;a and b before backward pass&quot; , a,b) # We will not do this now like we did in Numpy notebook # - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb # a_grad = -2 * error.mean() # b_grad = -2 * (x_tensor * error).mean() # We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. loss.backward() # Let&#39;s check the computed gradients... # Lets print the gradients for certain epochs only. if(epoch%100==0): print(&quot;a&#39;s gradient = &quot; , a.grad) print(&quot;b&#39;s gradient = &quot; , b.grad) # Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function) # We are telling Pytorch not to calculate gradients for steps within the &quot;with&quot; # torch.no_grad() allows us to perform regular Python operations on tensors, independent of PyTorch’s Dynamic computation graph. with torch.no_grad(): a-= lr * a.grad b.sub_(lr * b.grad) # Lets print a and b after the backward pass for certain epochs only. if(epoch%100==0): print(&quot;a and b after backward pass&quot; , a,b) . epoch number = 0 a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-3.3881], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.9439], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 100 a and b before backward pass tensor([0.2229], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1810], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-4.1505], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.8718], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.6379], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3682], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 200 a and b before backward pass tensor([0.2240], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3068], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-4.8702], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.7955], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.7110], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.4864], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 300 a and b before backward pass tensor([0.2004], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.5092], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-5.5295], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.7378], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.7533], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.6830], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 400 a and b before backward pass tensor([0.1578], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.7758], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-6.1127], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.7185], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.7690], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.9476], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 500 a and b before backward pass tensor([0.1038], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.0900], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-6.6066], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.7537], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.7645], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.2654], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 600 a and b before backward pass tensor([0.0476], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.4321], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-7.0014], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.8537], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.7478], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.6175], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 700 a and b before backward pass tensor([-0.0010], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7809], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-7.2911], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-2.0226], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.7281], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9832], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 800 a and b before backward pass tensor([-0.0321], device=&#39;cuda:0&#39;, requires_grad=True) tensor([2.1147], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-7.4737], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-2.2580], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.7153], device=&#39;cuda:0&#39;, requires_grad=True) tensor([2.3405], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 900 a and b before backward pass tensor([-0.0366], device=&#39;cuda:0&#39;, requires_grad=True) tensor([2.4134], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-7.5511], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-2.5508], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.7185], device=&#39;cuda:0&#39;, requires_grad=True) tensor([2.6685], device=&#39;cuda:0&#39;, requires_grad=True) . This ran successfully. But the values of a and b are wrong at 900th epoch and is getting worse. Our results from numpy exercise were very close to actual values after 1000 epochs. This is another issue due to Pytorch&#39;s computational graph. It keeps accumulating the gradient values. We need to tell it to let it go after an epoch. . . torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) a, b . (tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True), tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)) . print(a, b) . tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) . Lets try something else now. We will reset the grads to zero after each epoch . for epoch in range(n_epochs): yhat = a + b * x_train_tensor error = y_train_tensor - yhat loss = (error ** 2).mean() # Lets print a and b before the backward pass for certain epochs only. if(epoch%100==0): print(&quot;epoch number =&quot;, epoch) print(&quot;a and b before backward pass&quot; , a,b) # We will not do this now like we did in Numpy notebook # - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb # a_grad = -2 * error.mean() # b_grad = -2 * (x_tensor * error).mean() # We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. loss.backward() # Let&#39;s check the computed gradients... # Lets print the gradients for certain epochs only. if(epoch%100==0): print(&quot;a&#39;s gradient = &quot; , a.grad) print(&quot;b&#39;s gradient = &quot; , b.grad) # Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function) # We are telling Pytorch not to calculate gradients for steps within the &quot;with&quot; with torch.no_grad(): a-= lr * a.grad b.sub_(lr * b.grad) # Lets print a and b after the backward pass for certain epochs only. if(epoch%100==0): print(&quot;a and b after backward pass&quot; , a,b) # We reset it to zero after each epoch a.grad.zero_() b.grad.zero_() # Lets print a and b after the backward pass and after reset to zero for certain epochs only. if(epoch%100==0): print(&quot;a and b after backward pass after resetting grad to zero&quot; , a,b) . epoch number = 0 a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-3.3881], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.9439], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 100 a and b before backward pass tensor([1.1479], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7257], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0188], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0367], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 200 a and b before backward pass tensor([1.0507], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9159], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0041], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0080], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 300 a and b before backward pass tensor([1.0295], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9574], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0009], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0018], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 400 a and b before backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9664], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0002], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0004], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 500 a and b before backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([4.2574e-05], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-8.3295e-05], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 600 a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([9.3249e-06], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.8163e-05], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 700 a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([1.9097e-06], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-4.1103e-06], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 800 a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([5.1083e-07], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-8.8313e-07], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 900 a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([5.4762e-07], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-5.7358e-07], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) . This looks good. Compare the gradient values for each epoch for this loop vs the one before. The gradients keep improving here and the values of a and b keep coming closer to actual values. . a, b . (tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True), tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)) . The predicted values are close to actual values. . Dynamic Computational Graph . Now a and b are not messed up. But I am resetting them to check the Dynamic Computational Graph . torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) a, b . (tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True), tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)) . print(a, b) . tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) . Lets manually calculate the prediction, error and loss for epoch 0 . yhat = a + b * x_train_tensor error = y_train_tensor - yhat loss = (error ** 2).mean() . make_dot(yhat) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 139624134240632 AddBackward0 139624134241080 (1) 139624134241080&#45;&gt;139624134240632 139624134239400 MulBackward0 139624134239400&#45;&gt;139624134240632 139624134240968 (1) 139624134240968&#45;&gt;139624134239400 make_dot(error) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 139624134240408 SubBackward0 139624134240632 AddBackward0 139624134240632&#45;&gt;139624134240408 139624134241080 (1) 139624134241080&#45;&gt;139624134240632 139624134239400 MulBackward0 139624134239400&#45;&gt;139624134240632 139624134240968 (1) 139624134240968&#45;&gt;139624134239400 make_dot(loss) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; %3 139624133776552 MeanBackward0 139624133776160 PowBackward0 139624133776160&#45;&gt;139624133776552 139624133775656 SubBackward0 139624133775656&#45;&gt;139624133776160 139624133776440 AddBackward0 139624133776440&#45;&gt;139624133775656 139624133777056 (1) 139624133777056&#45;&gt;139624133776440 139624133777112 MulBackward0 139624133777112&#45;&gt;139624133776440 139624133777280 (1) 139624133777280&#45;&gt;139624133777112 Blue box - This is for the tensors that are used as parameters i.e. a and b and we have set requires_grad = True for them. . Grey box - This indicates some operation that involves a gradient-computing (like a or b) tensor and/or its dependencies. . Green box - This indicates the starting point of the gradient calculation for the backward pass. This and grey boxes are the places where gradients are calculated. . Note that there are no special mentions for x in the forward pass of the graph because it is not a tensor for which we compute gradients. . Lets torch it more.. We used pytorch to calculate the gradients before using the .backward function. . Lets use pytorch now to do the parameters = parameters - learning rate * gradients which is updating the weights or parameters. . Generally we do not update the weights/parameters manually since there will be a large number of them. We have to use the optim class of Pytorch. . In below code, we are basically using the optim code to update parameters through SGD (in this case since entire batch of data is used it works as Batch Gradient Descent). In real life, this would actually work as a Mini Batch Gradient Descent since mini batches of data are sent at a time in each epoch. . torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) a, b lr = 1e-1 n_epochs = 1000 # Define a SGD optimizer to update the parameters optimizer = optim.SGD([a, b], lr=lr) for epoch in range(n_epochs): yhat = a + b * x_train_tensor error = y_train_tensor - yhat loss = (error ** 2).mean() # Lets print a and b before the backward pass for certain epochs only. if(epoch%100==0): print(&quot;epoch number =&quot;, epoch) print(&quot;a and b before backward pass&quot; , a,b) # We will not do this now like we did in Numpy notebook # - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb # a_grad = -2 * error.mean() # b_grad = -2 * (x_tensor * error).mean() # We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. loss.backward() # Let&#39;s check the computed gradients... # Lets print the gradients for certain epochs only. if(epoch%100==0): print(&quot;a&#39;s gradient = &quot; , a.grad) print(&quot;b&#39;s gradient = &quot; , b.grad) # Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function) # We are telling Pytorch not to calculate gradients for steps within the &quot;with&quot; #with torch.no_grad(): # a-= lr * a.grad # b.sub_(lr * b.grad) # We will use optimizer to update the parameters. optimizer.step() # Lets print a and b after the backward pass for certain epochs only. if(epoch%100==0): print(&quot;a and b after backward pass&quot; , a,b) # We reset it to zero after each epoch #a.grad.zero_() #b.grad.zero_() # We will use optimizer to reset the gradients to zero. optimizer.zero_grad() # Lets print a and b after the backward pass and after reset to zero for certain epochs only. if(epoch%100==0): print(&quot;a and b after backward pass after resetting grad to zero&quot; , a,b) . epoch number = 0 a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-3.3881], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.9439], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 100 a and b before backward pass tensor([1.1479], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7257], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0188], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0367], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 200 a and b before backward pass tensor([1.0507], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9159], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0041], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0080], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 300 a and b before backward pass tensor([1.0295], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9574], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0009], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0018], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 400 a and b before backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9664], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0002], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0004], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 500 a and b before backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([4.2574e-05], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-8.3295e-05], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 600 a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([9.3249e-06], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.8163e-05], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 700 a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([1.9097e-06], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-4.1103e-06], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 800 a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([5.1083e-07], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-8.8313e-07], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 900 a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([5.4762e-07], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-5.7358e-07], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward pass after resetting grad to zero tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) . The output looks good and we used Pytorch&#39;s optim to update parameters and also reset the gradient after each epoch. Good going..! . What else is our handwritten code ? Prediction expression, Error and Loss calculations. . Lets work on Loss and Error first. . Pytorch has many Loss functions and we have to use the appropriate ones based on the need. . In this example, we are using MSE and we can use nn.MSELoss class for creating the loss function. . torch.manual_seed(42) a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device) a, b lr = 1e-1 n_epochs = 1000 # Define a SGD optimizer to update the parameters optimizer = optim.SGD([a, b], lr=lr) loss_func = nn.MSELoss(reduction=&#39;mean&#39;) # reduction = &#39;mean&#39; basically says you aggregate the losses by mean. # reduction can also be sum in which case it is SSE. Sum Squared Error. for epoch in range(n_epochs): yhat = a + b * x_train_tensor # error = y_train_tensor - yhat #loss = (error ** 2).mean() # We commented the loss calculation and error calculation since # both will be taken care of by loss_func defined above. loss = loss_func(y_train_tensor, yhat) # Lets print a and b before the backward pass for certain epochs only. if(epoch%100==0): print(&quot;epoch number =&quot;, epoch) print(&quot;a and b before backward pass&quot; , a,b) # We will not do this now like we did in Numpy notebook # - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb # a_grad = -2 * error.mean() # b_grad = -2 * (x_tensor * error).mean() # We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. loss.backward() # Let&#39;s check the computed gradients... # Lets print the gradients for certain epochs only. if(epoch%100==0): print(&quot;a&#39;s gradient = &quot; , a.grad) print(&quot;b&#39;s gradient = &quot; , b.grad) # Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function) # We are telling Pytorch not to calculate gradients for steps within the &quot;with&quot; #with torch.no_grad(): # a-= lr * a.grad # b.sub_(lr * b.grad) # We will use optimizer to update the parameters. optimizer.step() # Lets print a and b after the backward pass for certain epochs only. if(epoch%100==0): print(&quot;a and b after backward pass&quot; , a,b) # We reset it to zero after each epoch #a.grad.zero_() #b.grad.zero_() # We will use optimizer to reset the gradients to zero. optimizer.zero_grad() # Lets print a and b after the backward pass and after reset to zero for certain epochs only. if(epoch%100==0): print(&quot;a and b after backward and zeroing grad&quot; , a,b) . epoch number = 0 a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([-3.3881], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.9439], device=&#39;cuda:0&#39;) a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 100 a and b before backward pass tensor([1.1479], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7257], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0188], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0367], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 200 a and b before backward pass tensor([1.0507], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9159], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0041], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0080], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 300 a and b before backward pass tensor([1.0295], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9574], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0009], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0018], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 400 a and b before backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9664], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([0.0002], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-0.0004], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 500 a and b before backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([4.2574e-05], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-8.3295e-05], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 600 a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([9.3249e-06], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-1.8163e-05], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 700 a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([1.9097e-06], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-4.1103e-06], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 800 a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([5.1083e-07], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-8.8313e-07], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) epoch number = 900 a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a&#39;s gradient = tensor([5.4762e-07], device=&#39;cuda:0&#39;) b&#39;s gradient = tensor([-5.7358e-07], device=&#39;cuda:0&#39;) a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) a and b after backward and zeroing grad tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True) . Alright.. Loss calculated using Pytorch. What next ? . Lets tackle the remaining manually written expression. . The Prediction logic or the Model . For the model, we need to use the Module Class of Pytorch. We should create a new class which inherits from the Module class. . It needs to have two mandatory methods apart from any other methods as required for the problem at hand - init(self) - As the name says this is for the initialization. Setting up of the parameters. In this case of LR, we have a and b . forward(self, x) - The method which does the actual prediction logic. . Pytorch Module class is defined such that the forward method works as the core method of the class. . When the class is called like a function, the forward method is invoked. . class MyLRModel(nn.Module): def __init__(self): super().__init__() # To make &quot;a&quot; and &quot;b&quot; real parameters of the model, we need to wrap them in nn.Parameter self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float)) self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float)) def forward(self, x): # Computes the predictions and in this case it is a + bx return self.a + self.b * x . #Keeping the seed same as before. This has to be done before defining the model. torch.manual_seed(42) # Now we can create a model and send it to the device. Model should be on the same device as the parameters. model = MyLRModel().to(device) # To print the values of a and b , we use state_dict() method. print(model.state_dict()) lr = 1e-1 n_epochs = 1000 # Define a SGD optimizer to update the parameters. Note that parameters are now passed via model.parameters() and not manually. optimizer = optim.SGD(model.parameters(), lr=lr) loss_func = nn.MSELoss(reduction=&#39;mean&#39;) # reduction = &#39;mean&#39; basically says you aggregate the losses by mean. # reduction can also be sum in which case it is SSE. Sum Squared Error. for epoch in range(n_epochs): # This is to tell Pytorch that model should be in training mode at this time and not evaluation mode. # The model behavior might change in different modes. model.train() yhat = model(x_train_tensor) loss = loss_func(y_train_tensor, yhat) loss.backward() optimizer.step() optimizer.zero_grad() # To print the values of a and b after all epochs, we use state_dict() method. print(model.state_dict()) . OrderedDict([(&#39;a&#39;, tensor([0.3367], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([0.1288], device=&#39;cuda:0&#39;))]) OrderedDict([(&#39;a&#39;, tensor([1.0235], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([1.9690], device=&#39;cuda:0&#39;))]) . Output same as before. We have 5 statements in the loop. . yhat = model(x_train_tensor) loss = loss_func(y_train_tensor, yhat) loss.backward() optimizer.step() optimizer.zero_grad() . Can we refactor these ? . Lets refactor the training steps. Here, we just put all the code that kept repeating in the epoch loop in a method . def make_train_step(model, loss_fn, optimizer): # Builds function that performs a step in the train loop def train_step(x, y): # Sets model to training mode model.train() # Makes predictions yhat = model(x) # Computes loss loss = loss_func(y, yhat) # Computes gradients loss.backward() # Updates parameters and zeroes gradients optimizer.step() optimizer.zero_grad() # Returns the loss return loss.item() # Returns the function that will be called inside the train loop return train_step # Creates the train_step function for our model, loss function and optimizer train_step = make_train_step(model, loss_func, optimizer) losses = [] # For each epoch... for epoch in range(n_epochs): # Performs one train step and returns the corresponding loss loss = train_step(x_train_tensor, y_train_tensor) losses.append(loss) # Checks model&#39;s parameters print(model.state_dict()) . OrderedDict([(&#39;a&#39;, tensor([1.0235], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([1.9690], device=&#39;cuda:0&#39;))]) . In real life, data will not be so simple. . Therefore we pass data as (features, labels) to Pytorch . features are inputs and labels are outputs. . For this we use the Dataset class of Pytorch. Below we see a custom Dataset class that we defined by inheriting the Pytorch&#39;s Dataset class as well as TensorDataset class of Pytorch . Any dataset returns three things, (x,y) , a way to index through x,y and length(x) . from torch.utils.data import Dataset, TensorDataset class CustomDataset(Dataset): def __init__(self, x_tensor, y_tensor): self.x = x_tensor self.y = y_tensor def __getitem__(self, index): return (self.x[index], self.y[index]) def __len__(self): return len(self.x) x_train_tensor = torch.from_numpy(x_train).float() y_train_tensor = torch.from_numpy(y_train).float() train_data = CustomDataset(x_train_tensor, y_train_tensor) print(train_data[0]) type(train_data) . (tensor([0.7713]), tensor([2.4745])) . __main__.CustomDataset . train_data = TensorDataset(x_train_tensor, y_train_tensor) print(train_data[0]) type(train_data) . (tensor([0.7713]), tensor([2.4745])) . torch.utils.data.dataset.TensorDataset . The outputs of our Custom Dataset class and TensorDataset class are the same. This is because we are doing exactly what the TensorDataset will do although our class will not have other bells and whistles which TensorDataset class will have. . Okay.. What next ? . len(x_train_tensor) , type(x_train_tensor), len(train_data), type(train_data) . (80, torch.Tensor, 80, torch.utils.data.dataset.TensorDataset) . len(x_val), type(x_val) . (20, numpy.ndarray) . x_valid_tensor = torch.from_numpy(x_val).float() y_valid_tensor = torch.from_numpy(y_val).float() . valid_data = TensorDataset(x_valid_tensor, y_valid_tensor) . len(x_valid_tensor), type(x_valid_tensor), len(valid_data), type(valid_data) . (20, torch.Tensor, 20, torch.utils.data.dataset.TensorDataset) . In the above cells, we created Train and Valid Datasets using Pytorch TensorDataset class . Also note that these Datasets are on cpu and not the GPU. We will get to know the reason for this once we understand Dataloaders . Before that lets try to push the dataset to GPU.. . train_data.to(device) . AttributeError Traceback (most recent call last) &lt;ipython-input-82-9b29ccbff29d&gt; in &lt;module&gt;() -&gt; 1 train_data.to(device) AttributeError: &#39;TensorDataset&#39; object has no attribute &#39;to&#39; . Ahh... It fails.. . Lets see what these Dataloaders are. Generally data is sent to GPU and to the training loop in mini batches. Dataloader is a class which can store the dataset and allows to fetch mini batches of data via iteration. . from torch.utils.data import DataLoader train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True) . Below gives a mini batch of data from train_data dataset. . next(iter(train_loader)) . [tensor([[0.9507], [0.5427], [0.1409], [0.3745], [0.1987], [0.8948], [0.7722], [0.7852], [0.5248], [0.2809], [0.1159], [0.0740], [0.1849], [0.4561], [0.7608], [0.1560]]), tensor([[2.8715], [2.2161], [1.1211], [1.7578], [1.2654], [2.7393], [2.4208], [2.5283], [2.0167], [1.5846], [1.1603], [1.1713], [1.5888], [1.7706], [2.4970], [1.2901]])] . len(next(iter(train_loader.batch_sampler))) . 16 . Above cell show the size of one iteration of the Dataloader. It is 16 i.e the size we specified. . train_loader.batch_size . 16 . train_loader.dataset[0] . (tensor([0.7713]), tensor([2.4745])) . train_data[0] . (tensor([0.7713]), tensor([2.4745])) . Above cells show the value of the index 0 in dataset from within the Loader as well as direclty from the dataset . losses = [] train_step = make_train_step(model, loss_func, optimizer) for epoch in range(n_epochs): for x_batch, y_batch in train_loader: # The dataset, dataloader and its mini-batches all resides in the CPU for now. # Lets send those mini-batches to the GPU in below steps # Model is already in GPU x_batch = x_batch.to(device) y_batch = y_batch.to(device) loss = train_step(x_batch, y_batch) losses.append(loss) print(model.state_dict()) . OrderedDict([(&#39;a&#39;, tensor([1.0242], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([1.9682], device=&#39;cuda:0&#39;))]) . Well, we added an extra loop and it took slightly higher time to finish than our previous trials. . However, this is required in real life, because the data will be huge and cannot be loaded into GPU at once. . If you remember, we split the data 80:20 as train and valid in the beginning of this notebook. . However, we generally use Pytorch&#39;s feature to split the data into train and valid (random split) or we have data split as training and validation data using some functional rule beforehand. . We have trained the model, but we need to validate it. . For that we use the validation dataloader-&gt; dataset-&gt; data . valid_loader = DataLoader(dataset=valid_data, batch_size=16, shuffle=True) . losses = [] val_losses = [] train_step = make_train_step(model, loss_func, optimizer) for epoch in range(n_epochs): for x_batch, y_batch in train_loader: x_batch = x_batch.to(device) y_batch = y_batch.to(device) loss = train_step(x_batch, y_batch) losses.append(loss) with torch.no_grad(): for x_val, y_val in valid_loader: x_val = x_val.to(device) y_val = y_val.to(device) model.eval() yhat = model(x_val) val_loss = loss_func(y_val, yhat) val_losses.append(val_loss.item()) print(model.state_dict()) . OrderedDict([(&#39;a&#39;, tensor([1.0290], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([1.9720], device=&#39;cuda:0&#39;))]) . len(val_losses) . 2000 . How did this number come ? valid data length (see first few lines of notebook) which is 20 / 16 batch size = 2 (approx) . 2 * number of epochs(1000) = 2000 . | val_losses[1990:] . [0.00672027375549078, 0.017072057351469994, 0.010100197046995163, 0.004003560170531273, 0.008860822767019272, 0.007289723493158817, 0.007129967212677002, 0.015293894335627556, 0.0099174864590168, 0.00720137357711792] . len(losses) . 5000 . How did this number come ? train data length (see first few lines of notebook) which is 80 / 16 batch size = 5 | 5 * number of epochs(1000) = 5000 . losses[4990:] . [0.01098373532295227, 0.007446237839758396, 0.004498552531003952, 0.011742398142814636, 0.006587368436157703, 0.006659486331045628, 0.008902094326913357, 0.006174780428409576, 0.010141816921532154, 0.008895618841052055] . As you can see the final losses of train and valid data, training loss is slightly higher than validation loss. . OrderedDict([(&#39;a&#39;, tensor([1.0290], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([1.9720], device=&#39;cuda:0&#39;))]) . This is the final prediction of the model which is pretty accurate, slightly more accurate than our numpy model - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb . &lt;/div&gt; .",
            "url": "https://mrg-ai.github.io/blog/2019/12/31/Linear-Regression-Using-Pytorch.html",
            "relUrl": "/2019/12/31/Linear-Regression-Using-Pytorch.html",
            "date": " • Dec 31, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Exploratory-Data-Analysis-Rossman-Data",
            "content": "%reload_ext autoreload %autoreload 2 . from fastai.basics import * import pandas as pd import matplotlib.pyplot as plt . !curl -s https://course.fast.ai/setup/colab | bash . Updating fastai... Done. . Data preparation / Feature engineering . In addition to the provided data, we will be using external datasets put together by participants in the Kaggle competition. You can download all of them here. Then you shold untar them in the dirctory to which PATH is pointing below. . For completeness, the implementation used to put them together is included below. . untar_data(&#39;http://files.fast.ai/part2/lesson14/rossmann&#39;,dest=&#39;/content/data/rossmann&#39;) . PosixPath(&#39;/content/data/rossmann/rossmann&#39;) . (Config().data_path()/&#39;rossmann&#39;).ls() . [PosixPath(&#39;/root/.fastai/data/rossmann/googletrend.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/weather.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/store_states.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/state_names.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/sample_submission.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/train.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/store.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/test.csv&#39;)] . (Config().data_path()/&#39;rossmann&#39;) . PosixPath(&#39;/root/.fastai/data/rossmann&#39;) . PATH=(Config().data_path()/&#39;rossmann&#39;) . table_names = [&#39;train&#39;, &#39;store&#39;, &#39;store_states&#39;, &#39;state_names&#39;, &#39;googletrend&#39;, &#39;weather&#39;, &#39;test&#39;] type(table_names) . list . ??pd.read_csv . tables = [pd.read_csv(PATH/f&#39;{fname}.csv&#39;, low_memory=False) for fname in table_names] train, store, store_states, state_names, googletrend, weather, test = tables len(train),len(test) . (1017209, 41088) . ??train.isnull() . Define functions for Data Exploration . def data_shape_and_head(df): pd.set_option(&#39;float_format&#39;, &#39;{:f}&#39;.format) print(f&quot;Data Frame Shape: {df.shape}&quot;) return df.head() . def percentage_of_null_data(df): pd.options.mode.use_inf_as_na=True total = df.isnull().sum() percent = (df.isnull().sum()/df.isnull().count()*100) tt = pd.concat([total, percent], axis=1, keys=[&#39;Total&#39;, &#39;Percent&#39;]) types = [] for col in df.columns: dtype = str(df[col].dtype) types.append(dtype) tt[&#39;Types&#39;] = types return(np.transpose(tt)) . def unique_values_eachcolumn(df): for column in df.columns.values: print(f&quot;[df] Unique values of &#39;{column}&#39; : {df[column].nunique()}&quot;) . def plot_col_count_4_top20(column_name, title, df, size=6): # Displays the count of records for each value of the column. # Displays data for first 20 most frequent values import seaborn as sns f, ax = plt.subplots(1,1, figsize=(4*size,4)) total = float(len(df)) g = sns.countplot(df[column_name], order = df[column_name].value_counts().index[:30], palette=&#39;Dark2&#39;) g.set_title(&quot;Number and Percentage of {}&quot;.format(title)) if(size &gt; 2): plt.xticks(rotation=90, size=8) for p in ax.patches: height = p.get_height() ax.text(p.get_x()+p.get_width()/2., height + 3, &#39;{:1.2f}%&#39;.format(100*height/total), ha=&quot;center&quot;) plt.show() . def plot_most_or_least_populated_data(df,most=True): import seaborn as sns total = df.isnull().count() - df.isnull().sum() percent = 100 - (df.isnull().sum()/df.isnull().count()*100) tt = pd.concat([total, percent], axis=1, keys=[&#39;Total&#39;, &#39;Percent&#39;]) tt = pd.DataFrame(tt.reset_index()) tt= (tt.sort_values([&#39;Total&#39;], ascending=most)) plt.figure(figsize=(10, 8)) sns.set(style=&#39;darkgrid&#39;) ax = sns.barplot(x=&#39;Percent&#39;, y=&#39;index&#39;, data=tt.head(30), color=&#39;DarkOrange&#39;) plt.title((&#39;Most&#39; if most else &#39;Least&#39; ) + &#39; frequent columns/features in the dataframe&#39;) plt.ylabel(&#39;Features/Columns&#39;) plt.show() . Do the data exploration for train Dataframe . data_shape_and_head(train) . Data Frame Shape: (1017209, 9) . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | 0 | 1 | . 1 2 | 5 | 2015-07-31 | 6064 | 625 | 1 | 1 | 0 | 1 | . 2 3 | 5 | 2015-07-31 | 8314 | 821 | 1 | 1 | 0 | 1 | . 3 4 | 5 | 2015-07-31 | 13995 | 1498 | 1 | 1 | 0 | 1 | . 4 5 | 5 | 2015-07-31 | 4822 | 559 | 1 | 1 | 0 | 1 | . train.describe() . Store DayOfWeek Sales Customers Open Promo SchoolHoliday . count 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | . mean 558.429727 | 3.998341 | 5773.818972 | 633.145946 | 0.830107 | 0.381515 | 0.178647 | . std 321.908651 | 1.997391 | 3849.926175 | 464.411734 | 0.375539 | 0.485759 | 0.383056 | . min 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 280.000000 | 2.000000 | 3727.000000 | 405.000000 | 1.000000 | 0.000000 | 0.000000 | . 50% 558.000000 | 4.000000 | 5744.000000 | 609.000000 | 1.000000 | 0.000000 | 0.000000 | . 75% 838.000000 | 6.000000 | 7856.000000 | 837.000000 | 1.000000 | 1.000000 | 0.000000 | . max 1115.000000 | 7.000000 | 41551.000000 | 7388.000000 | 1.000000 | 1.000000 | 1.000000 | . train.describe().T . count mean std min 25% 50% 75% max . Store 1017209.000000 | 558.429727 | 321.908651 | 1.000000 | 280.000000 | 558.000000 | 838.000000 | 1115.000000 | . DayOfWeek 1017209.000000 | 3.998341 | 1.997391 | 1.000000 | 2.000000 | 4.000000 | 6.000000 | 7.000000 | . Sales 1017209.000000 | 5773.818972 | 3849.926175 | 0.000000 | 3727.000000 | 5744.000000 | 7856.000000 | 41551.000000 | . Customers 1017209.000000 | 633.145946 | 464.411734 | 0.000000 | 405.000000 | 609.000000 | 837.000000 | 7388.000000 | . Open 1017209.000000 | 0.830107 | 0.375539 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . Promo 1017209.000000 | 0.381515 | 0.485759 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | . SchoolHoliday 1017209.000000 | 0.178647 | 0.383056 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . train.count() . Store 1017209 DayOfWeek 1017209 Date 1017209 Sales 1017209 Customers 1017209 Open 1017209 Promo 1017209 StateHoliday 1017209 SchoolHoliday 1017209 dtype: int64 . train[&#39;DayOfWeek&#39;].count() . 1017209 . unique_values_eachcolumn(train) . [df] Unique values of &#39;Store&#39; : 1115 [df] Unique values of &#39;DayOfWeek&#39; : 7 [df] Unique values of &#39;Date&#39; : 942 [df] Unique values of &#39;Sales&#39; : 21734 [df] Unique values of &#39;Customers&#39; : 4086 [df] Unique values of &#39;Open&#39; : 2 [df] Unique values of &#39;Promo&#39; : 2 [df] Unique values of &#39;StateHoliday&#39; : 4 [df] Unique values of &#39;SchoolHoliday&#39; : 2 . plot_col_count_4_top20(&#39;Date&#39;, &#39;Counts&#39;, train,size=6) . plot_most_or_least_populated_data(train) . train.DayOfWeek.nunique() . 7 . train.StateHoliday.unique() . array([&#39;0&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;], dtype=object) . percentage_of_null_data(train) . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday . Total 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Percent 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . Types int64 | int64 | object | int64 | int64 | int64 | int64 | object | int64 | . train.Date . 0 2015-07-31 1 2015-07-31 2 2015-07-31 3 2015-07-31 4 2015-07-31 ... 1017204 2013-01-01 1017205 2013-01-01 1017206 2013-01-01 1017207 2013-01-01 1017208 2013-01-01 Name: Date, Length: 1017209, dtype: object . train.groupby(&#39;Date&#39;).max().Sales . Date 2013-01-01 17267 2013-01-02 25357 2013-01-03 23303 2013-01-04 21996 2013-01-05 22521 ... 2015-07-27 27881 2015-07-28 25518 2015-07-29 25840 2015-07-30 24395 2015-07-31 27508 Name: Sales, Length: 942, dtype: int64 . train.sample(5, random_state=300).groupby(&#39;Date&#39;).max().Customers . Date 2013-07-10 440 2014-05-09 540 2014-11-02 0 2015-04-06 0 2015-04-20 482 Name: Customers, dtype: int64 . train.sample(20, random_state=5).groupby(&#39;Date&#39;).max().Sales.plot(kind=&#39;barh&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f85651a9828&gt; . train.groupby(&#39;DayOfWeek&#39;).DayOfWeek . &lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x7f85650f2198&gt; . (train.groupby(&#39;DayOfWeek&#39;).sum()) . Store Sales Customers Open Promo SchoolHoliday . DayOfWeek . 1 80821168 | 1130203012 | 117675012 | 137560 | 77760 | 34060 | . 2 81344288 | 1020411930 | 110848063 | 143961 | 77580 | 36595 | . 3 81345276 | 954962863 | 105117642 | 141936 | 77580 | 34636 | . 4 81443338 | 911177709 | 101732938 | 134644 | 77580 | 34747 | . 5 81443338 | 980555941 | 108384820 | 138640 | 77580 | 36235 | . 6 80821168 | 846317735 | 95103854 | 144058 | 0 | 2724 | . 7 80821168 | 29551433 | 5179426 | 3593 | 0 | 2724 | . train[0:10] . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | 0 | 1 | . 1 2 | 5 | 2015-07-31 | 6064 | 625 | 1 | 1 | 0 | 1 | . 2 3 | 5 | 2015-07-31 | 8314 | 821 | 1 | 1 | 0 | 1 | . 3 4 | 5 | 2015-07-31 | 13995 | 1498 | 1 | 1 | 0 | 1 | . 4 5 | 5 | 2015-07-31 | 4822 | 559 | 1 | 1 | 0 | 1 | . 5 6 | 5 | 2015-07-31 | 5651 | 589 | 1 | 1 | 0 | 1 | . 6 7 | 5 | 2015-07-31 | 15344 | 1414 | 1 | 1 | 0 | 1 | . 7 8 | 5 | 2015-07-31 | 8492 | 833 | 1 | 1 | 0 | 1 | . 8 9 | 5 | 2015-07-31 | 8565 | 687 | 1 | 1 | 0 | 1 | . 9 10 | 5 | 2015-07-31 | 7185 | 681 | 1 | 1 | 0 | 1 | . We looked at the train data. Similarly we can look at other tabular data as well and figure out the features that we can use. . . We turn state Holidays to booleans, to make them more convenient for modeling. We can do calculations on pandas fields using notation very similar (often identical) to numpy. . train.StateHoliday = train.StateHoliday!=&#39;0&#39; test.StateHoliday = test.StateHoliday!=&#39;0&#39; . join_df is a function for joining tables on specific fields. By default, we&#39;ll be doing a left outer join of right on the left argument using the given fields for each table. . Pandas does joins using the merge method. The suffixes argument describes the naming convention for duplicate fields. We&#39;ve elected to leave the duplicate field names on the left untouched, and append a &quot;_y&quot; to those on the right. . ??pd.merge() . def join_df(left, right, left_on, right_on=None, suffix=&#39;_y&#39;): if right_on is None: right_on = left_on return left.merge(right, how=&#39;left&#39;, left_on=left_on, right_on=right_on, suffixes=(&quot;&quot;, suffix)) . Join weather/state names. . weather = join_df(weather, state_names, &quot;file&quot;, &quot;StateName&quot;) . weather . file Date Max_TemperatureC Mean_TemperatureC Min_TemperatureC Dew_PointC MeanDew_PointC Min_DewpointC Max_Humidity Mean_Humidity Min_Humidity Max_Sea_Level_PressurehPa Mean_Sea_Level_PressurehPa Min_Sea_Level_PressurehPa Max_VisibilityKm Mean_VisibilityKm Min_VisibilitykM Max_Wind_SpeedKm_h Mean_Wind_SpeedKm_h Max_Gust_SpeedKm_h Precipitationmm CloudCover Events WindDirDegrees StateName State . 0 NordrheinWestfalen | 2013-01-01 | 8 | 4 | 2 | 7 | 5 | 1 | 94 | 87 | 65 | 1013 | 1005 | 1001 | 31.000000 | 12.000000 | 4.000000 | 39 | 26 | 58.000000 | 5.080000 | 6.000000 | Rain | 215 | NordrheinWestfalen | NW | . 1 NordrheinWestfalen | 2013-01-02 | 7 | 4 | 1 | 5 | 3 | 2 | 93 | 85 | 78 | 1028 | 1022 | 1014 | 31.000000 | 14.000000 | 10.000000 | 24 | 16 | nan | 0.000000 | 6.000000 | Rain | 225 | NordrheinWestfalen | NW | . 2 NordrheinWestfalen | 2013-01-03 | 11 | 8 | 6 | 10 | 8 | 4 | 100 | 93 | 77 | 1035 | 1030 | 1026 | 31.000000 | 8.000000 | 2.000000 | 26 | 21 | nan | 1.020000 | 7.000000 | Rain | 240 | NordrheinWestfalen | NW | . 3 NordrheinWestfalen | 2013-01-04 | 9 | 9 | 8 | 9 | 9 | 8 | 100 | 94 | 87 | 1036 | 1035 | 1034 | 11.000000 | 5.000000 | 2.000000 | 23 | 14 | nan | 0.250000 | 7.000000 | Rain | 263 | NordrheinWestfalen | NW | . 4 NordrheinWestfalen | 2013-01-05 | 8 | 8 | 7 | 8 | 7 | 6 | 100 | 94 | 87 | 1035 | 1034 | 1033 | 10.000000 | 6.000000 | 3.000000 | 16 | 10 | nan | 0.000000 | 7.000000 | Rain | 268 | NordrheinWestfalen | NW | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15835 Saarland | 2015-09-13 | 21 | 17 | 12 | 16 | 14 | 12 | 100 | 88 | 68 | 1015 | 1010 | 1005 | 31.000000 | 15.000000 | 10.000000 | 27 | 10 | 50.000000 | 1.020000 | 7.000000 | Rain | 113 | Saarland | SL | . 15836 Saarland | 2015-09-14 | 18 | 14 | 11 | 15 | 12 | 7 | 99 | 85 | 61 | 1009 | 1005 | 1004 | 31.000000 | 13.000000 | 4.000000 | 32 | 16 | 53.000000 | 9.910000 | 5.000000 | Rain-Thunderstorm | 213 | Saarland | SL | . 15837 Saarland | 2015-09-15 | 16 | 12 | 9 | 11 | 8 | 7 | 93 | 77 | 62 | 1010 | 1008 | 1004 | 31.000000 | 12.000000 | 10.000000 | 34 | 14 | nan | 0.000000 | 5.000000 | Rain | 193 | Saarland | SL | . 15838 Saarland | 2015-09-16 | 19 | 15 | 11 | 16 | 13 | 10 | 97 | 90 | 75 | 1004 | 999 | 995 | 31.000000 | 10.000000 | 4.000000 | 32 | 14 | 45.000000 | 20.070000 | 7.000000 | Rain-Thunderstorm | 147 | Saarland | SL | . 15839 Saarland | 2015-09-17 | 14 | 13 | 12 | 14 | 12 | 10 | 99 | 92 | 82 | 1013 | 1005 | 999 | 31.000000 | 14.000000 | 8.000000 | 27 | 16 | 47.000000 | 6.100000 | 6.000000 | Rain | 202 | Saarland | SL | . 15840 rows × 26 columns . In pandas you can add new columns to a dataframe by simply defining it. We&#39;ll do this for googletrends by extracting dates and state names from the given data and adding those columns. . We&#39;re also going to replace all instances of state name &#39;NI&#39; to match the usage in the rest of the data: &#39;HB,NI&#39;. This is a good opportunity to highlight pandas indexing. We can use .loc[rows, cols] to select a list of rows and a list of columns from the dataframe. In this case, we&#39;re selecting rows w/ statename &#39;NI&#39; by using a boolean list googletrend.State==&#39;NI&#39; and selecting &quot;State&quot;. . googletrend.index[5] . 5 . googletrend.week.str.split(&#39; - &#39;, expand=True)[1] . 0 2012-12-08 1 2012-12-15 2 2012-12-22 3 2012-12-29 4 2013-01-05 ... 2067 2015-09-05 2068 2015-09-12 2069 2015-09-19 2070 2015-09-26 2071 2015-10-03 Name: 1, Length: 2072, dtype: object . googletrend . file week trend Date State . 0 Rossmann_DE_SN | 2012-12-02 - 2012-12-08 | 96 | 2012-12-02 | SN | . 1 Rossmann_DE_SN | 2012-12-09 - 2012-12-15 | 95 | 2012-12-09 | SN | . 2 Rossmann_DE_SN | 2012-12-16 - 2012-12-22 | 91 | 2012-12-16 | SN | . 3 Rossmann_DE_SN | 2012-12-23 - 2012-12-29 | 48 | 2012-12-23 | SN | . 4 Rossmann_DE_SN | 2012-12-30 - 2013-01-05 | 67 | 2012-12-30 | SN | . ... ... | ... | ... | ... | ... | . 2067 Rossmann_DE_SL | 2015-08-30 - 2015-09-05 | 95 | 2015-08-30 | SL | . 2068 Rossmann_DE_SL | 2015-09-06 - 2015-09-12 | 47 | 2015-09-06 | SL | . 2069 Rossmann_DE_SL | 2015-09-13 - 2015-09-19 | 80 | 2015-09-13 | SL | . 2070 Rossmann_DE_SL | 2015-09-20 - 2015-09-26 | 57 | 2015-09-20 | SL | . 2071 Rossmann_DE_SL | 2015-09-27 - 2015-10-03 | 0 | 2015-09-27 | SL | . 2072 rows × 5 columns . googletrend[&#39;Date&#39;] = googletrend.week.str.split(&#39; - &#39;, expand=True)[0] googletrend[&#39;State&#39;] = googletrend.file.str.split(&#39;_&#39;, expand=True)[2] . googletrend.loc[googletrend.State==&#39;NI&#39;, &quot;State&quot;] . Series([], Name: State, dtype: object) . googletrend.loc[googletrend.State==&#39;NI&#39;, &quot;State&quot;] = &#39;HB,NI&#39; . googletrend . file week trend Date State . 0 Rossmann_DE_SN | 2012-12-02 - 2012-12-08 | 96 | 2012-12-02 | SN | . 1 Rossmann_DE_SN | 2012-12-09 - 2012-12-15 | 95 | 2012-12-09 | SN | . 2 Rossmann_DE_SN | 2012-12-16 - 2012-12-22 | 91 | 2012-12-16 | SN | . 3 Rossmann_DE_SN | 2012-12-23 - 2012-12-29 | 48 | 2012-12-23 | SN | . 4 Rossmann_DE_SN | 2012-12-30 - 2013-01-05 | 67 | 2012-12-30 | SN | . ... ... | ... | ... | ... | ... | . 2067 Rossmann_DE_SL | 2015-08-30 - 2015-09-05 | 95 | 2015-08-30 | SL | . 2068 Rossmann_DE_SL | 2015-09-06 - 2015-09-12 | 47 | 2015-09-06 | SL | . 2069 Rossmann_DE_SL | 2015-09-13 - 2015-09-19 | 80 | 2015-09-13 | SL | . 2070 Rossmann_DE_SL | 2015-09-20 - 2015-09-26 | 57 | 2015-09-20 | SL | . 2071 Rossmann_DE_SL | 2015-09-27 - 2015-10-03 | 0 | 2015-09-27 | SL | . 2072 rows × 5 columns . The following extracts particular date fields from a complete datetime for the purpose of constructing categoricals. . You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can&#39;t capture any trend/cyclical behavior as a function of time at any of these granularities. We&#39;ll add to every table with a date field. . def add_datepart(df, fldname, drop=True, time=False): &quot;Helper function that adds columns relevant to a date.&quot; fld = df[fldname] fld_dtype = fld.dtype if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype): fld_dtype = np.datetime64 if not np.issubdtype(fld_dtype, np.datetime64): df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True) targ_pre = re.sub(&#39;[Dd]ate$&#39;, &#39;&#39;, fldname) attr = [&#39;Year&#39;, &#39;Month&#39;, &#39;Week&#39;, &#39;Day&#39;, &#39;Dayofweek&#39;, &#39;Dayofyear&#39;, &#39;Is_month_end&#39;, &#39;Is_month_start&#39;, &#39;Is_quarter_end&#39;, &#39;Is_quarter_start&#39;, &#39;Is_year_end&#39;, &#39;Is_year_start&#39;] if time: attr = attr + [&#39;Hour&#39;, &#39;Minute&#39;, &#39;Second&#39;] for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower()) df[targ_pre + &#39;Elapsed&#39;] = fld.astype(np.int64) // 10 ** 9 if drop: df.drop(fldname, axis=1, inplace=True) . add_datepart(weather, &quot;Date&quot;, drop=False) add_datepart(googletrend, &quot;Date&quot;, drop=False) add_datepart(train, &quot;Date&quot;, drop=False) add_datepart(test, &quot;Date&quot;, drop=False) . The Google trends data has a special category for the whole of the Germany - we&#39;ll pull that out so we can use it explicitly. . trend_de = googletrend[googletrend.file == &#39;Rossmann_DE&#39;] . Now we can outer join all of our data into a single dataframe. Recall that in outer joins everytime a value in the joining field on the left table does not have a corresponding value on the right table, the corresponding row in the new table has Null values for all right table fields. One way to check that all records are consistent and complete is to check for Null values post-join, as we do here. . Aside: Why not just do an inner join? If you are assuming that all records are complete and match on the field you desire, an inner join will do the same thing as an outer join. However, in the event you are wrong or a mistake is made, an outer join followed by a null-check will catch it. (Comparing before/after # of rows for inner join is equivalent, but requires keeping track of before/after row #&#39;s. Outer join is easier.) . store = join_df(store, store_states, &quot;Store&quot;) len(store[store.State.isnull()]) . 0 . ??join_df . joined = join_df(train, store, &quot;Store&quot;) joined_test = join_df(test, store, &quot;Store&quot;) len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()]) . (0, 0) . joined = join_df(joined, googletrend, [&quot;State&quot;,&quot;Year&quot;, &quot;Week&quot;]) joined_test = join_df(joined_test, googletrend, [&quot;State&quot;,&quot;Year&quot;, &quot;Week&quot;]) len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()]) . (0, 0) . joined = joined.merge(trend_de, &#39;left&#39;, [&quot;Year&quot;, &quot;Week&quot;], suffixes=(&#39;&#39;, &#39;_DE&#39;)) joined_test = joined_test.merge(trend_de, &#39;left&#39;, [&quot;Year&quot;, &quot;Week&quot;], suffixes=(&#39;&#39;, &#39;_DE&#39;)) len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()]) . (0, 0) . joined = join_df(joined, weather, [&quot;State&quot;,&quot;Date&quot;]) joined_test = join_df(joined_test, weather, [&quot;State&quot;,&quot;Date&quot;]) len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()]) . (0, 0) . joined.describe() . Store DayOfWeek Sales Customers Open Promo SchoolHoliday Year Month Week Day Dayofweek Dayofyear Elapsed CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear trend Month_y Day_y Dayofweek_y Dayofyear_y Elapsed_y trend_DE Month_DE Day_DE Dayofweek_DE Dayofyear_DE Elapsed_DE Max_TemperatureC Mean_TemperatureC Min_TemperatureC Dew_PointC MeanDew_PointC Min_DewpointC Max_Humidity Mean_Humidity Min_Humidity Max_Sea_Level_PressurehPa Mean_Sea_Level_PressurehPa Min_Sea_Level_PressurehPa Max_VisibilityKm Mean_VisibilityKm Min_VisibilitykM Max_Wind_SpeedKm_h Mean_Wind_SpeedKm_h Max_Gust_SpeedKm_h Precipitationmm CloudCover WindDirDegrees Year_y Month_y Week_y Day_y Dayofweek_y Dayofyear_y Elapsed_y . count 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1014567.000000 | 693861.000000 | 693861.000000 | 1017209.000000 | 509178.000000 | 509178.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 992564.000000 | 992564.000000 | 992564.000000 | 1017209.000000 | 1017209.000000 | 236167.000000 | 1017209.000000 | 936890.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | . mean 558.429727 | 3.998341 | 5773.818972 | 633.145946 | 0.830107 | 0.381515 | 0.178647 | 2013.832292 | 5.846762 | 23.615515 | 15.702790 | 2.998341 | 162.276385 | 1397179842.846062 | 5430.085652 | 7.222866 | 2008.690228 | 0.500564 | 23.269093 | 2011.752774 | 65.219190 | 5.891212 | 15.562106 | 6.000000 | 163.476310 | 1397283516.407739 | 66.234798 | 5.891212 | 15.562106 | 6.000000 | 163.476310 | 1397283516.407739 | 14.328300 | 10.073856 | 5.900282 | 8.216752 | 5.835002 | 3.263247 | 93.283786 | 73.952151 | 49.923426 | 1018.644120 | 1015.429182 | 1012.428608 | 23.542818 | 12.127392 | 7.284987 | 22.672915 | 11.870788 | 48.459272 | 0.790271 | 5.521105 | 177.063058 | 2013.832292 | 5.846762 | 23.615515 | 15.702790 | 2.998341 | 162.276385 | 1397179842.846062 | . std 321.908651 | 1.997391 | 3849.926175 | 464.411734 | 0.375539 | 0.485759 | 0.383056 | 0.777396 | 3.326097 | 14.433381 | 8.787638 | 1.997391 | 101.616189 | 23712834.841055 | 7715.323700 | 3.211832 | 5.992644 | 0.500000 | 14.095973 | 1.662870 | 11.339865 | 3.308500 | 8.802947 | 0.000000 | 101.235953 | 23757279.032632 | 9.618326 | 3.308500 | 8.802947 | 0.000000 | 101.235953 | 23757279.032632 | 8.464778 | 7.239083 | 6.459730 | 6.140112 | 5.993374 | 6.033076 | 7.717332 | 13.206449 | 19.626353 | 7.915110 | 8.248712 | 8.748368 | 9.222368 | 4.871022 | 4.771402 | 8.940418 | 5.897191 | 13.142078 | 2.502615 | 1.683795 | 101.748176 | 0.777396 | 3.326097 | 14.433381 | 8.787638 | 1.997391 | 101.616189 | 23712834.841055 | . min 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 2013.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | 1356998400.000000 | 20.000000 | 1.000000 | 1900.000000 | 0.000000 | 1.000000 | 2009.000000 | 28.000000 | 1.000000 | 1.000000 | 6.000000 | 4.000000 | 1357430400.000000 | 50.000000 | 1.000000 | 1.000000 | 6.000000 | 4.000000 | 1357430400.000000 | -11.000000 | -13.000000 | -15.000000 | -14.000000 | -15.000000 | -73.000000 | 44.000000 | 30.000000 | 4.000000 | 976.000000 | 974.000000 | 970.000000 | 0.000000 | 0.000000 | 0.000000 | 3.000000 | 2.000000 | 21.000000 | 0.000000 | 0.000000 | -1.000000 | 2013.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | 1356998400.000000 | . 25% 280.000000 | 2.000000 | 3727.000000 | 405.000000 | 1.000000 | 0.000000 | 0.000000 | 2013.000000 | 3.000000 | 11.000000 | 8.000000 | 1.000000 | 77.000000 | 1376697600.000000 | 710.000000 | 4.000000 | 2006.000000 | 0.000000 | 13.000000 | 2011.000000 | 57.000000 | 3.000000 | 8.000000 | 6.000000 | 76.000000 | 1376784000.000000 | 58.000000 | 3.000000 | 8.000000 | 6.000000 | 76.000000 | 1376784000.000000 | 8.000000 | 4.000000 | 1.000000 | 3.000000 | 1.000000 | -1.000000 | 88.000000 | 64.000000 | 34.000000 | 1014.000000 | 1011.000000 | 1007.000000 | 11.000000 | 10.000000 | 3.000000 | 16.000000 | 8.000000 | 39.000000 | 0.000000 | 5.000000 | 79.000000 | 2013.000000 | 3.000000 | 11.000000 | 8.000000 | 1.000000 | 77.000000 | 1376697600.000000 | . 50% 558.000000 | 4.000000 | 5744.000000 | 609.000000 | 1.000000 | 0.000000 | 0.000000 | 2014.000000 | 6.000000 | 22.000000 | 16.000000 | 3.000000 | 153.000000 | 1396396800.000000 | 2330.000000 | 8.000000 | 2010.000000 | 1.000000 | 22.000000 | 2012.000000 | 65.000000 | 6.000000 | 15.000000 | 6.000000 | 153.000000 | 1396137600.000000 | 66.000000 | 6.000000 | 15.000000 | 6.000000 | 153.000000 | 1396137600.000000 | 15.000000 | 10.000000 | 6.000000 | 8.000000 | 6.000000 | 3.000000 | 94.000000 | 75.000000 | 49.000000 | 1019.000000 | 1016.000000 | 1013.000000 | 31.000000 | 11.000000 | 8.000000 | 21.000000 | 11.000000 | 47.000000 | 0.000000 | 6.000000 | 201.000000 | 2014.000000 | 6.000000 | 22.000000 | 16.000000 | 3.000000 | 153.000000 | 1396396800.000000 | . 75% 838.000000 | 6.000000 | 7856.000000 | 837.000000 | 1.000000 | 1.000000 | 0.000000 | 2014.000000 | 8.000000 | 35.000000 | 23.000000 | 5.000000 | 241.000000 | 1418342400.000000 | 6890.000000 | 10.000000 | 2013.000000 | 1.000000 | 37.000000 | 2013.000000 | 73.000000 | 8.000000 | 23.000000 | 6.000000 | 243.000000 | 1418515200.000000 | 72.000000 | 8.000000 | 23.000000 | 6.000000 | 243.000000 | 1418515200.000000 | 21.000000 | 16.000000 | 11.000000 | 13.000000 | 11.000000 | 8.000000 | 100.000000 | 84.000000 | 65.000000 | 1024.000000 | 1021.000000 | 1018.000000 | 31.000000 | 14.000000 | 10.000000 | 27.000000 | 14.000000 | 55.000000 | 0.250000 | 7.000000 | 257.000000 | 2014.000000 | 8.000000 | 35.000000 | 23.000000 | 5.000000 | 241.000000 | 1418342400.000000 | . max 1115.000000 | 7.000000 | 41551.000000 | 7388.000000 | 1.000000 | 1.000000 | 1.000000 | 2015.000000 | 12.000000 | 52.000000 | 31.000000 | 6.000000 | 365.000000 | 1438300800.000000 | 75860.000000 | 12.000000 | 2015.000000 | 1.000000 | 50.000000 | 2015.000000 | 100.000000 | 12.000000 | 31.000000 | 6.000000 | 363.000000 | 1438473600.000000 | 100.000000 | 12.000000 | 31.000000 | 6.000000 | 363.000000 | 1438473600.000000 | 38.000000 | 31.000000 | 24.000000 | 25.000000 | 20.000000 | 18.000000 | 100.000000 | 100.000000 | 100.000000 | 1043.000000 | 1040.000000 | 1038.000000 | 31.000000 | 31.000000 | 31.000000 | 101.000000 | 53.000000 | 111.000000 | 58.930000 | 8.000000 | 360.000000 | 2015.000000 | 12.000000 | 52.000000 | 31.000000 | 6.000000 | 365.000000 | 1438300800.000000 | . for df in (joined, joined_test): for c in df.columns: if c.endswith(&#39;_y&#39;): if c in df.columns: print(c) . Date_y Month_y Day_y Dayofweek_y Dayofyear_y Is_month_end_y Is_month_start_y Is_quarter_end_y Is_quarter_start_y Is_year_end_y Is_year_start_y Elapsed_y file_y Year_y Month_y Week_y Day_y Dayofweek_y Dayofyear_y Is_month_end_y Is_month_start_y Is_quarter_end_y Is_quarter_start_y Is_year_end_y Is_year_start_y Elapsed_y Date_y Month_y Day_y Dayofweek_y Dayofyear_y Is_month_end_y Is_month_start_y Is_quarter_end_y Is_quarter_start_y Is_year_end_y Is_year_start_y Elapsed_y file_y Year_y Month_y Week_y Day_y Dayofweek_y Dayofyear_y Is_month_end_y Is_month_start_y Is_quarter_end_y Is_quarter_start_y Is_year_end_y Is_year_start_y Elapsed_y . for df in (joined, joined_test): for c in df.columns: if c.endswith(&#39;_y&#39;): if c in df.columns: df.drop(c, inplace=True, axis=1) . Next we&#39;ll fill in missing values to avoid complications with NA&#39;s. NA (not available) is how Pandas indicates missing values; many models have problems when missing values are present, so it&#39;s always important to think about how to deal with them. In these cases, we are picking an arbitrary signal value that doesn&#39;t otherwise appear in the data. . joined.shape . (1017209, 74) . for column in joined.columns: print(column) . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday Year Month Week Day Dayofweek Dayofyear Is_month_end Is_month_start Is_quarter_end Is_quarter_start Is_year_end Is_year_start Elapsed StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval State file week trend file_DE week_DE trend_DE Date_DE State_DE Month_DE Day_DE Dayofweek_DE Dayofyear_DE Is_month_end_DE Is_month_start_DE Is_quarter_end_DE Is_quarter_start_DE Is_year_end_DE Is_year_start_DE Elapsed_DE Max_TemperatureC Mean_TemperatureC Min_TemperatureC Dew_PointC MeanDew_PointC Min_DewpointC Max_Humidity Mean_Humidity Min_Humidity Max_Sea_Level_PressurehPa Mean_Sea_Level_PressurehPa Min_Sea_Level_PressurehPa Max_VisibilityKm Mean_VisibilityKm Min_VisibilitykM Max_Wind_SpeedKm_h Mean_Wind_SpeedKm_h Max_Gust_SpeedKm_h Precipitationmm CloudCover Events WindDirDegrees StateName . len(joined.loc[joined.StateName.isna() , &quot;StateName&quot;]) . 0 . for df in (joined,joined_test): df[&#39;CompetitionOpenSinceYear&#39;] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32) df[&#39;CompetitionOpenSinceMonth&#39;] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32) df[&#39;Promo2SinceYear&#39;] = df.Promo2SinceYear.fillna(1900).astype(np.int32) df[&#39;Promo2SinceWeek&#39;] = df.Promo2SinceWeek.fillna(1).astype(np.int32) . Next we&#39;ll extract features &quot;CompetitionOpenSince&quot; and &quot;CompetitionDaysOpen&quot;. Note the use of apply() in mapping a function across dataframe values. . for df in (joined,joined_test): df[&quot;CompetitionOpenSince&quot;] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, month=df.CompetitionOpenSinceMonth, day=15)) df[&quot;CompetitionDaysOpen&quot;] = df.Date.subtract(df.CompetitionOpenSince).dt.days . We&#39;ll replace some erroneous / outlying data. . for df in (joined,joined_test): df.loc[df.CompetitionDaysOpen&lt;0, &quot;CompetitionDaysOpen&quot;] = 0 df.loc[df.CompetitionOpenSinceYear&lt;1990, &quot;CompetitionDaysOpen&quot;] = 0 . We add &quot;CompetitionMonthsOpen&quot; field, limiting the maximum to 2 years to limit number of unique categories. . for df in (joined,joined_test): df[&quot;CompetitionMonthsOpen&quot;] = df[&quot;CompetitionDaysOpen&quot;]//30 df.loc[df.CompetitionMonthsOpen&gt;24, &quot;CompetitionMonthsOpen&quot;] = 24 joined.CompetitionMonthsOpen.unique() . array([24, 3, 19, 9, 0, 16, 17, 7, 15, 22, 11, 13, 2, 23, 12, 4, 10, 1, 14, 20, 8, 18, 6, 21, 5]) . Same process for Promo dates. You may need to install the isoweek package first. . ! pip install isoweek . Collecting isoweek Downloading https://files.pythonhosted.org/packages/c2/d4/fe7e2637975c476734fcbf53776e650a29680194eb0dd21dbdc020ca92de/isoweek-1.3.3-py2.py3-none-any.whl Installing collected packages: isoweek Successfully installed isoweek-1.3.3 . from isoweek import Week for df in (joined,joined_test): df[&quot;Promo2Since&quot;] = pd.to_datetime(df.apply(lambda x: Week( x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1)) df[&quot;Promo2Days&quot;] = df.Date.subtract(df[&quot;Promo2Since&quot;]).dt.days . joined . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday Year Month Week Day Dayofweek Dayofyear Is_month_end Is_month_start Is_quarter_end Is_quarter_start Is_year_end Is_year_start Elapsed StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval State file week trend file_DE week_DE trend_DE Date_DE State_DE Month_DE Day_DE Dayofweek_DE Dayofyear_DE Is_month_end_DE Is_month_start_DE Is_quarter_end_DE Is_quarter_start_DE Is_year_end_DE Is_year_start_DE Elapsed_DE Max_TemperatureC Mean_TemperatureC Min_TemperatureC Dew_PointC MeanDew_PointC Min_DewpointC Max_Humidity Mean_Humidity Min_Humidity Max_Sea_Level_PressurehPa Mean_Sea_Level_PressurehPa Min_Sea_Level_PressurehPa Max_VisibilityKm Mean_VisibilityKm Min_VisibilitykM Max_Wind_SpeedKm_h Mean_Wind_SpeedKm_h Max_Gust_SpeedKm_h Precipitationmm CloudCover Events WindDirDegrees StateName CompetitionOpenSince CompetitionDaysOpen CompetitionMonthsOpen Promo2Since Promo2Days . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | c | a | 1270.000000 | 9 | 2008 | 0 | 1 | 1900 | NaN | HE | Rossmann_DE_HE | 2015-08-02 - 2015-08-08 | 85 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 23 | 16 | 8 | 9 | 6 | 3 | 98 | 54 | 18 | 1021 | 1018 | 1015 | 31.000000 | 15.000000 | 10.000000 | 24 | 11 | nan | 0.000000 | 1.000000 | Fog | 13 | Hessen | 2008-09-15 | 2510 | 24 | 1900-01-01 | 42214 | . 1 2 | 5 | 2015-07-31 | 6064 | 625 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | a | a | 570.000000 | 11 | 2007 | 1 | 13 | 2010 | Jan,Apr,Jul,Oct | TH | Rossmann_DE_TH | 2015-08-02 - 2015-08-08 | 80 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 19 | 13 | 7 | 9 | 6 | 3 | 100 | 62 | 25 | 1021 | 1019 | 1017 | 10.000000 | 10.000000 | 10.000000 | 14 | 11 | nan | 0.000000 | 4.000000 | Fog | 309 | Thueringen | 2007-11-15 | 2815 | 24 | 2010-03-29 | 1950 | . 2 3 | 5 | 2015-07-31 | 8314 | 821 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | a | a | 14130.000000 | 12 | 2006 | 1 | 14 | 2011 | Jan,Apr,Jul,Oct | NW | Rossmann_DE_NW | 2015-08-02 - 2015-08-08 | 86 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 21 | 13 | 6 | 10 | 7 | 4 | 100 | 61 | 24 | 1022 | 1019 | 1017 | 31.000000 | 14.000000 | 10.000000 | 14 | 5 | nan | 0.000000 | 2.000000 | Fog | 354 | NordrheinWestfalen | 2006-12-15 | 3150 | 24 | 2011-04-04 | 1579 | . 3 4 | 5 | 2015-07-31 | 13995 | 1498 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | c | c | 620.000000 | 9 | 2009 | 0 | 1 | 1900 | NaN | BE | Rossmann_DE_BE | 2015-08-02 - 2015-08-08 | 74 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 19 | 14 | 9 | 9 | 7 | 4 | 94 | 61 | 30 | 1019 | 1017 | 1014 | 10.000000 | 10.000000 | 10.000000 | 23 | 16 | nan | 0.000000 | 6.000000 | NaN | 282 | Berlin | 2009-09-15 | 2145 | 24 | 1900-01-01 | 42214 | . 4 5 | 5 | 2015-07-31 | 4822 | 559 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | a | a | 29910.000000 | 4 | 2015 | 0 | 1 | 1900 | NaN | SN | Rossmann_DE_SN | 2015-08-02 - 2015-08-08 | 82 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 20 | 15 | 10 | 8 | 6 | 5 | 82 | 55 | 26 | 1020 | 1018 | 1016 | 10.000000 | 10.000000 | 10.000000 | 14 | 11 | nan | 0.000000 | 4.000000 | NaN | 290 | Sachsen | 2015-04-15 | 107 | 3 | 1900-01-01 | 42214 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1017204 1111 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | a | a | 1900.000000 | 6 | 2014 | 1 | 31 | 2013 | Jan,Apr,Jul,Oct | NW | Rossmann_DE_NW | 2013-01-06 - 2013-01-12 | 63 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 4 | 2 | 7 | 5 | 1 | 94 | 87 | 65 | 1013 | 1005 | 1001 | 31.000000 | 12.000000 | 4.000000 | 39 | 26 | 58.000000 | 5.080000 | 6.000000 | Rain | 215 | NordrheinWestfalen | 2014-06-15 | 0 | 0 | 2013-07-29 | -209 | . 1017205 1112 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | c | c | 1880.000000 | 4 | 2006 | 0 | 1 | 1900 | NaN | NW | Rossmann_DE_NW | 2013-01-06 - 2013-01-12 | 63 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 4 | 2 | 7 | 5 | 1 | 94 | 87 | 65 | 1013 | 1005 | 1001 | 31.000000 | 12.000000 | 4.000000 | 39 | 26 | 58.000000 | 5.080000 | 6.000000 | Rain | 215 | NordrheinWestfalen | 2006-04-15 | 2453 | 24 | 1900-01-01 | 41273 | . 1017206 1113 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | a | c | 9260.000000 | 1 | 1900 | 0 | 1 | 1900 | NaN | SH | Rossmann_DE_SH | 2013-01-06 - 2013-01-12 | 72 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 7 | 6 | 7 | 7 | 5 | 100 | 95 | 93 | 1002 | 1000 | 999 | 10.000000 | 8.000000 | 5.000000 | 23 | 16 | nan | 0.000000 | 6.000000 | Rain | 251 | SchleswigHolstein | 1900-01-15 | 0 | 0 | 1900-01-01 | 41273 | . 1017207 1114 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | a | c | 870.000000 | 1 | 1900 | 0 | 1 | 1900 | NaN | HH | Rossmann_DE_HH | 2013-01-06 - 2013-01-12 | 63 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 6 | 4 | 8 | 6 | 3 | 100 | 92 | 70 | 1007 | 1001 | 997 | 31.000000 | 11.000000 | 6.000000 | 40 | 23 | 63.000000 | 6.100000 | 6.000000 | Rain | 234 | Hamburg | 1900-01-15 | 0 | 0 | 1900-01-01 | 41273 | . 1017208 1115 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | d | c | 5350.000000 | 1 | 1900 | 1 | 22 | 2012 | Mar,Jun,Sept,Dec | HE | Rossmann_DE_HE | 2013-01-06 - 2013-01-12 | 73 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 6 | 3 | 6 | 3 | 1 | 93 | 80 | 59 | 1015 | 1008 | 1006 | 31.000000 | 12.000000 | 10.000000 | 23 | 14 | 39.000000 | 2.030000 | 6.000000 | Rain | 206 | Hessen | 1900-01-15 | 0 | 0 | 2012-05-28 | 218 | . 1017209 rows × 79 columns . for df in (joined,joined_test): df.loc[df.Promo2Days&lt;0, &quot;Promo2Days&quot;] = 0 df.loc[df.Promo2SinceYear&lt;1990, &quot;Promo2Days&quot;] = 0 df[&quot;Promo2Weeks&quot;] = df[&quot;Promo2Days&quot;]//7 df.loc[df.Promo2Weeks&lt;0, &quot;Promo2Weeks&quot;] = 0 df.loc[df.Promo2Weeks&gt;25, &quot;Promo2Weeks&quot;] = 25 df.Promo2Weeks.unique() . df.Promo2Weeks.unique() . array([ 0, 25, 24, 15, 20, 23, 14, 19, 22, 13, 18, 21, 12, 17, 11, 16, 10, 9, 8]) . joined.to_pickle(PATH/&#39;joined&#39;) joined_test.to_pickle(PATH/&#39;joined_test&#39;) . Durations . It is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.: . Running averages | Time until next event | Time since last event | . This is often difficult to do with most table manipulation frameworks, since they are designed to work with relationships across columns. As such, we&#39;ve created a class to handle this type of data. . We&#39;ll define a function get_elapsed for cumulative counting across a sorted dataframe. Given a particular field fld to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero. . Upon initialization, this will result in datetime na&#39;s until the field is encountered. This is reset every time a new store is seen. We&#39;ll see how to use this shortly. . def get_elapsed(fld, pre): day1 = np.timedelta64(1, &#39;D&#39;) last_date = np.datetime64() last_store = 0 res = [] for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values): if s != last_store: last_date = np.datetime64() last_store = s if v: last_date = d res.append(((d-last_date).astype(&#39;timedelta64[D]&#39;) / day1)) df[pre+fld] = res . We&#39;ll be applying this to a subset of columns: . columns = [&quot;Date&quot;, &quot;Store&quot;, &quot;Promo&quot;, &quot;StateHoliday&quot;, &quot;SchoolHoliday&quot;] . df = train[columns].append(test[columns]) . Let&#39;s walk through an example. . Say we&#39;re looking at School Holiday. We&#39;ll first sort by Store, then Date, and then call add_elapsed(&#39;SchoolHoliday&#39;, &#39;After&#39;): This will apply to each row with School Holiday: . A applied to every row of the dataframe in order of store and date | Will add to the dataframe the days since seeing a School Holiday | If we sort in the other direction, this will count the days until another holiday. | . ??df.sort_values() . fld = &#39;SchoolHoliday&#39; df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;]) get_elapsed(fld, &#39;After&#39;) df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;], ascending=[True, False]) get_elapsed(fld, &#39;Before&#39;) . df . Date Store Promo StateHoliday SchoolHoliday AfterSchoolHoliday BeforeSchoolHoliday . 0 2015-09-17 | 1 | 1 | False | 0 | 13.000000 | nan | . 856 2015-09-16 | 1 | 1 | False | 0 | 12.000000 | nan | . 1712 2015-09-15 | 1 | 1 | False | 0 | 11.000000 | nan | . 2568 2015-09-14 | 1 | 1 | False | 0 | 10.000000 | nan | . 3424 2015-09-13 | 1 | 0 | False | 0 | 9.000000 | nan | . ... ... | ... | ... | ... | ... | ... | ... | . 1012749 2013-01-05 | 1115 | 0 | False | 1 | 0.000000 | 0.000000 | . 1013864 2013-01-04 | 1115 | 0 | False | 1 | 0.000000 | 0.000000 | . 1014979 2013-01-03 | 1115 | 0 | False | 1 | 0.000000 | 0.000000 | . 1016094 2013-01-02 | 1115 | 0 | False | 1 | 0.000000 | 0.000000 | . 1017208 2013-01-01 | 1115 | 0 | True | 1 | 0.000000 | 0.000000 | . 1058297 rows × 7 columns . We&#39;ll do this for two more fields. . fld = &#39;StateHoliday&#39; df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;]) get_elapsed(fld, &#39;After&#39;) df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;], ascending=[True, False]) get_elapsed(fld, &#39;Before&#39;) . fld = &#39;Promo&#39; df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;]) get_elapsed(fld, &#39;After&#39;) df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;], ascending=[True, False]) get_elapsed(fld, &#39;Before&#39;) . We&#39;re going to set the active index to Date. . df = df.set_index(&quot;Date&quot;) . Then set null values from elapsed field calculations to 0. . columns = [&#39;SchoolHoliday&#39;, &#39;StateHoliday&#39;, &#39;Promo&#39;] . for o in [&#39;Before&#39;, &#39;After&#39;]: for p in columns: a = o+p df[a] = df[a].fillna(0).astype(int) . Next we&#39;ll demonstrate window functions in pandas to calculate rolling quantities. . Here we&#39;re sorting by date (sort_index()) and counting the number of events of interest (sum()) defined in columns in the following week (rolling()), grouped by Store (groupby()). We do the same in the opposite direction. . bwd = df[[&#39;Store&#39;]+columns].sort_index().groupby(&quot;Store&quot;).rolling(7, min_periods=1).sum() . AttributeError Traceback (most recent call last) &lt;ipython-input-158-b3a2d175a68c&gt; in &lt;module&gt;() -&gt; 1 bwd = df[[&#39;Store&#39;]+columns].sort_index().groupby(&#34;Store&#34;).rolling(7, min_periods=1).sum() /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in sort_index(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, by) 5088 5089 indexer = nargsort( -&gt; 5090 labels, kind=kind, ascending=ascending, na_position=na_position 5091 ) 5092 /usr/local/lib/python3.6/dist-packages/pandas/core/sorting.py in nargsort(items, kind, ascending, na_position) 244 245 items = extract_array(items) --&gt; 246 mask = np.asarray(isna(items)) 247 248 if is_extension_array_dtype(items): /usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/missing.py in isna(obj) 120 Name: 1, dtype: bool 121 &#34;&#34;&#34; --&gt; 122 return _isna(obj) 123 124 /usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/missing.py in _isna_old(obj) 177 return _isna_ndarraylike_old(obj) 178 elif isinstance(obj, ABCGeneric): --&gt; 179 return obj._constructor(obj._data.isna(func=_isna_old)) 180 elif isinstance(obj, list): 181 return _isna_ndarraylike_old(np.asarray(obj, dtype=object)) AttributeError: &#39;DatetimeArray&#39; object has no attribute &#39;_constructor&#39; . fwd = df[[&#39;Store&#39;]+columns].sort_index(ascending=False ).groupby(&quot;Store&quot;).rolling(7, min_periods=1).sum() . AttributeError Traceback (most recent call last) &lt;ipython-input-159-05ebca394bc4&gt; in &lt;module&gt;() -&gt; 1 fwd = df[[&#39;Store&#39;]+columns].sort_index(ascending=False 2 ).groupby(&#34;Store&#34;).rolling(7, min_periods=1).sum() /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in sort_index(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, by) 5088 5089 indexer = nargsort( -&gt; 5090 labels, kind=kind, ascending=ascending, na_position=na_position 5091 ) 5092 /usr/local/lib/python3.6/dist-packages/pandas/core/sorting.py in nargsort(items, kind, ascending, na_position) 244 245 items = extract_array(items) --&gt; 246 mask = np.asarray(isna(items)) 247 248 if is_extension_array_dtype(items): /usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/missing.py in isna(obj) 120 Name: 1, dtype: bool 121 &#34;&#34;&#34; --&gt; 122 return _isna(obj) 123 124 /usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/missing.py in _isna_old(obj) 177 return _isna_ndarraylike_old(obj) 178 elif isinstance(obj, ABCGeneric): --&gt; 179 return obj._constructor(obj._data.isna(func=_isna_old)) 180 elif isinstance(obj, list): 181 return _isna_ndarraylike_old(np.asarray(obj, dtype=object)) AttributeError: &#39;DatetimeArray&#39; object has no attribute &#39;_constructor&#39; . Next we want to drop the Store indices grouped together in the window function. . Often in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets. . bwd.drop(&#39;Store&#39;,1,inplace=True) bwd.reset_index(inplace=True) . NameError Traceback (most recent call last) &lt;ipython-input-160-5283e630275b&gt; in &lt;module&gt;() -&gt; 1 bwd.drop(&#39;Store&#39;,1,inplace=True) 2 bwd.reset_index(inplace=True) NameError: name &#39;bwd&#39; is not defined . fwd.drop(&#39;Store&#39;,1,inplace=True) fwd.reset_index(inplace=True) . df.reset_index(inplace=True) . Now we&#39;ll merge these values onto the df. . df = df.merge(bwd, &#39;left&#39;, [&#39;Date&#39;, &#39;Store&#39;], suffixes=[&#39;&#39;, &#39;_bw&#39;]) df = df.merge(fwd, &#39;left&#39;, [&#39;Date&#39;, &#39;Store&#39;], suffixes=[&#39;&#39;, &#39;_fw&#39;]) . df.drop(columns,1,inplace=True) . df.head() . Date Store AfterSchoolHoliday BeforeSchoolHoliday AfterStateHoliday BeforeStateHoliday AfterPromo BeforePromo SchoolHoliday_bw StateHoliday_bw Promo_bw SchoolHoliday_fw StateHoliday_fw Promo_fw . 0 2015-09-17 | 1 | 13 | 0 | 105 | 0 | 0 | 0 | 0.0 | 0.0 | 4.0 | 0.0 | 0.0 | 1.0 | . 1 2015-09-16 | 1 | 12 | 0 | 104 | 0 | 0 | 0 | 0.0 | 0.0 | 3.0 | 0.0 | 0.0 | 2.0 | . 2 2015-09-15 | 1 | 11 | 0 | 103 | 0 | 0 | 0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 3.0 | . 3 2015-09-14 | 1 | 10 | 0 | 102 | 0 | 0 | 0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 4.0 | . 4 2015-09-13 | 1 | 9 | 0 | 101 | 0 | 9 | -1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.0 | . It&#39;s usually a good idea to back up large tables of extracted / wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it. . df.to_pickle(PATH/&#39;df&#39;) . df[&quot;Date&quot;] = pd.to_datetime(df.Date) . AttributeError Traceback (most recent call last) &lt;ipython-input-155-ccbc6b250f10&gt; in &lt;module&gt;() -&gt; 1 df[&#34;Date&#34;] = pd.to_datetime(df.Date) /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py in __getattr__(self, name) 5177 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5178 return self[name] -&gt; 5179 return object.__getattribute__(self, name) 5180 5181 def __setattr__(self, name, value): AttributeError: &#39;DataFrame&#39; object has no attribute &#39;Date&#39; . df.columns . Index([&#39;Date&#39;, &#39;Store&#39;, &#39;AfterSchoolHoliday&#39;, &#39;BeforeSchoolHoliday&#39;, &#39;AfterStateHoliday&#39;, &#39;BeforeStateHoliday&#39;, &#39;AfterPromo&#39;, &#39;BeforePromo&#39;, &#39;SchoolHoliday_bw&#39;, &#39;StateHoliday_bw&#39;, &#39;Promo_bw&#39;, &#39;SchoolHoliday_fw&#39;, &#39;StateHoliday_fw&#39;, &#39;Promo_fw&#39;], dtype=&#39;object&#39;) . joined = pd.read_pickle(PATH/&#39;joined&#39;) joined_test = pd.read_pickle(PATH/f&#39;joined_test&#39;) . joined = join_df(joined, df, [&#39;Store&#39;, &#39;Date&#39;]) . joined_test = join_df(joined_test, df, [&#39;Store&#39;, &#39;Date&#39;]) . The authors also removed all instances where the store had zero sale / was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior. . joined = joined[joined.Sales!=0] . We&#39;ll back this up as well. . joined.reset_index(inplace=True) joined_test.reset_index(inplace=True) . joined.to_pickle(PATH/&#39;train_clean&#39;) joined_test.to_pickle(PATH/&#39;test_clean&#39;) .",
            "url": "https://mrg-ai.github.io/blog/2019/12/12/Exploratory-Data-Analysis-Rossman-Data.html",
            "relUrl": "/2019/12/12/Exploratory-Data-Analysis-Rossman-Data.html",
            "date": " • Dec 12, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "https://twitter.com/ParamsAndActivs . http://mrgwrites.blogspot.com/ .",
          "url": "https://mrg-ai.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mrg-ai.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}