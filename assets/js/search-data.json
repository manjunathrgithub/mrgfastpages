{
  
    
        "post0": {
            "title": "Machine Learning Notes",
            "content": "Deep Learning (Neural Networks) are much better to deal with most problems which all these algorithms can do. . Libraries – sklearn (scikit), IPython . Pandas and Numpy work well with each other. . Different Machine Learning Algorithms - . https://scikit-learn.org/stable/supervised_learning.html . https://towardsdatascience.com/do-you-know-how-to-choose-the-right-machine-learning-algorithm-among-7-different-types-295d0b0c7f60 . Prediction of Auction price for Bulldozers using Random Forests is the exercise. . Curse of Dimensionality and No Free Lunch theorem do not generally apply in real world. . https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-1-84a1dc2b5236 | . RandomForestRegressor — regressor is a method for predicting continuous variables (i.e. regression) . RandomForestClassifier — classifier is a method for predicting categorical variables (i.e. classification) . For Notebooks – There is a bit of work which needs to be done before it is usable. . !pip install fastai==0.7.0 . !pip3 install torch torchvision . !pip install ”torchtext==0.3.1” . !pip3 install spacy  . !python -m spacy download en . from fastai.imports import * . from fastai.structured import * . from torch import *  . from pandas_summary import DataFrameSummary . from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier . from IPython.display import display . from sklearn import metrics . Data will also have to be uploaded and will vanish once the Colab is disconnected. . For Lesson 1 – train.csv was uploaded after creating necessary folders on the colab machine. . Alternate solution for data is to use google drive and using that path in the program or downloading from URL (Kaggle or Google Drive). Use whichever works and is the fastest/easiest. . Random Forest Exercise Summary . Refer the Jupyter notebook for details - https://colab.research.google.com/drive/1ARrxUcm8cJdQ0J7sONa5FfQuan9nZSDH#scrollTo=6jeziZfhnA2x . Take an input csv. (CSV can be populated by any means with whatever relevant data is needed for your requirement) . | The goal is to predict a measure (in this case Auction Price) . | Look at the data to understand what it has type of columns (String, number, date…), NULL values, blank values etc. . | Read the csv data into a Pandas Dataframe. While reading there are options to specify if there are any date columns. Specify that and Pandas will read those as Dates rather than strings. . | Choose a Metric to evaluate the prediction vs actual. . | . It’s important to note what metric is being used for a project. Generally, selecting the metric(s) is an important part of the project setup. However, in this case Kaggle tells us what metric to use: RMSLE (root mean squared log error) to compare between the actual and predicted auction prices. Therefore, we take the log of the prices and then RMSE on top of this will give RMSLE. . The data contains continuous and categorical values. . | Any Date columns needs to be converted to Categoricals. If it is continuous (like how it is in the file i.e. just a date column), then we cannot capture trend and will not be useful for prediction. Below function does that and should be defined in your program and used. It is also present in fastai’s old library. But it converts to String Categoricals. . | Since the above functions converts the date into String Categoricals, we still need to convert it into Panda categoricals. Panda categoricals are like key value pair (0 value1 , 1 value2 and so on) for each of the values. This is needed for efficiency of the Random Forest. Below function is used for that purpose. . | Both these functions change the dataframe in-place. The function will need to be changed if it has to write to a different dataframe. . | Dataframe now has Category Objects in it for all non-numerical columns (each non-numerical column we saw in csv is a category object now). There are functions which can be run on Category objects in Python. Cat is one such function and it further has child functions. . | We now check for blank or missing values for all category objects. Below can give the percentage of such values in each of them. We will deal with them in a while. . | . display (df_raw.isnull().sum().sort_index()/len(df_raw)) . We can save the dataframe at such a point in a feather format. Saves to disc in the same format as it was in RAM. . | We can load it back from disc to a dataframe again later. . | Then we process the dataframe (proc_df) to fix missing values and more importantly makes a copy of dataframe, drops the dependent value (in this case Auction Price object) in this copy dataframe and converts categorical data into their respective codes It also adds 1 to codes to convert Null categories which will have code as -1 to 0 and this also changes the other codes to 1,2,3 and so on. Below is the function which can help with that. This function further uses other child functions mentioned below. . | . The original dataframe is used to get the dependent value into a “y” variable. | . The above output is in a format that can be passed into Random Forest Regressor. . In statistics, the coefficient of determination, denoted R2 or r2 and pronounced “R squared”, is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). https://en.wikipedia.org/wiki/Coefficient_of_determination . m = RandomForestRegressor(n_jobs=-1) . m.fit(df, y) . m.score(df,y) . However, this could be an overfit since we have used all the data from train.csv . We need to verify this using a validation set. There are various hyperparameters that need to be set and various methods to be employed to get balances Random forest which has a balance between variance (highly overfit for Training data) and bias (highly biased even with Training data) . Good read - https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76 . Different options for Random Forest Regressor. . https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html .",
            "url": "https://mrg-ai.github.io/blog/2020/09/20/Machine-Learning-Notes.html",
            "relUrl": "/2020/09/20/Machine-Learning-Notes.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Rossmann",
            "content": "%reload_ext autoreload %autoreload 2 . from fastai.basics import * import pandas as pd import matplotlib.pyplot as plt . !curl -s https://course.fast.ai/setup/colab | bash . Updating fastai... Done. . Data preparation / Feature engineering . In addition to the provided data, we will be using external datasets put together by participants in the Kaggle competition. You can download all of them here. Then you shold untar them in the dirctory to which PATH is pointing below. . For completeness, the implementation used to put them together is included below. . untar_data(&#39;http://files.fast.ai/part2/lesson14/rossmann&#39;,dest=&#39;/content/data/rossmann&#39;) . PosixPath(&#39;/content/data/rossmann/rossmann&#39;) . (Config().data_path()/&#39;rossmann&#39;).ls() . [PosixPath(&#39;/root/.fastai/data/rossmann/googletrend.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/weather.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/store_states.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/state_names.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/sample_submission.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/train.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/store.csv&#39;), PosixPath(&#39;/root/.fastai/data/rossmann/test.csv&#39;)] . (Config().data_path()/&#39;rossmann&#39;) . PosixPath(&#39;/root/.fastai/data/rossmann&#39;) . PATH=(Config().data_path()/&#39;rossmann&#39;) . table_names = [&#39;train&#39;, &#39;store&#39;, &#39;store_states&#39;, &#39;state_names&#39;, &#39;googletrend&#39;, &#39;weather&#39;, &#39;test&#39;] type(table_names) . list . ??pd.read_csv . tables = [pd.read_csv(PATH/f&#39;{fname}.csv&#39;, low_memory=False) for fname in table_names] train, store, store_states, state_names, googletrend, weather, test = tables len(train),len(test) . (1017209, 41088) . ??train.isnull() . Define functions for Data Exploration . def data_shape_and_head(df): pd.set_option(&#39;float_format&#39;, &#39;{:f}&#39;.format) print(f&quot;Data Frame Shape: {df.shape}&quot;) return df.head() . def percentage_of_null_data(df): pd.options.mode.use_inf_as_na=True total = df.isnull().sum() percent = (df.isnull().sum()/df.isnull().count()*100) tt = pd.concat([total, percent], axis=1, keys=[&#39;Total&#39;, &#39;Percent&#39;]) types = [] for col in df.columns: dtype = str(df[col].dtype) types.append(dtype) tt[&#39;Types&#39;] = types return(np.transpose(tt)) . def unique_values_eachcolumn(df): for column in df.columns.values: print(f&quot;[df] Unique values of &#39;{column}&#39; : {df[column].nunique()}&quot;) . def plot_col_count_4_top20(column_name, title, df, size=6): # Displays the count of records for each value of the column. # Displays data for first 20 most frequent values import seaborn as sns f, ax = plt.subplots(1,1, figsize=(4*size,4)) total = float(len(df)) g = sns.countplot(df[column_name], order = df[column_name].value_counts().index[:30], palette=&#39;Dark2&#39;) g.set_title(&quot;Number and Percentage of {}&quot;.format(title)) if(size &gt; 2): plt.xticks(rotation=90, size=8) for p in ax.patches: height = p.get_height() ax.text(p.get_x()+p.get_width()/2., height + 3, &#39;{:1.2f}%&#39;.format(100*height/total), ha=&quot;center&quot;) plt.show() . def plot_most_or_least_populated_data(df,most=True): import seaborn as sns total = df.isnull().count() - df.isnull().sum() percent = 100 - (df.isnull().sum()/df.isnull().count()*100) tt = pd.concat([total, percent], axis=1, keys=[&#39;Total&#39;, &#39;Percent&#39;]) tt = pd.DataFrame(tt.reset_index()) tt= (tt.sort_values([&#39;Total&#39;], ascending=most)) plt.figure(figsize=(10, 8)) sns.set(style=&#39;darkgrid&#39;) ax = sns.barplot(x=&#39;Percent&#39;, y=&#39;index&#39;, data=tt.head(30), color=&#39;DarkOrange&#39;) plt.title((&#39;Most&#39; if most else &#39;Least&#39; ) + &#39; frequent columns/features in the dataframe&#39;) plt.ylabel(&#39;Features/Columns&#39;) plt.show() . Do the data exploration for train Dataframe . data_shape_and_head(train) . Data Frame Shape: (1017209, 9) . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | 0 | 1 | . 1 2 | 5 | 2015-07-31 | 6064 | 625 | 1 | 1 | 0 | 1 | . 2 3 | 5 | 2015-07-31 | 8314 | 821 | 1 | 1 | 0 | 1 | . 3 4 | 5 | 2015-07-31 | 13995 | 1498 | 1 | 1 | 0 | 1 | . 4 5 | 5 | 2015-07-31 | 4822 | 559 | 1 | 1 | 0 | 1 | . train.describe() . Store DayOfWeek Sales Customers Open Promo SchoolHoliday . count 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | . mean 558.429727 | 3.998341 | 5773.818972 | 633.145946 | 0.830107 | 0.381515 | 0.178647 | . std 321.908651 | 1.997391 | 3849.926175 | 464.411734 | 0.375539 | 0.485759 | 0.383056 | . min 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 280.000000 | 2.000000 | 3727.000000 | 405.000000 | 1.000000 | 0.000000 | 0.000000 | . 50% 558.000000 | 4.000000 | 5744.000000 | 609.000000 | 1.000000 | 0.000000 | 0.000000 | . 75% 838.000000 | 6.000000 | 7856.000000 | 837.000000 | 1.000000 | 1.000000 | 0.000000 | . max 1115.000000 | 7.000000 | 41551.000000 | 7388.000000 | 1.000000 | 1.000000 | 1.000000 | . train.describe().T . count mean std min 25% 50% 75% max . Store 1017209.000000 | 558.429727 | 321.908651 | 1.000000 | 280.000000 | 558.000000 | 838.000000 | 1115.000000 | . DayOfWeek 1017209.000000 | 3.998341 | 1.997391 | 1.000000 | 2.000000 | 4.000000 | 6.000000 | 7.000000 | . Sales 1017209.000000 | 5773.818972 | 3849.926175 | 0.000000 | 3727.000000 | 5744.000000 | 7856.000000 | 41551.000000 | . Customers 1017209.000000 | 633.145946 | 464.411734 | 0.000000 | 405.000000 | 609.000000 | 837.000000 | 7388.000000 | . Open 1017209.000000 | 0.830107 | 0.375539 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . Promo 1017209.000000 | 0.381515 | 0.485759 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | . SchoolHoliday 1017209.000000 | 0.178647 | 0.383056 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | . train.count() . Store 1017209 DayOfWeek 1017209 Date 1017209 Sales 1017209 Customers 1017209 Open 1017209 Promo 1017209 StateHoliday 1017209 SchoolHoliday 1017209 dtype: int64 . train[&#39;DayOfWeek&#39;].count() . 1017209 . unique_values_eachcolumn(train) . [df] Unique values of &#39;Store&#39; : 1115 [df] Unique values of &#39;DayOfWeek&#39; : 7 [df] Unique values of &#39;Date&#39; : 942 [df] Unique values of &#39;Sales&#39; : 21734 [df] Unique values of &#39;Customers&#39; : 4086 [df] Unique values of &#39;Open&#39; : 2 [df] Unique values of &#39;Promo&#39; : 2 [df] Unique values of &#39;StateHoliday&#39; : 4 [df] Unique values of &#39;SchoolHoliday&#39; : 2 . plot_col_count_4_top20(&#39;Date&#39;, &#39;Counts&#39;, train,size=6) . plot_most_or_least_populated_data(train) . train.DayOfWeek.nunique() . 7 . train.StateHoliday.unique() . array([&#39;0&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;], dtype=object) . percentage_of_null_data(train) . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday . Total 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Percent 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . Types int64 | int64 | object | int64 | int64 | int64 | int64 | object | int64 | . train.Date . 0 2015-07-31 1 2015-07-31 2 2015-07-31 3 2015-07-31 4 2015-07-31 ... 1017204 2013-01-01 1017205 2013-01-01 1017206 2013-01-01 1017207 2013-01-01 1017208 2013-01-01 Name: Date, Length: 1017209, dtype: object . train.groupby(&#39;Date&#39;).max().Sales . Date 2013-01-01 17267 2013-01-02 25357 2013-01-03 23303 2013-01-04 21996 2013-01-05 22521 ... 2015-07-27 27881 2015-07-28 25518 2015-07-29 25840 2015-07-30 24395 2015-07-31 27508 Name: Sales, Length: 942, dtype: int64 . train.sample(5, random_state=300).groupby(&#39;Date&#39;).max().Customers . Date 2013-07-10 440 2014-05-09 540 2014-11-02 0 2015-04-06 0 2015-04-20 482 Name: Customers, dtype: int64 . train.sample(20, random_state=5).groupby(&#39;Date&#39;).max().Sales.plot(kind=&#39;barh&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f85651a9828&gt; . train.groupby(&#39;DayOfWeek&#39;).DayOfWeek . &lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x7f85650f2198&gt; . (train.groupby(&#39;DayOfWeek&#39;).sum()) . Store Sales Customers Open Promo SchoolHoliday . DayOfWeek . 1 80821168 | 1130203012 | 117675012 | 137560 | 77760 | 34060 | . 2 81344288 | 1020411930 | 110848063 | 143961 | 77580 | 36595 | . 3 81345276 | 954962863 | 105117642 | 141936 | 77580 | 34636 | . 4 81443338 | 911177709 | 101732938 | 134644 | 77580 | 34747 | . 5 81443338 | 980555941 | 108384820 | 138640 | 77580 | 36235 | . 6 80821168 | 846317735 | 95103854 | 144058 | 0 | 2724 | . 7 80821168 | 29551433 | 5179426 | 3593 | 0 | 2724 | . train[0:10] . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | 0 | 1 | . 1 2 | 5 | 2015-07-31 | 6064 | 625 | 1 | 1 | 0 | 1 | . 2 3 | 5 | 2015-07-31 | 8314 | 821 | 1 | 1 | 0 | 1 | . 3 4 | 5 | 2015-07-31 | 13995 | 1498 | 1 | 1 | 0 | 1 | . 4 5 | 5 | 2015-07-31 | 4822 | 559 | 1 | 1 | 0 | 1 | . 5 6 | 5 | 2015-07-31 | 5651 | 589 | 1 | 1 | 0 | 1 | . 6 7 | 5 | 2015-07-31 | 15344 | 1414 | 1 | 1 | 0 | 1 | . 7 8 | 5 | 2015-07-31 | 8492 | 833 | 1 | 1 | 0 | 1 | . 8 9 | 5 | 2015-07-31 | 8565 | 687 | 1 | 1 | 0 | 1 | . 9 10 | 5 | 2015-07-31 | 7185 | 681 | 1 | 1 | 0 | 1 | . We looked at the train data. Similarly we can look at other tabular data as well and figure out the features that we can use. . . We turn state Holidays to booleans, to make them more convenient for modeling. We can do calculations on pandas fields using notation very similar (often identical) to numpy. . train.StateHoliday = train.StateHoliday!=&#39;0&#39; test.StateHoliday = test.StateHoliday!=&#39;0&#39; . join_df is a function for joining tables on specific fields. By default, we&#39;ll be doing a left outer join of right on the left argument using the given fields for each table. . Pandas does joins using the merge method. The suffixes argument describes the naming convention for duplicate fields. We&#39;ve elected to leave the duplicate field names on the left untouched, and append a &quot;_y&quot; to those on the right. . ??pd.merge() . def join_df(left, right, left_on, right_on=None, suffix=&#39;_y&#39;): if right_on is None: right_on = left_on return left.merge(right, how=&#39;left&#39;, left_on=left_on, right_on=right_on, suffixes=(&quot;&quot;, suffix)) . Join weather/state names. . weather = join_df(weather, state_names, &quot;file&quot;, &quot;StateName&quot;) . weather . file Date Max_TemperatureC Mean_TemperatureC Min_TemperatureC Dew_PointC MeanDew_PointC Min_DewpointC Max_Humidity Mean_Humidity Min_Humidity Max_Sea_Level_PressurehPa Mean_Sea_Level_PressurehPa Min_Sea_Level_PressurehPa Max_VisibilityKm Mean_VisibilityKm Min_VisibilitykM Max_Wind_SpeedKm_h Mean_Wind_SpeedKm_h Max_Gust_SpeedKm_h Precipitationmm CloudCover Events WindDirDegrees StateName State . 0 NordrheinWestfalen | 2013-01-01 | 8 | 4 | 2 | 7 | 5 | 1 | 94 | 87 | 65 | 1013 | 1005 | 1001 | 31.000000 | 12.000000 | 4.000000 | 39 | 26 | 58.000000 | 5.080000 | 6.000000 | Rain | 215 | NordrheinWestfalen | NW | . 1 NordrheinWestfalen | 2013-01-02 | 7 | 4 | 1 | 5 | 3 | 2 | 93 | 85 | 78 | 1028 | 1022 | 1014 | 31.000000 | 14.000000 | 10.000000 | 24 | 16 | nan | 0.000000 | 6.000000 | Rain | 225 | NordrheinWestfalen | NW | . 2 NordrheinWestfalen | 2013-01-03 | 11 | 8 | 6 | 10 | 8 | 4 | 100 | 93 | 77 | 1035 | 1030 | 1026 | 31.000000 | 8.000000 | 2.000000 | 26 | 21 | nan | 1.020000 | 7.000000 | Rain | 240 | NordrheinWestfalen | NW | . 3 NordrheinWestfalen | 2013-01-04 | 9 | 9 | 8 | 9 | 9 | 8 | 100 | 94 | 87 | 1036 | 1035 | 1034 | 11.000000 | 5.000000 | 2.000000 | 23 | 14 | nan | 0.250000 | 7.000000 | Rain | 263 | NordrheinWestfalen | NW | . 4 NordrheinWestfalen | 2013-01-05 | 8 | 8 | 7 | 8 | 7 | 6 | 100 | 94 | 87 | 1035 | 1034 | 1033 | 10.000000 | 6.000000 | 3.000000 | 16 | 10 | nan | 0.000000 | 7.000000 | Rain | 268 | NordrheinWestfalen | NW | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15835 Saarland | 2015-09-13 | 21 | 17 | 12 | 16 | 14 | 12 | 100 | 88 | 68 | 1015 | 1010 | 1005 | 31.000000 | 15.000000 | 10.000000 | 27 | 10 | 50.000000 | 1.020000 | 7.000000 | Rain | 113 | Saarland | SL | . 15836 Saarland | 2015-09-14 | 18 | 14 | 11 | 15 | 12 | 7 | 99 | 85 | 61 | 1009 | 1005 | 1004 | 31.000000 | 13.000000 | 4.000000 | 32 | 16 | 53.000000 | 9.910000 | 5.000000 | Rain-Thunderstorm | 213 | Saarland | SL | . 15837 Saarland | 2015-09-15 | 16 | 12 | 9 | 11 | 8 | 7 | 93 | 77 | 62 | 1010 | 1008 | 1004 | 31.000000 | 12.000000 | 10.000000 | 34 | 14 | nan | 0.000000 | 5.000000 | Rain | 193 | Saarland | SL | . 15838 Saarland | 2015-09-16 | 19 | 15 | 11 | 16 | 13 | 10 | 97 | 90 | 75 | 1004 | 999 | 995 | 31.000000 | 10.000000 | 4.000000 | 32 | 14 | 45.000000 | 20.070000 | 7.000000 | Rain-Thunderstorm | 147 | Saarland | SL | . 15839 Saarland | 2015-09-17 | 14 | 13 | 12 | 14 | 12 | 10 | 99 | 92 | 82 | 1013 | 1005 | 999 | 31.000000 | 14.000000 | 8.000000 | 27 | 16 | 47.000000 | 6.100000 | 6.000000 | Rain | 202 | Saarland | SL | . 15840 rows × 26 columns . In pandas you can add new columns to a dataframe by simply defining it. We&#39;ll do this for googletrends by extracting dates and state names from the given data and adding those columns. . We&#39;re also going to replace all instances of state name &#39;NI&#39; to match the usage in the rest of the data: &#39;HB,NI&#39;. This is a good opportunity to highlight pandas indexing. We can use .loc[rows, cols] to select a list of rows and a list of columns from the dataframe. In this case, we&#39;re selecting rows w/ statename &#39;NI&#39; by using a boolean list googletrend.State==&#39;NI&#39; and selecting &quot;State&quot;. . googletrend.index[5] . 5 . googletrend.week.str.split(&#39; - &#39;, expand=True)[1] . 0 2012-12-08 1 2012-12-15 2 2012-12-22 3 2012-12-29 4 2013-01-05 ... 2067 2015-09-05 2068 2015-09-12 2069 2015-09-19 2070 2015-09-26 2071 2015-10-03 Name: 1, Length: 2072, dtype: object . googletrend . file week trend Date State . 0 Rossmann_DE_SN | 2012-12-02 - 2012-12-08 | 96 | 2012-12-02 | SN | . 1 Rossmann_DE_SN | 2012-12-09 - 2012-12-15 | 95 | 2012-12-09 | SN | . 2 Rossmann_DE_SN | 2012-12-16 - 2012-12-22 | 91 | 2012-12-16 | SN | . 3 Rossmann_DE_SN | 2012-12-23 - 2012-12-29 | 48 | 2012-12-23 | SN | . 4 Rossmann_DE_SN | 2012-12-30 - 2013-01-05 | 67 | 2012-12-30 | SN | . ... ... | ... | ... | ... | ... | . 2067 Rossmann_DE_SL | 2015-08-30 - 2015-09-05 | 95 | 2015-08-30 | SL | . 2068 Rossmann_DE_SL | 2015-09-06 - 2015-09-12 | 47 | 2015-09-06 | SL | . 2069 Rossmann_DE_SL | 2015-09-13 - 2015-09-19 | 80 | 2015-09-13 | SL | . 2070 Rossmann_DE_SL | 2015-09-20 - 2015-09-26 | 57 | 2015-09-20 | SL | . 2071 Rossmann_DE_SL | 2015-09-27 - 2015-10-03 | 0 | 2015-09-27 | SL | . 2072 rows × 5 columns . googletrend[&#39;Date&#39;] = googletrend.week.str.split(&#39; - &#39;, expand=True)[0] googletrend[&#39;State&#39;] = googletrend.file.str.split(&#39;_&#39;, expand=True)[2] . googletrend.loc[googletrend.State==&#39;NI&#39;, &quot;State&quot;] . Series([], Name: State, dtype: object) . googletrend.loc[googletrend.State==&#39;NI&#39;, &quot;State&quot;] = &#39;HB,NI&#39; . googletrend . file week trend Date State . 0 Rossmann_DE_SN | 2012-12-02 - 2012-12-08 | 96 | 2012-12-02 | SN | . 1 Rossmann_DE_SN | 2012-12-09 - 2012-12-15 | 95 | 2012-12-09 | SN | . 2 Rossmann_DE_SN | 2012-12-16 - 2012-12-22 | 91 | 2012-12-16 | SN | . 3 Rossmann_DE_SN | 2012-12-23 - 2012-12-29 | 48 | 2012-12-23 | SN | . 4 Rossmann_DE_SN | 2012-12-30 - 2013-01-05 | 67 | 2012-12-30 | SN | . ... ... | ... | ... | ... | ... | . 2067 Rossmann_DE_SL | 2015-08-30 - 2015-09-05 | 95 | 2015-08-30 | SL | . 2068 Rossmann_DE_SL | 2015-09-06 - 2015-09-12 | 47 | 2015-09-06 | SL | . 2069 Rossmann_DE_SL | 2015-09-13 - 2015-09-19 | 80 | 2015-09-13 | SL | . 2070 Rossmann_DE_SL | 2015-09-20 - 2015-09-26 | 57 | 2015-09-20 | SL | . 2071 Rossmann_DE_SL | 2015-09-27 - 2015-10-03 | 0 | 2015-09-27 | SL | . 2072 rows × 5 columns . The following extracts particular date fields from a complete datetime for the purpose of constructing categoricals. . You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can&#39;t capture any trend/cyclical behavior as a function of time at any of these granularities. We&#39;ll add to every table with a date field. . def add_datepart(df, fldname, drop=True, time=False): &quot;Helper function that adds columns relevant to a date.&quot; fld = df[fldname] fld_dtype = fld.dtype if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype): fld_dtype = np.datetime64 if not np.issubdtype(fld_dtype, np.datetime64): df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True) targ_pre = re.sub(&#39;[Dd]ate$&#39;, &#39;&#39;, fldname) attr = [&#39;Year&#39;, &#39;Month&#39;, &#39;Week&#39;, &#39;Day&#39;, &#39;Dayofweek&#39;, &#39;Dayofyear&#39;, &#39;Is_month_end&#39;, &#39;Is_month_start&#39;, &#39;Is_quarter_end&#39;, &#39;Is_quarter_start&#39;, &#39;Is_year_end&#39;, &#39;Is_year_start&#39;] if time: attr = attr + [&#39;Hour&#39;, &#39;Minute&#39;, &#39;Second&#39;] for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower()) df[targ_pre + &#39;Elapsed&#39;] = fld.astype(np.int64) // 10 ** 9 if drop: df.drop(fldname, axis=1, inplace=True) . add_datepart(weather, &quot;Date&quot;, drop=False) add_datepart(googletrend, &quot;Date&quot;, drop=False) add_datepart(train, &quot;Date&quot;, drop=False) add_datepart(test, &quot;Date&quot;, drop=False) . The Google trends data has a special category for the whole of the Germany - we&#39;ll pull that out so we can use it explicitly. . trend_de = googletrend[googletrend.file == &#39;Rossmann_DE&#39;] . Now we can outer join all of our data into a single dataframe. Recall that in outer joins everytime a value in the joining field on the left table does not have a corresponding value on the right table, the corresponding row in the new table has Null values for all right table fields. One way to check that all records are consistent and complete is to check for Null values post-join, as we do here. . Aside: Why not just do an inner join? If you are assuming that all records are complete and match on the field you desire, an inner join will do the same thing as an outer join. However, in the event you are wrong or a mistake is made, an outer join followed by a null-check will catch it. (Comparing before/after # of rows for inner join is equivalent, but requires keeping track of before/after row #&#39;s. Outer join is easier.) . store = join_df(store, store_states, &quot;Store&quot;) len(store[store.State.isnull()]) . 0 . ??join_df . joined = join_df(train, store, &quot;Store&quot;) joined_test = join_df(test, store, &quot;Store&quot;) len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()]) . (0, 0) . joined = join_df(joined, googletrend, [&quot;State&quot;,&quot;Year&quot;, &quot;Week&quot;]) joined_test = join_df(joined_test, googletrend, [&quot;State&quot;,&quot;Year&quot;, &quot;Week&quot;]) len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()]) . (0, 0) . joined = joined.merge(trend_de, &#39;left&#39;, [&quot;Year&quot;, &quot;Week&quot;], suffixes=(&#39;&#39;, &#39;_DE&#39;)) joined_test = joined_test.merge(trend_de, &#39;left&#39;, [&quot;Year&quot;, &quot;Week&quot;], suffixes=(&#39;&#39;, &#39;_DE&#39;)) len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()]) . (0, 0) . joined = join_df(joined, weather, [&quot;State&quot;,&quot;Date&quot;]) joined_test = join_df(joined_test, weather, [&quot;State&quot;,&quot;Date&quot;]) len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()]) . (0, 0) . joined.describe() . Store DayOfWeek Sales Customers Open Promo SchoolHoliday Year Month Week Day Dayofweek Dayofyear Elapsed CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear trend Month_y Day_y Dayofweek_y Dayofyear_y Elapsed_y trend_DE Month_DE Day_DE Dayofweek_DE Dayofyear_DE Elapsed_DE Max_TemperatureC Mean_TemperatureC Min_TemperatureC Dew_PointC MeanDew_PointC Min_DewpointC Max_Humidity Mean_Humidity Min_Humidity Max_Sea_Level_PressurehPa Mean_Sea_Level_PressurehPa Min_Sea_Level_PressurehPa Max_VisibilityKm Mean_VisibilityKm Min_VisibilitykM Max_Wind_SpeedKm_h Mean_Wind_SpeedKm_h Max_Gust_SpeedKm_h Precipitationmm CloudCover WindDirDegrees Year_y Month_y Week_y Day_y Dayofweek_y Dayofyear_y Elapsed_y . count 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1014567.000000 | 693861.000000 | 693861.000000 | 1017209.000000 | 509178.000000 | 509178.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 992564.000000 | 992564.000000 | 992564.000000 | 1017209.000000 | 1017209.000000 | 236167.000000 | 1017209.000000 | 936890.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | 1017209.000000 | . mean 558.429727 | 3.998341 | 5773.818972 | 633.145946 | 0.830107 | 0.381515 | 0.178647 | 2013.832292 | 5.846762 | 23.615515 | 15.702790 | 2.998341 | 162.276385 | 1397179842.846062 | 5430.085652 | 7.222866 | 2008.690228 | 0.500564 | 23.269093 | 2011.752774 | 65.219190 | 5.891212 | 15.562106 | 6.000000 | 163.476310 | 1397283516.407739 | 66.234798 | 5.891212 | 15.562106 | 6.000000 | 163.476310 | 1397283516.407739 | 14.328300 | 10.073856 | 5.900282 | 8.216752 | 5.835002 | 3.263247 | 93.283786 | 73.952151 | 49.923426 | 1018.644120 | 1015.429182 | 1012.428608 | 23.542818 | 12.127392 | 7.284987 | 22.672915 | 11.870788 | 48.459272 | 0.790271 | 5.521105 | 177.063058 | 2013.832292 | 5.846762 | 23.615515 | 15.702790 | 2.998341 | 162.276385 | 1397179842.846062 | . std 321.908651 | 1.997391 | 3849.926175 | 464.411734 | 0.375539 | 0.485759 | 0.383056 | 0.777396 | 3.326097 | 14.433381 | 8.787638 | 1.997391 | 101.616189 | 23712834.841055 | 7715.323700 | 3.211832 | 5.992644 | 0.500000 | 14.095973 | 1.662870 | 11.339865 | 3.308500 | 8.802947 | 0.000000 | 101.235953 | 23757279.032632 | 9.618326 | 3.308500 | 8.802947 | 0.000000 | 101.235953 | 23757279.032632 | 8.464778 | 7.239083 | 6.459730 | 6.140112 | 5.993374 | 6.033076 | 7.717332 | 13.206449 | 19.626353 | 7.915110 | 8.248712 | 8.748368 | 9.222368 | 4.871022 | 4.771402 | 8.940418 | 5.897191 | 13.142078 | 2.502615 | 1.683795 | 101.748176 | 0.777396 | 3.326097 | 14.433381 | 8.787638 | 1.997391 | 101.616189 | 23712834.841055 | . min 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 2013.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | 1356998400.000000 | 20.000000 | 1.000000 | 1900.000000 | 0.000000 | 1.000000 | 2009.000000 | 28.000000 | 1.000000 | 1.000000 | 6.000000 | 4.000000 | 1357430400.000000 | 50.000000 | 1.000000 | 1.000000 | 6.000000 | 4.000000 | 1357430400.000000 | -11.000000 | -13.000000 | -15.000000 | -14.000000 | -15.000000 | -73.000000 | 44.000000 | 30.000000 | 4.000000 | 976.000000 | 974.000000 | 970.000000 | 0.000000 | 0.000000 | 0.000000 | 3.000000 | 2.000000 | 21.000000 | 0.000000 | 0.000000 | -1.000000 | 2013.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | 1356998400.000000 | . 25% 280.000000 | 2.000000 | 3727.000000 | 405.000000 | 1.000000 | 0.000000 | 0.000000 | 2013.000000 | 3.000000 | 11.000000 | 8.000000 | 1.000000 | 77.000000 | 1376697600.000000 | 710.000000 | 4.000000 | 2006.000000 | 0.000000 | 13.000000 | 2011.000000 | 57.000000 | 3.000000 | 8.000000 | 6.000000 | 76.000000 | 1376784000.000000 | 58.000000 | 3.000000 | 8.000000 | 6.000000 | 76.000000 | 1376784000.000000 | 8.000000 | 4.000000 | 1.000000 | 3.000000 | 1.000000 | -1.000000 | 88.000000 | 64.000000 | 34.000000 | 1014.000000 | 1011.000000 | 1007.000000 | 11.000000 | 10.000000 | 3.000000 | 16.000000 | 8.000000 | 39.000000 | 0.000000 | 5.000000 | 79.000000 | 2013.000000 | 3.000000 | 11.000000 | 8.000000 | 1.000000 | 77.000000 | 1376697600.000000 | . 50% 558.000000 | 4.000000 | 5744.000000 | 609.000000 | 1.000000 | 0.000000 | 0.000000 | 2014.000000 | 6.000000 | 22.000000 | 16.000000 | 3.000000 | 153.000000 | 1396396800.000000 | 2330.000000 | 8.000000 | 2010.000000 | 1.000000 | 22.000000 | 2012.000000 | 65.000000 | 6.000000 | 15.000000 | 6.000000 | 153.000000 | 1396137600.000000 | 66.000000 | 6.000000 | 15.000000 | 6.000000 | 153.000000 | 1396137600.000000 | 15.000000 | 10.000000 | 6.000000 | 8.000000 | 6.000000 | 3.000000 | 94.000000 | 75.000000 | 49.000000 | 1019.000000 | 1016.000000 | 1013.000000 | 31.000000 | 11.000000 | 8.000000 | 21.000000 | 11.000000 | 47.000000 | 0.000000 | 6.000000 | 201.000000 | 2014.000000 | 6.000000 | 22.000000 | 16.000000 | 3.000000 | 153.000000 | 1396396800.000000 | . 75% 838.000000 | 6.000000 | 7856.000000 | 837.000000 | 1.000000 | 1.000000 | 0.000000 | 2014.000000 | 8.000000 | 35.000000 | 23.000000 | 5.000000 | 241.000000 | 1418342400.000000 | 6890.000000 | 10.000000 | 2013.000000 | 1.000000 | 37.000000 | 2013.000000 | 73.000000 | 8.000000 | 23.000000 | 6.000000 | 243.000000 | 1418515200.000000 | 72.000000 | 8.000000 | 23.000000 | 6.000000 | 243.000000 | 1418515200.000000 | 21.000000 | 16.000000 | 11.000000 | 13.000000 | 11.000000 | 8.000000 | 100.000000 | 84.000000 | 65.000000 | 1024.000000 | 1021.000000 | 1018.000000 | 31.000000 | 14.000000 | 10.000000 | 27.000000 | 14.000000 | 55.000000 | 0.250000 | 7.000000 | 257.000000 | 2014.000000 | 8.000000 | 35.000000 | 23.000000 | 5.000000 | 241.000000 | 1418342400.000000 | . max 1115.000000 | 7.000000 | 41551.000000 | 7388.000000 | 1.000000 | 1.000000 | 1.000000 | 2015.000000 | 12.000000 | 52.000000 | 31.000000 | 6.000000 | 365.000000 | 1438300800.000000 | 75860.000000 | 12.000000 | 2015.000000 | 1.000000 | 50.000000 | 2015.000000 | 100.000000 | 12.000000 | 31.000000 | 6.000000 | 363.000000 | 1438473600.000000 | 100.000000 | 12.000000 | 31.000000 | 6.000000 | 363.000000 | 1438473600.000000 | 38.000000 | 31.000000 | 24.000000 | 25.000000 | 20.000000 | 18.000000 | 100.000000 | 100.000000 | 100.000000 | 1043.000000 | 1040.000000 | 1038.000000 | 31.000000 | 31.000000 | 31.000000 | 101.000000 | 53.000000 | 111.000000 | 58.930000 | 8.000000 | 360.000000 | 2015.000000 | 12.000000 | 52.000000 | 31.000000 | 6.000000 | 365.000000 | 1438300800.000000 | . for df in (joined, joined_test): for c in df.columns: if c.endswith(&#39;_y&#39;): if c in df.columns: print(c) . Date_y Month_y Day_y Dayofweek_y Dayofyear_y Is_month_end_y Is_month_start_y Is_quarter_end_y Is_quarter_start_y Is_year_end_y Is_year_start_y Elapsed_y file_y Year_y Month_y Week_y Day_y Dayofweek_y Dayofyear_y Is_month_end_y Is_month_start_y Is_quarter_end_y Is_quarter_start_y Is_year_end_y Is_year_start_y Elapsed_y Date_y Month_y Day_y Dayofweek_y Dayofyear_y Is_month_end_y Is_month_start_y Is_quarter_end_y Is_quarter_start_y Is_year_end_y Is_year_start_y Elapsed_y file_y Year_y Month_y Week_y Day_y Dayofweek_y Dayofyear_y Is_month_end_y Is_month_start_y Is_quarter_end_y Is_quarter_start_y Is_year_end_y Is_year_start_y Elapsed_y . for df in (joined, joined_test): for c in df.columns: if c.endswith(&#39;_y&#39;): if c in df.columns: df.drop(c, inplace=True, axis=1) . Next we&#39;ll fill in missing values to avoid complications with NA&#39;s. NA (not available) is how Pandas indicates missing values; many models have problems when missing values are present, so it&#39;s always important to think about how to deal with them. In these cases, we are picking an arbitrary signal value that doesn&#39;t otherwise appear in the data. . joined.shape . (1017209, 74) . for column in joined.columns: print(column) . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday Year Month Week Day Dayofweek Dayofyear Is_month_end Is_month_start Is_quarter_end Is_quarter_start Is_year_end Is_year_start Elapsed StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval State file week trend file_DE week_DE trend_DE Date_DE State_DE Month_DE Day_DE Dayofweek_DE Dayofyear_DE Is_month_end_DE Is_month_start_DE Is_quarter_end_DE Is_quarter_start_DE Is_year_end_DE Is_year_start_DE Elapsed_DE Max_TemperatureC Mean_TemperatureC Min_TemperatureC Dew_PointC MeanDew_PointC Min_DewpointC Max_Humidity Mean_Humidity Min_Humidity Max_Sea_Level_PressurehPa Mean_Sea_Level_PressurehPa Min_Sea_Level_PressurehPa Max_VisibilityKm Mean_VisibilityKm Min_VisibilitykM Max_Wind_SpeedKm_h Mean_Wind_SpeedKm_h Max_Gust_SpeedKm_h Precipitationmm CloudCover Events WindDirDegrees StateName . len(joined.loc[joined.StateName.isna() , &quot;StateName&quot;]) . 0 . for df in (joined,joined_test): df[&#39;CompetitionOpenSinceYear&#39;] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32) df[&#39;CompetitionOpenSinceMonth&#39;] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32) df[&#39;Promo2SinceYear&#39;] = df.Promo2SinceYear.fillna(1900).astype(np.int32) df[&#39;Promo2SinceWeek&#39;] = df.Promo2SinceWeek.fillna(1).astype(np.int32) . Next we&#39;ll extract features &quot;CompetitionOpenSince&quot; and &quot;CompetitionDaysOpen&quot;. Note the use of apply() in mapping a function across dataframe values. . for df in (joined,joined_test): df[&quot;CompetitionOpenSince&quot;] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, month=df.CompetitionOpenSinceMonth, day=15)) df[&quot;CompetitionDaysOpen&quot;] = df.Date.subtract(df.CompetitionOpenSince).dt.days . We&#39;ll replace some erroneous / outlying data. . for df in (joined,joined_test): df.loc[df.CompetitionDaysOpen&lt;0, &quot;CompetitionDaysOpen&quot;] = 0 df.loc[df.CompetitionOpenSinceYear&lt;1990, &quot;CompetitionDaysOpen&quot;] = 0 . We add &quot;CompetitionMonthsOpen&quot; field, limiting the maximum to 2 years to limit number of unique categories. . for df in (joined,joined_test): df[&quot;CompetitionMonthsOpen&quot;] = df[&quot;CompetitionDaysOpen&quot;]//30 df.loc[df.CompetitionMonthsOpen&gt;24, &quot;CompetitionMonthsOpen&quot;] = 24 joined.CompetitionMonthsOpen.unique() . array([24, 3, 19, 9, 0, 16, 17, 7, 15, 22, 11, 13, 2, 23, 12, 4, 10, 1, 14, 20, 8, 18, 6, 21, 5]) . Same process for Promo dates. You may need to install the isoweek package first. . ! pip install isoweek . Collecting isoweek Downloading https://files.pythonhosted.org/packages/c2/d4/fe7e2637975c476734fcbf53776e650a29680194eb0dd21dbdc020ca92de/isoweek-1.3.3-py2.py3-none-any.whl Installing collected packages: isoweek Successfully installed isoweek-1.3.3 . from isoweek import Week for df in (joined,joined_test): df[&quot;Promo2Since&quot;] = pd.to_datetime(df.apply(lambda x: Week( x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1)) df[&quot;Promo2Days&quot;] = df.Date.subtract(df[&quot;Promo2Since&quot;]).dt.days . joined . Store DayOfWeek Date Sales Customers Open Promo StateHoliday SchoolHoliday Year Month Week Day Dayofweek Dayofyear Is_month_end Is_month_start Is_quarter_end Is_quarter_start Is_year_end Is_year_start Elapsed StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval State file week trend file_DE week_DE trend_DE Date_DE State_DE Month_DE Day_DE Dayofweek_DE Dayofyear_DE Is_month_end_DE Is_month_start_DE Is_quarter_end_DE Is_quarter_start_DE Is_year_end_DE Is_year_start_DE Elapsed_DE Max_TemperatureC Mean_TemperatureC Min_TemperatureC Dew_PointC MeanDew_PointC Min_DewpointC Max_Humidity Mean_Humidity Min_Humidity Max_Sea_Level_PressurehPa Mean_Sea_Level_PressurehPa Min_Sea_Level_PressurehPa Max_VisibilityKm Mean_VisibilityKm Min_VisibilitykM Max_Wind_SpeedKm_h Mean_Wind_SpeedKm_h Max_Gust_SpeedKm_h Precipitationmm CloudCover Events WindDirDegrees StateName CompetitionOpenSince CompetitionDaysOpen CompetitionMonthsOpen Promo2Since Promo2Days . 0 1 | 5 | 2015-07-31 | 5263 | 555 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | c | a | 1270.000000 | 9 | 2008 | 0 | 1 | 1900 | NaN | HE | Rossmann_DE_HE | 2015-08-02 - 2015-08-08 | 85 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 23 | 16 | 8 | 9 | 6 | 3 | 98 | 54 | 18 | 1021 | 1018 | 1015 | 31.000000 | 15.000000 | 10.000000 | 24 | 11 | nan | 0.000000 | 1.000000 | Fog | 13 | Hessen | 2008-09-15 | 2510 | 24 | 1900-01-01 | 42214 | . 1 2 | 5 | 2015-07-31 | 6064 | 625 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | a | a | 570.000000 | 11 | 2007 | 1 | 13 | 2010 | Jan,Apr,Jul,Oct | TH | Rossmann_DE_TH | 2015-08-02 - 2015-08-08 | 80 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 19 | 13 | 7 | 9 | 6 | 3 | 100 | 62 | 25 | 1021 | 1019 | 1017 | 10.000000 | 10.000000 | 10.000000 | 14 | 11 | nan | 0.000000 | 4.000000 | Fog | 309 | Thueringen | 2007-11-15 | 2815 | 24 | 2010-03-29 | 1950 | . 2 3 | 5 | 2015-07-31 | 8314 | 821 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | a | a | 14130.000000 | 12 | 2006 | 1 | 14 | 2011 | Jan,Apr,Jul,Oct | NW | Rossmann_DE_NW | 2015-08-02 - 2015-08-08 | 86 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 21 | 13 | 6 | 10 | 7 | 4 | 100 | 61 | 24 | 1022 | 1019 | 1017 | 31.000000 | 14.000000 | 10.000000 | 14 | 5 | nan | 0.000000 | 2.000000 | Fog | 354 | NordrheinWestfalen | 2006-12-15 | 3150 | 24 | 2011-04-04 | 1579 | . 3 4 | 5 | 2015-07-31 | 13995 | 1498 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | c | c | 620.000000 | 9 | 2009 | 0 | 1 | 1900 | NaN | BE | Rossmann_DE_BE | 2015-08-02 - 2015-08-08 | 74 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 19 | 14 | 9 | 9 | 7 | 4 | 94 | 61 | 30 | 1019 | 1017 | 1014 | 10.000000 | 10.000000 | 10.000000 | 23 | 16 | nan | 0.000000 | 6.000000 | NaN | 282 | Berlin | 2009-09-15 | 2145 | 24 | 1900-01-01 | 42214 | . 4 5 | 5 | 2015-07-31 | 4822 | 559 | 1 | 1 | False | 1 | 2015 | 7 | 31 | 31 | 4 | 212 | True | False | False | False | False | False | 1438300800 | a | a | 29910.000000 | 4 | 2015 | 0 | 1 | 1900 | NaN | SN | Rossmann_DE_SN | 2015-08-02 - 2015-08-08 | 82 | Rossmann_DE | 2015-08-02 - 2015-08-08 | 83 | 2015-08-02 | None | 8 | 2 | 6 | 214 | False | False | False | False | False | False | 1438473600 | 20 | 15 | 10 | 8 | 6 | 5 | 82 | 55 | 26 | 1020 | 1018 | 1016 | 10.000000 | 10.000000 | 10.000000 | 14 | 11 | nan | 0.000000 | 4.000000 | NaN | 290 | Sachsen | 2015-04-15 | 107 | 3 | 1900-01-01 | 42214 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1017204 1111 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | a | a | 1900.000000 | 6 | 2014 | 1 | 31 | 2013 | Jan,Apr,Jul,Oct | NW | Rossmann_DE_NW | 2013-01-06 - 2013-01-12 | 63 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 4 | 2 | 7 | 5 | 1 | 94 | 87 | 65 | 1013 | 1005 | 1001 | 31.000000 | 12.000000 | 4.000000 | 39 | 26 | 58.000000 | 5.080000 | 6.000000 | Rain | 215 | NordrheinWestfalen | 2014-06-15 | 0 | 0 | 2013-07-29 | -209 | . 1017205 1112 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | c | c | 1880.000000 | 4 | 2006 | 0 | 1 | 1900 | NaN | NW | Rossmann_DE_NW | 2013-01-06 - 2013-01-12 | 63 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 4 | 2 | 7 | 5 | 1 | 94 | 87 | 65 | 1013 | 1005 | 1001 | 31.000000 | 12.000000 | 4.000000 | 39 | 26 | 58.000000 | 5.080000 | 6.000000 | Rain | 215 | NordrheinWestfalen | 2006-04-15 | 2453 | 24 | 1900-01-01 | 41273 | . 1017206 1113 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | a | c | 9260.000000 | 1 | 1900 | 0 | 1 | 1900 | NaN | SH | Rossmann_DE_SH | 2013-01-06 - 2013-01-12 | 72 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 7 | 6 | 7 | 7 | 5 | 100 | 95 | 93 | 1002 | 1000 | 999 | 10.000000 | 8.000000 | 5.000000 | 23 | 16 | nan | 0.000000 | 6.000000 | Rain | 251 | SchleswigHolstein | 1900-01-15 | 0 | 0 | 1900-01-01 | 41273 | . 1017207 1114 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | a | c | 870.000000 | 1 | 1900 | 0 | 1 | 1900 | NaN | HH | Rossmann_DE_HH | 2013-01-06 - 2013-01-12 | 63 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 6 | 4 | 8 | 6 | 3 | 100 | 92 | 70 | 1007 | 1001 | 997 | 31.000000 | 11.000000 | 6.000000 | 40 | 23 | 63.000000 | 6.100000 | 6.000000 | Rain | 234 | Hamburg | 1900-01-15 | 0 | 0 | 1900-01-01 | 41273 | . 1017208 1115 | 2 | 2013-01-01 | 0 | 0 | 0 | 0 | True | 1 | 2013 | 1 | 1 | 1 | 1 | 1 | False | True | False | True | False | True | 1356998400 | d | c | 5350.000000 | 1 | 1900 | 1 | 22 | 2012 | Mar,Jun,Sept,Dec | HE | Rossmann_DE_HE | 2013-01-06 - 2013-01-12 | 73 | Rossmann_DE | 2013-01-06 - 2013-01-12 | 62 | 2013-01-06 | None | 1 | 6 | 6 | 6 | False | False | False | False | False | False | 1357430400 | 8 | 6 | 3 | 6 | 3 | 1 | 93 | 80 | 59 | 1015 | 1008 | 1006 | 31.000000 | 12.000000 | 10.000000 | 23 | 14 | 39.000000 | 2.030000 | 6.000000 | Rain | 206 | Hessen | 1900-01-15 | 0 | 0 | 2012-05-28 | 218 | . 1017209 rows × 79 columns . for df in (joined,joined_test): df.loc[df.Promo2Days&lt;0, &quot;Promo2Days&quot;] = 0 df.loc[df.Promo2SinceYear&lt;1990, &quot;Promo2Days&quot;] = 0 df[&quot;Promo2Weeks&quot;] = df[&quot;Promo2Days&quot;]//7 df.loc[df.Promo2Weeks&lt;0, &quot;Promo2Weeks&quot;] = 0 df.loc[df.Promo2Weeks&gt;25, &quot;Promo2Weeks&quot;] = 25 df.Promo2Weeks.unique() . df.Promo2Weeks.unique() . array([ 0, 25, 24, 15, 20, 23, 14, 19, 22, 13, 18, 21, 12, 17, 11, 16, 10, 9, 8]) . joined.to_pickle(PATH/&#39;joined&#39;) joined_test.to_pickle(PATH/&#39;joined_test&#39;) . Durations . It is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.: . Running averages | Time until next event | Time since last event | . This is often difficult to do with most table manipulation frameworks, since they are designed to work with relationships across columns. As such, we&#39;ve created a class to handle this type of data. . We&#39;ll define a function get_elapsed for cumulative counting across a sorted dataframe. Given a particular field fld to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero. . Upon initialization, this will result in datetime na&#39;s until the field is encountered. This is reset every time a new store is seen. We&#39;ll see how to use this shortly. . def get_elapsed(fld, pre): day1 = np.timedelta64(1, &#39;D&#39;) last_date = np.datetime64() last_store = 0 res = [] for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values): if s != last_store: last_date = np.datetime64() last_store = s if v: last_date = d res.append(((d-last_date).astype(&#39;timedelta64[D]&#39;) / day1)) df[pre+fld] = res . We&#39;ll be applying this to a subset of columns: . columns = [&quot;Date&quot;, &quot;Store&quot;, &quot;Promo&quot;, &quot;StateHoliday&quot;, &quot;SchoolHoliday&quot;] . df = train[columns].append(test[columns]) . Let&#39;s walk through an example. . Say we&#39;re looking at School Holiday. We&#39;ll first sort by Store, then Date, and then call add_elapsed(&#39;SchoolHoliday&#39;, &#39;After&#39;): This will apply to each row with School Holiday: . A applied to every row of the dataframe in order of store and date | Will add to the dataframe the days since seeing a School Holiday | If we sort in the other direction, this will count the days until another holiday. | . ??df.sort_values() . fld = &#39;SchoolHoliday&#39; df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;]) get_elapsed(fld, &#39;After&#39;) df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;], ascending=[True, False]) get_elapsed(fld, &#39;Before&#39;) . df . Date Store Promo StateHoliday SchoolHoliday AfterSchoolHoliday BeforeSchoolHoliday . 0 2015-09-17 | 1 | 1 | False | 0 | 13.000000 | nan | . 856 2015-09-16 | 1 | 1 | False | 0 | 12.000000 | nan | . 1712 2015-09-15 | 1 | 1 | False | 0 | 11.000000 | nan | . 2568 2015-09-14 | 1 | 1 | False | 0 | 10.000000 | nan | . 3424 2015-09-13 | 1 | 0 | False | 0 | 9.000000 | nan | . ... ... | ... | ... | ... | ... | ... | ... | . 1012749 2013-01-05 | 1115 | 0 | False | 1 | 0.000000 | 0.000000 | . 1013864 2013-01-04 | 1115 | 0 | False | 1 | 0.000000 | 0.000000 | . 1014979 2013-01-03 | 1115 | 0 | False | 1 | 0.000000 | 0.000000 | . 1016094 2013-01-02 | 1115 | 0 | False | 1 | 0.000000 | 0.000000 | . 1017208 2013-01-01 | 1115 | 0 | True | 1 | 0.000000 | 0.000000 | . 1058297 rows × 7 columns . We&#39;ll do this for two more fields. . fld = &#39;StateHoliday&#39; df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;]) get_elapsed(fld, &#39;After&#39;) df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;], ascending=[True, False]) get_elapsed(fld, &#39;Before&#39;) . fld = &#39;Promo&#39; df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;]) get_elapsed(fld, &#39;After&#39;) df = df.sort_values([&#39;Store&#39;, &#39;Date&#39;], ascending=[True, False]) get_elapsed(fld, &#39;Before&#39;) . We&#39;re going to set the active index to Date. . df = df.set_index(&quot;Date&quot;) . Then set null values from elapsed field calculations to 0. . columns = [&#39;SchoolHoliday&#39;, &#39;StateHoliday&#39;, &#39;Promo&#39;] . for o in [&#39;Before&#39;, &#39;After&#39;]: for p in columns: a = o+p df[a] = df[a].fillna(0).astype(int) . Next we&#39;ll demonstrate window functions in pandas to calculate rolling quantities. . Here we&#39;re sorting by date (sort_index()) and counting the number of events of interest (sum()) defined in columns in the following week (rolling()), grouped by Store (groupby()). We do the same in the opposite direction. . bwd = df[[&#39;Store&#39;]+columns].sort_index().groupby(&quot;Store&quot;).rolling(7, min_periods=1).sum() . AttributeError Traceback (most recent call last) &lt;ipython-input-158-b3a2d175a68c&gt; in &lt;module&gt;() -&gt; 1 bwd = df[[&#39;Store&#39;]+columns].sort_index().groupby(&#34;Store&#34;).rolling(7, min_periods=1).sum() /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in sort_index(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, by) 5088 5089 indexer = nargsort( -&gt; 5090 labels, kind=kind, ascending=ascending, na_position=na_position 5091 ) 5092 /usr/local/lib/python3.6/dist-packages/pandas/core/sorting.py in nargsort(items, kind, ascending, na_position) 244 245 items = extract_array(items) --&gt; 246 mask = np.asarray(isna(items)) 247 248 if is_extension_array_dtype(items): /usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/missing.py in isna(obj) 120 Name: 1, dtype: bool 121 &#34;&#34;&#34; --&gt; 122 return _isna(obj) 123 124 /usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/missing.py in _isna_old(obj) 177 return _isna_ndarraylike_old(obj) 178 elif isinstance(obj, ABCGeneric): --&gt; 179 return obj._constructor(obj._data.isna(func=_isna_old)) 180 elif isinstance(obj, list): 181 return _isna_ndarraylike_old(np.asarray(obj, dtype=object)) AttributeError: &#39;DatetimeArray&#39; object has no attribute &#39;_constructor&#39; . fwd = df[[&#39;Store&#39;]+columns].sort_index(ascending=False ).groupby(&quot;Store&quot;).rolling(7, min_periods=1).sum() . AttributeError Traceback (most recent call last) &lt;ipython-input-159-05ebca394bc4&gt; in &lt;module&gt;() -&gt; 1 fwd = df[[&#39;Store&#39;]+columns].sort_index(ascending=False 2 ).groupby(&#34;Store&#34;).rolling(7, min_periods=1).sum() /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in sort_index(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, by) 5088 5089 indexer = nargsort( -&gt; 5090 labels, kind=kind, ascending=ascending, na_position=na_position 5091 ) 5092 /usr/local/lib/python3.6/dist-packages/pandas/core/sorting.py in nargsort(items, kind, ascending, na_position) 244 245 items = extract_array(items) --&gt; 246 mask = np.asarray(isna(items)) 247 248 if is_extension_array_dtype(items): /usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/missing.py in isna(obj) 120 Name: 1, dtype: bool 121 &#34;&#34;&#34; --&gt; 122 return _isna(obj) 123 124 /usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/missing.py in _isna_old(obj) 177 return _isna_ndarraylike_old(obj) 178 elif isinstance(obj, ABCGeneric): --&gt; 179 return obj._constructor(obj._data.isna(func=_isna_old)) 180 elif isinstance(obj, list): 181 return _isna_ndarraylike_old(np.asarray(obj, dtype=object)) AttributeError: &#39;DatetimeArray&#39; object has no attribute &#39;_constructor&#39; . Next we want to drop the Store indices grouped together in the window function. . Often in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets. . bwd.drop(&#39;Store&#39;,1,inplace=True) bwd.reset_index(inplace=True) . NameError Traceback (most recent call last) &lt;ipython-input-160-5283e630275b&gt; in &lt;module&gt;() -&gt; 1 bwd.drop(&#39;Store&#39;,1,inplace=True) 2 bwd.reset_index(inplace=True) NameError: name &#39;bwd&#39; is not defined . fwd.drop(&#39;Store&#39;,1,inplace=True) fwd.reset_index(inplace=True) . df.reset_index(inplace=True) . Now we&#39;ll merge these values onto the df. . df = df.merge(bwd, &#39;left&#39;, [&#39;Date&#39;, &#39;Store&#39;], suffixes=[&#39;&#39;, &#39;_bw&#39;]) df = df.merge(fwd, &#39;left&#39;, [&#39;Date&#39;, &#39;Store&#39;], suffixes=[&#39;&#39;, &#39;_fw&#39;]) . df.drop(columns,1,inplace=True) . df.head() . Date Store AfterSchoolHoliday BeforeSchoolHoliday AfterStateHoliday BeforeStateHoliday AfterPromo BeforePromo SchoolHoliday_bw StateHoliday_bw Promo_bw SchoolHoliday_fw StateHoliday_fw Promo_fw . 0 2015-09-17 | 1 | 13 | 0 | 105 | 0 | 0 | 0 | 0.0 | 0.0 | 4.0 | 0.0 | 0.0 | 1.0 | . 1 2015-09-16 | 1 | 12 | 0 | 104 | 0 | 0 | 0 | 0.0 | 0.0 | 3.0 | 0.0 | 0.0 | 2.0 | . 2 2015-09-15 | 1 | 11 | 0 | 103 | 0 | 0 | 0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 3.0 | . 3 2015-09-14 | 1 | 10 | 0 | 102 | 0 | 0 | 0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 4.0 | . 4 2015-09-13 | 1 | 9 | 0 | 101 | 0 | 9 | -1 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.0 | . It&#39;s usually a good idea to back up large tables of extracted / wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it. . df.to_pickle(PATH/&#39;df&#39;) . df[&quot;Date&quot;] = pd.to_datetime(df.Date) . AttributeError Traceback (most recent call last) &lt;ipython-input-155-ccbc6b250f10&gt; in &lt;module&gt;() -&gt; 1 df[&#34;Date&#34;] = pd.to_datetime(df.Date) /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py in __getattr__(self, name) 5177 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5178 return self[name] -&gt; 5179 return object.__getattribute__(self, name) 5180 5181 def __setattr__(self, name, value): AttributeError: &#39;DataFrame&#39; object has no attribute &#39;Date&#39; . df.columns . Index([&#39;Date&#39;, &#39;Store&#39;, &#39;AfterSchoolHoliday&#39;, &#39;BeforeSchoolHoliday&#39;, &#39;AfterStateHoliday&#39;, &#39;BeforeStateHoliday&#39;, &#39;AfterPromo&#39;, &#39;BeforePromo&#39;, &#39;SchoolHoliday_bw&#39;, &#39;StateHoliday_bw&#39;, &#39;Promo_bw&#39;, &#39;SchoolHoliday_fw&#39;, &#39;StateHoliday_fw&#39;, &#39;Promo_fw&#39;], dtype=&#39;object&#39;) . joined = pd.read_pickle(PATH/&#39;joined&#39;) joined_test = pd.read_pickle(PATH/f&#39;joined_test&#39;) . joined = join_df(joined, df, [&#39;Store&#39;, &#39;Date&#39;]) . joined_test = join_df(joined_test, df, [&#39;Store&#39;, &#39;Date&#39;]) . The authors also removed all instances where the store had zero sale / was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior. . joined = joined[joined.Sales!=0] . We&#39;ll back this up as well. . joined.reset_index(inplace=True) joined_test.reset_index(inplace=True) . joined.to_pickle(PATH/&#39;train_clean&#39;) joined_test.to_pickle(PATH/&#39;test_clean&#39;) .",
            "url": "https://mrg-ai.github.io/blog/2020/09/20/EDA-Example-using-rossman_data.html",
            "relUrl": "/2020/09/20/EDA-Example-using-rossman_data.html",
            "date": " • Sep 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Deep Learning Notes",
            "content": "Deep Learning Notes . . My Notes . General stuff . Jupyter notebook – Installed through Anaconda. But there are various other ways. . | Google Collab - https://colab.research.google.com/notebooks/welcome.ipynb - recent=true . | Use GPU while running the code. . | GPUs are good at running similar code (in this case mathematical models) multiple times and hence are necessary. CPUs can’t handle or are slow. . | Pytorch, TensorFlow, FastAI, numpy, pandas, matplotlib etc are all Python libraries. Some are deep learning specific and some are math specific. . | Data is stored on google compute VM (Collab’s backend) by default. However we can store on Google Drive and access it from within python code. . | Custom Data can be used and external links (google drive, dropbox etc) will be needed for any proper usage of programs (see lesson 2 notebook in google drive of &lt;emailmanjunathrg@gmail.com)&gt; . | Visual Studio Code can be used to browse through fastai or pytorch classes and understand the library code. . | Render can be used to deploy web apps; Google Compute Engine is another option. . | Further reading - Different types of Models (Resnet, Inception, VGGNet, AlexNet etc) . | . Below lines of code are needed when running notebooks using FastAI. Few are for Google Drive, Library Reloads, Ignore Pytorch related warnings, plotting inline in the notebook . !curl -s https://course.fast.ai/setup/colab  |  bash | . %reload_ext autoreload . %autoreload 2 . %matplotlib inline . import warnings . warnings.filterwarnings(“ignore”, category=UserWarning, module=”torch.nn.functional”) . from google.colab import drive . drive.mount(‘/content/gdrive’, force_remount=True) . root_dir = ”/content/gdrive/My Drive/” . base_dir = root_dir + ’fastai-v3/’ . Below is a javascript to download URLs from google search images into a csv file. I think it should work with any google search result. . Press CtrlShiftJ in Windows/Linux and CmdOptJ in Mac and paste below and enter. A csv file should get downloaded. . urls = Array.from(document.querySelectorAll(‘.rg_di .rg_meta’)).map(el=&gt;JSON.parse(el.textContent).ou); window.open(‘data:text/csv;charset=utf-8,’ + escape(urls.join(‘ n’))); . General Process for Training a Model . Below is a general flow for train a model. This is very general and at a high level. . There can be lots of variations and other steps in between and after. . Get the data with data (like images) and labels. Example command below . Labels can be present in various ways and below command/process will need to be changed accordingly. . data = ImageDataBunch.from_folder(path, train=”.”, valid_pct=0.2, .         ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats) . Train the model using a CNN (convolutional neural network) . learn = cnn_learner(data, models.resnet34, metrics=error_rate) . Fit the data to the curve correctly using appropriate number of epochs . learn.fit_one_cycle(4) . You can interpret the data after this. You can plot confusion matrix or most confused output also. . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_confusion_matrix() . Once you think the model has learnt, you can export the model so that it can be used in an application. This will create a file named ‘export.pkl’ in the directory where we were working that contains everything we need to deploy our model (the model, the weights but also some metadata like the classes or the transforms/normalization used). . learn.export() . This export file can be used to deploy on Render.com or Google Compute etc. . Example - . https://github.com/manjunathrgithub/BajjiVadeApp . ** DataBlock API** . In the previous section, we used ImageDataBunch.from_folder method to get the “data” which was then passed to a Learner. This was a factory method from FastAI library. It does quite a few things in the backend and also makes some decisions. . We cannot use Factory methods all the time. We will go through the steps that happen in these type of Factory methods and understand the flow. Then we can use those classes and we can have more control. This will also help in understanding what happens to the data before we send it to a learner. Some of these are Pytorch Classes i.e. Dataset, Dataloader and Databunch is a FastAI class . Dataset – This is the first step in getting the data. An object (like image) and its label(s) form a dataset. . Dataloader – A dataset is not enough to train a model. Batches of datasets need to be sent to the model. For creating these batches, we use a Dataloader. . Databunch – It still isn’t enough to train a model, because we’ve got no way to validate the model. If all we have is a training set, then we have no way to know how we’re doing. We need a separate set of held out data, a validation set, to see how we’re getting along. We might also use a test set. . To get the data in a format that we can send to Learner - We use a fastai class called a DataBunch. A DataBunch is something which binds together a training data loader and a validation data loader. . This object can then be sent to a Learner and we can start fitting the data using a proper learning rate, number of epochs, appropriate model etc. . Below is an example for an Image Dataset. . . In this example, ImageFileList.fromfolder creates Datasets using the files which are in a folder with name as “train-jpg” and the files with a suffix of .jpg. The information about the labels are obtained from a csv file and hence it uses .label_from_csv to which we pass the csv file name. . The data is split randomly (80:20 ratio) for training and validation datasets since we do not have them separately in this example. If we do, we should not use this class. We should use .split_by_folder if they are available in different folders. . Then we convert them into Pytorch Datasets using the .datasets . They are transformed using certain transformation rules. . Finally they are converted into a dataloader and eventually a databunch using the .databunch . Below is another example. This is an image example where images of 3 and 7 are stored in folders called 3 and 7. . path = untar_data(URLs.MNIST_TINY) . tfms = get_transforms(do_flip=False) . path.ls() . [PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/valid’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/models’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/train’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/test’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/labels.csv’)] . (path/’train’).ls() . [PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/train/3’), . PosixPath(‘/home/jhoward/.fastai/data/mnist_tiny/train/7’)] . data = (ImageFileList.from_folder(path) #Where to find the data? -&gt; in path and its subfolders . .label_from_folder() #How to label? -&gt; depending on the folder of the filenames . .split_by_folder() #How to split in train/valid? -&gt; use the folders . .add_test_folder() #Optionally add a test set . .datasets() #How to convert to datasets? . .transform(tfms, size=224) #Data augmentation? -&gt; use tfms with a size of 224 . .databunch()) #Finally? -&gt; use the defaults for conversion to ImageDataBunch . What kind of data set is this going to be? . It’s going to come from a list of image files which are in some folder. . They’re labeled according to the folder name that they’re in. . We’re going to split it into train and validation according to the folder that they’re in (train and valid). . You can optionally add a test set. We’re going to be talking more about test sets later in the course. . We’ll convert those into PyTorch datasets. . We will then transform them using this set of transforms (tfms), and we’re going to transform into size 224. . Then we’re going to convert them into a data bunch. . In each of these stages inside the parentheses, there are various parameters that you can pass to and customize how that all works. But in the case of something like this MNIST dataset, all the defaults pretty much work and hence no customizations are done. . Multi Label Dataset example . Movie Genres, Satellite Image Descriptions are some examples of Multi label datasets. . Each image can have multiple labels and the labels can repeat across images. . The same approach followed for single label example will work here also. Only thing that will change is the parameters or arguments passed to various classes that we call. For more details - . https://colab.research.google.com/github/fastai/course-v3/blob/master/nbs/dl1/lesson3-planet.ipynb - scrollTo=_bRElObaCKsr . Segmentation . In very simple words, normal image classification for every single pixel in every single image is what is called segmentation. Self Driving cars software use this a lot to differentiate between different things that the car sensors see. . It is also used in medical science for scans, images etc. . For segmentation, we don’t use a convolutional neural network. We can, but actually an architecture called U-Net turns out to be better. Learner.create_unet will be used rather than a cnn. . Look at camvid example in fastai or google drive - https://colab.research.google.com/drive/1O6zJfhQVjnAFMZhi5KG4yl9xrvFHpR5w . Some Learning Rate related notes - . Learning Rate Annealing - idea of decreasing the learning rate during training is an older one. . Recent idea is to keep increasing the learning rate initially and then decreasing so that the model learns quickly or reaches the correct point on the graph quickly. . Classification Problem for Text - NLP (natural language processing) . Text classification is different from images. Images are generally RGB layers and each pixel can be represented by a number and it is easier than it is for Texts. . Texts have to be converted to Numbers before we do any deep learning on it. . The two processes are Tokenization and Numericalization. . Idea for a NLP program - Pass a Hashtag to the program or app. . The App should give a bar graph showing how many of the tweets using the hashtag are Positive and how many are negative. . NLP uses RNN instead of CNN. RNN is Recurrent Neural Network. . Use a Wikipedia Dataset pretrained model to transfer learn for your group/corpus of data -Example – IMDB Movie Review Dataset (which has positive/negative labels). This model predicts the next word in a sentence because we train for language model labels and not on positive/negative labels. However, that is not much useful for a classification problem. We want to classify as Positive or Negative Sentiment. . Language Model Learner has a part called Encoder which stores information about what the model has understood about the statement it was input. Also, there is something called vocab which contains the tokes and numbers for each word. . These will be used further to predict the sentiment in the text classifier model. . We will use this IMBD trained Language Model Learner data’s vocab along with the original IMDB data (positive and negative labels) to create a classification model which can predict the labels for input data. . Tabular Data . Dataframes (panda library) are used to read tabular data from csv, RDBMS, Hadoop etc. . Generally, for Tabular data, Random Forests, Gradient Descent and other algorithms are used, but NNs can also be used. . Note – Go through the Machine Learning course for other non-NN algorithms. . http://course18.fast.ai/lessonsml1/lesson3.html . First 3 chapters give a good understanding of Random Forests. Notes about that below. Apart from RF (even that can be replaced with NNs), other algorithms can all be replaced with NNs. . Some reading - https://www.dataquest.io/blog/top-10-machine-learning-algorithms-for-beginners/ . https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab . Collab Filtering . This is for Recommendation systems like Amazons or Netflix where they suggest that you might like this because you bought this/saw this. . Theory Behind NNs . Dataloaders -&gt; [ Linear Function (Matrix Multiplication) -&gt; Activation Function] -&gt; Next layer … . Back propagation is nothing but . parameters minus= learning rate * parameters.gradient_descent(loss function) i.e. . parameters = parameters minus learning rate * parameters.gradient_descent(loss function) . . Revision of GD (Gradient Descent) (Lesson 2 from the middle of the video) . Yi= a1Xi1+a2Xi2+constant and let’s assume Xi2=1 for simplicity. . If the above equation is executed for different “i” it will become a matrix multiplication. . . Using pytorch above equation can be written as below. (X first column is Random numbers) . . Tensor is a regular sized (non-ragged) array. It can be rectangular (2D) or a Cube (3D) or more dimensions. But it is always a regular size. . An example – Image is a Rank 3 tensor since it has 3 layers (RGB). . If we plot the y function from screenshot above . . If we want to fit a line through these points without knowing the coefficients 3 and 2 i.e. Tensor A, we start with some random values and we can plot the line. . We can move around the line by using the derivative of the Loss (in this case MSE) and see how Y changes. . MSE – (y_hat(prediction)-y(actual))**2.mean() . Loss is nothing but the distance of the line from the dots. If we reduce the loss, the line will match the dots and go through them thereby keeping the loss at a minimum. . . The gradient/derivative is used along with learning rate to change the value of the co-efficients and bring the line closer to where we want. . . Weight Decay – All learners have a weight decay parameters in fastai and a value of 0.1 should help in general. . W- Weights/Parameters . L(x,w) – Loss function . M(x,w) – y_hat . The box is same as . parameters = parameters minus learning rate * parameters.gradient_descent(loss function) . . Momentum – The update to the parameters/weights is not just based on the derivative, instead 1/10 of it is the derivative and 90% of it is just the same direction that we went last time. This is exponentially weighted moving average of the step. . Generally, a momentum of 0.9 is used with SGD if we want to use momentum. . RMSProp – This is similar to Momentum but it is exponentially weighted moving average of the gradient squared. . Adam is Momentum + RMSProp – https://github.com/hiromis/notes/blob/master/Lesson5.md . Cross Entropy Loss – Loss used for Single Label Multi class classification problem . SoftMax Activation function – Activation function used in Single Label Multi class classification problems. . RESNET – Residual Net. The input from previous layer is added to this layer’s output. In other words the inputs skips this convolution (skip connection). Added is a + here. This is the basic theory of Resnet. . DenseNet – Same as resnet but instead of a +, a concat is used to concatenate previous layer’s input and its output and that is passed to this layer. It is memory intensive but works well for smaller datasets. . UNet – As the name says, it is in a U shape. It can be thought of Resnet from halfpoint onwards. . Example - First layer output is skipping all layers and directly going as input to last layer. . . Nearest Neighbor interpolation . Bi Linear interpolation – In layman terms - Techniques to increase the resolution sizes of image inputs. . CNN – . The matrix multiplications in a NN are replaced with a special Matrix multiplication in case of CNNs and they are called Convolutions. . Conv Kernel multiplied by Input matrix to get one single number. Kernel Matrix can be considered to be a matrix of weights which are tied at certain places and also is sparse. . By doing this, we achieve identifying different parts of an image and later it can all be tied together to identify the whole image. . . For example – a 3x3 matrix multiplied to another 3x3 gives a 3x3 in normal matrix multiplication. However, convolution will only give a single value. . Therefore, it is generally padded with zeroes to as shown below. . Note : This is 2D example, However images are generally 3D and the same idea extends to that as well. You will have more conv kernels and so on. For images, it could be 3 conv kernels to start with along with some kind of padding to increase the size of the Tensor. However practically, there are more kernels used even during start. . . However, we don’t use Matrix multiplication since its slow and the matrix is anyways sparse with many zeroes. . There are functions which do this called Convolution functions. . Stride 2 Convolution – Same as above, but skip one layer of cells or columns in matrix when moving to next section. After the left top corner is done, you skip to top right corner in above example instead of the middle 3 columns. This reduces the height and width of output matrix but we generally increase the kernels and the depth (channels) actually increases. . Average Pooling – After multiple layers, we will have more number of channels and some small height and width. What we need is Category of the image in a Image classifier. There could be say 5 categories. To get to this point, we do various things and one of the first things is Average pooling. It basically takes average of each channel. If the final layer gave an output of say 20 layers, we take average of each layer and get one value for each layer. These are stacked in a column vector of size 20. This is still more than the categories we expect. . This is passed through a linear layer to get the 5 categories that we want. One of the categories in this vector should have a high probability value based on which the prediction can be obtained. . ResNet – See above. . DenseNet – See above. . UNET – See above. . UNET gives an output image of same size as input. The down sampling reduces the image size for sure and we know that. For increasing the size by final layer, it does a Stride ½ convolution. . Apart from padding on the perimeter, it also adds padding in between as shown below. . The 2x2 blue colored pixels are original image. Remaining are all padding. . . This is slightly older where all the padded pixels are basically zeros or white pixels. . Below is what is done now to do up-sampling. This is called Nearest Neighbor interpolation. . Bilinear Interpolation is similar but takes a weighted average of the surrounding neighbors. . Techniques like above are used in up sampling path of the UNET. However the down sampling would have reduced the size and up sampling only from that will be not useful. . A skip connection or identity connection is added in up direction where the inputs of the down sampling path are added as inputs. See UNET diagram above. . . GAN . . RNN . Inputs – Get the inputs as Tensors . | Weights/Parameters/Coefficients – Multiply with Random weights or Pretrained weights. This is Linear computation i.e. Linear layer or Affine Layer. . | Activations – The output of Affine function are also Activations. But they are Linear activations. Pass the output of previous step through a non linear activation function like ReLU or Softmax. This is the non-linearity in the NeuralNet. . Activation Function – ReLU, Softmax, Sigmoid and many others. | . | Output – The output is obtained. . | Layers – Each of these is a layer . | Loss – Compare the output with actual output and calculate the Loss. MSE, RMSE, Log(MSE) etc are some of the Loss functions. . | . L(x,w) = MSE(m(x,w),y) +WD* Sum(W**2) . Since we calculate the gradient of a Loss function, that calculates the gradient of the WD*(Sum(W**2)) . Adding the +WD* Sum(W**2) to Loss function is called L2 regularization. . The gradient of WD*(Sum(W**2)) which is used in Back propagation (params = params-LR*gradient (Loss function) i.e. 2*WD*W (generalized to WD*W) is called Weight Decay. . Back propagation is nothing but parameters = parameters minus learning rate * parameters.gradient_descent(loss function) . | One Hot Encoded Matrix – This is done to preprocess the input data that is fed to a neural net. This is helpful to pass the data into NN in a common format of Matrices. One matrix of One Hot Encoded Matrix and the other input Random Weights Matrix. . | Embedding – Array Lookup or Matrix Multiply by One hot encoded matrix. . | Latent features or Latent Factors – Insights that NNs can give. Embeddings also give latent features in Collab learning. . | N_factors in collab problems - Size of the embedding matrix. . | Weight Decay - Weight Decay is a type of Regularization. Weight decay is a number used to multiply the parameters during Loss calculation. We should use many parameters and along with that use weight decay. General value should be 0.1 . | . Parameters at time/epoch T = parameters at epoch T-1 – LR*derivative (loss with respect to parameters at T-1) . Adam – Momentum+RMS Prop . Momentum - parameters = parameters minus learning rate * [ x% (parameters.gradient_descent(loss function)) + (100-x)% (previous derivative)] . Or it can also be written as Step T = alpha*gradient + (1-alpha)*Step T-1 . This second part of 1-alpha is called Exponentially weighted moving average of past gradients. Step T-1 inturn uses Step T-2. So (1-alpha) gets multiplied multiple times. | . | RMS Prop - Exponentially weighted moving average of past gradient square and use that to divide as shown below . | . | . parameters = parameters minus learning rate *{ [ x% (parameters.gradient_descent(loss function)) + (100-x)% (previous derivative)]}/{ Exponentially weighted moving average of past gradient square} . Metric . | User Bias and Item Bias – Terms in Collab which highlight the biases that the user (like customer id, user_id) can have and the biases of the Item (like movie (popular movie) or product (star product like Apple devices)) . | Cross-Entropy – Loss function where we want a single value to be selected as output rather than probability of being close to the output. That is why it’s used in Single Label Multi class classification problems. . Loss cannot be MSE for learners which predict individual classes. MSE is more suited for finding a number on a graph kinda problems. . | Cross Entropy is a Loss function which provides low loss when the prediction is correct and its confident and high loss for wrong predictions with high confidence. . | . | Softmax – For Cross Entropy Loss function to work correctly and give positive probability and to be sure that sum of probabilities for possible values to be less than 1, the activation function to be used along with this Loss function is Softmax function. . | Dropout – Dropout is a type of regularization. Some activations are thrown out in each mini batch. We keep lot of parameters but throw away some activation randomly in each mini batch so that it does not overfit. . | BatchNorm – Generally for continuous variables. Used for most cases. This is basically to scale up the output. For example if the range of values we expect is 1 to 5, but the NN gives -1 to 1, using batch norm layer would help to normalize it.. . Yhat = f(w0, w1, w2…. Wn, X)*g + b . | Loss = Sum (y-yhat)**2 . | . | WeightNorm is another new normalization used in Fastai. Recent thing.. . | Data Augmentation - Modify model inputs during training in order to effectively increase data size. For examples - Images can be flipped or warped or perspective changes etc to basically convert one image into multiple images which look different from one another. . | Label_Cls option in Tabular - Used to pass options like Output variable to be considered a Float, Take Log of it while creating labels for it etc.. . | Tabular Architecture - Mostly Embeddings of many layers. . | Data Augmentation . | Fine Tuning . | Layer Deletion and Random weights . | Freezing and Unfreezing . | .",
            "url": "https://mrg-ai.github.io/blog/2020/09/20/Deep-Learning-Notes.html",
            "relUrl": "/2020/09/20/Deep-Learning-Notes.html",
            "date": " • Sep 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "https://twitter.com/ParamsAndActivs . https://twitter.com/ParamsAndActivs .",
          "url": "https://mrg-ai.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mrg-ai.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}