<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Linear Regression using Pytorch from Scratch | ParamsAndActivs</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Linear Regression using Pytorch from Scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An ML/DL/AI enthusiast’s thoughts and understandings." />
<meta property="og:description" content="An ML/DL/AI enthusiast’s thoughts and understandings." />
<link rel="canonical" href="https://mrg-ai.github.io/blog/2019/12/31/Linear-Regression-Using-Pytorch.html" />
<meta property="og:url" content="https://mrg-ai.github.io/blog/2019/12/31/Linear-Regression-Using-Pytorch.html" />
<meta property="og:site_name" content="ParamsAndActivs" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-31T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://mrg-ai.github.io/blog/2019/12/31/Linear-Regression-Using-Pytorch.html","@type":"BlogPosting","headline":"Linear Regression using Pytorch from Scratch","dateModified":"2019-12-31T00:00:00-06:00","datePublished":"2019-12-31T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mrg-ai.github.io/blog/2019/12/31/Linear-Regression-Using-Pytorch.html"},"description":"An ML/DL/AI enthusiast’s thoughts and understandings.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mrg-ai.github.io/blog/feed.xml" title="ParamsAndActivs" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">ParamsAndActivs</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Linear Regression using Pytorch from Scratch</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-12-31T00:00:00-06:00" itemprop="datePublished">
        Dec 31, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      36 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/mrg-ai/blog/tree/master/_notebooks/2019-12-31-Linear-Regression-Using-Pytorch.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/mrg-ai/blog/master?filepath=_notebooks%2F2019-12-31-Linear-Regression-Using-Pytorch.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/mrg-ai/blog/blob/master/_notebooks/2019-12-31-Linear-Regression-Using-Pytorch.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-12-31-Linear-Regression-Using-Pytorch.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will start with Numpy and then look at Pytorch</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Below equation is y = a + bx + noise where a = 1 and b = 2</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="o">.</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Shuffles the indices</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>

<span class="c1"># Uses first 80 random indices for train</span>
<span class="n">train_idx</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:</span><span class="mi">80</span><span class="p">]</span>
<span class="c1"># Uses the remaining indices for validation</span>
<span class="n">val_idx</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="mi">80</span><span class="p">:]</span>

<span class="c1"># Generates train and validation sets</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">val_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">val_idx</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(array([[0.77127035],
        [0.06355835],
        [0.86310343],
        [0.02541913],
        [0.73199394]]), array([[2.47453822],
        [1.19277206],
        [2.9127843 ],
        [1.07850733],
        [2.47316396]]))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&lt;class &#39;numpy.ndarray&#39;&gt;
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above code cells are to create the data as Numpy arrays. This is the same data that we created in LR model with Numpy. 
<a href="https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy">https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy</a></p>
<p>We will convert the numpy arrays into Tensors in below cells so that we can use them with Pytorch.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install torchviz
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting torchviz
  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)
     |████████████████████████████████| 51kB 2.9MB/s 
Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.3.1)
Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)
Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-&gt;torchviz) (1.17.4)
Building wheels for collected packages: torchviz
  Building wheel for torchviz (setup.py) ... done
  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3520 sha256=28d2000b9a7b828cc19400716528eb1a55cf9aadeea8860bf65c75189b590f79
  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667
Successfully built torchviz
Installing collected packages: torchviz
Successfully installed torchviz-0.0.1
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torchviz</span> <span class="kn">import</span> <span class="n">make_dot</span>

<span class="c1">#Setting the Device to CPU or GPU based on the availability</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1">#Since we are using GPU here, devide shows as cuda. Otherwise it would show as cpu.</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>cuda
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># For using with Pytorch, we need to have PyTorch&#39;s Tensors</span>
<span class="c1"># </span>
<span class="n">x_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">y_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x_train</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">),</span> <span class="n">x_train_tensor</span><span class="o">.</span><span class="n">type</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>&lt;class &#39;numpy.ndarray&#39;&gt; &lt;class &#39;torch.Tensor&#39;&gt; torch.cuda.FloatTensor
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see x_train_tensor is now a Tensor and .type() shows that it is a Tensor which is on GPU and is of Float datatype</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This is similar to what we did in Numpy.</span>
<span class="c1"># We set REQUIRES_GRAD = TRUE because we want to calculate partial derivatives or gradients on these parameters</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([-0.0125], requires_grad=True) tensor([-0.2016], requires_grad=True)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This looks good. These are Tensors and can be used with Pytorch. Is it ??</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;torch.FloatTensor&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a Float Tensor but resides on CPU. To use GPU, we need to push it to GPU.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;torch.cuda.FloatTensor&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;torch.cuda.FloatTensor&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Okay. Now we have pushed them to GPU.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([-0.0125], device=&#39;cuda:0&#39;, grad_fn=&lt;CopyBackwards&gt;),
 tensor([-0.2016], device=&#39;cuda:0&#39;, grad_fn=&lt;CopyBackwards&gt;))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What happened to requires_grad=True !? Looks like we lost it when we moved to GPU. 
So, this is not the right approach to create Tensors on GPU which require gradient descent</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">b</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([-0.4192], device=&#39;cuda:0&#39;, requires_grad=True),
 tensor([-0.1465], device=&#39;cuda:0&#39;, requires_grad=True))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="However-the-best-way-is-to-create-the-Tensors-right-away-on-GPU-instead-of-all-this-moving-around.">However the best way is to create the Tensors right away on GPU instead of all this moving around.<a class="anchor-link" href="#However-the-best-way-is-to-create-the-Tensors-right-away-on-GPU-instead-of-all-this-moving-around."> </a></h1><h1 id="The-output-of-this-and-above-cells-is-essentially-the-same,-but-this-is-a-best-practice-and-straightforward.">The output of this and above cells is essentially the same, but this is a best practice and straightforward.<a class="anchor-link" href="#The-output-of-this-and-above-cells-is-essentially-the-same,-but-this-is-a-best-practice-and-straightforward."> </a></h1>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True),
 tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Okay.. Now we have a and b Tensors on GPU.. 
What next ? Start Forward Pass -&gt; calculate Loss -&gt; calculate gradients -&gt; do backward pass and repeat it for number of epochs ??</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># set a learning rate and number of epochs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this next section we will run a for loop and calculate the predictions, error and MSE loss similar to what we did before in 
LR with Numpy exercise.</p>
<p>However we will not calculate the gradient by hand. Rather we will use Pytorch's autograd function and its capabilities. 
Autograd is PyTorch’s automatic differentiation package and it will take care of calculating Partial Derivatives,
Chain rule of derivatives where we multiply different partial derivatives to get a final derivative and so on.</p>
<p>It will also keep accumulating the gradient for each change of that object. We have set a and b to requires_grad=True.
Therefore gradients will be calculated for them by Pytorch.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x_train_tensor</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y_train_tensor</span> <span class="o">-</span> <span class="n">yhat</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="c1"># Lets print a and b before the backward pass.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch number =&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b before backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># We will not do this now like we did in Numpy notebook </span>
    <span class="c1"># - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb</span>
    <span class="c1"># a_grad = -2 * error.mean()</span>
    <span class="c1"># b_grad = -2 * (x_tensor * error).mean()</span>
    
    <span class="c1"># We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. </span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Let&#39;s check the computed gradients...</span>
    <span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="c1"># Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function)</span>
  
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b after backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch number = 0
a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-3.3881], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.9439], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;) tensor([0.3335], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;)
epoch number = 1
a and b before backward pass tensor([0.5328], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;) tensor([0.3335], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;)
a&#39;s gradient =  None
b&#39;s gradient =  None
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-42-ee387fc7ae7e&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span>     <span class="ansi-red-fg"># Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function)</span>
<span class="ansi-green-intense-fg ansi-bold">     23</span> 
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">     </span>a <span class="ansi-blue-fg">=</span> a <span class="ansi-blue-fg">-</span> lr <span class="ansi-blue-fg">*</span> a<span class="ansi-blue-fg">.</span>grad
<span class="ansi-green-intense-fg ansi-bold">     25</span>     b <span class="ansi-blue-fg">=</span> b <span class="ansi-blue-fg">-</span> lr <span class="ansi-blue-fg">*</span> b<span class="ansi-blue-fg">.</span>grad
<span class="ansi-green-intense-fg ansi-bold">     26</span>     print<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;a and b after backward pass&#34;</span> <span class="ansi-blue-fg">,</span> a<span class="ansi-blue-fg">,</span>b<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">TypeError</span>: unsupported operand type(s) for *: &#39;float&#39; and &#39;NoneType&#39;</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Well.. It did not work. a.grad and b.grad became None after epoch 0 and the parameter update statement fails for epoch 1. 
What happened ?</p>
<p>epoch number = 0
a and b before backward pass tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)</p>
<p>See what happened after backward pass during epoch 0. The requires_grad=True vanished again after a and b were reassigned new values. 
This is similar to what happened before when we moved a and b from CPU to GPU. 
See here. Link to the cell - <a href="https://colab.research.google.com/drive/1OBbzaMpf33M8Fc-Z1yY0jqFKLScvYgco#scrollTo=mp9lvYynHw3U">https://colab.research.google.com/drive/1OBbzaMpf33M8Fc-Z1yY0jqFKLScvYgco#scrollTo=mp9lvYynHw3U</a></p>
<p>a and b after backward pass tensor([0.5328], device='cuda:0', grad_fn=<SubBackward0>) tensor([0.3335], device='cuda:0', grad_fn=<SubBackward0>)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([0.5328], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;) tensor([0.3335], device=&#39;cuda:0&#39;, grad_fn=&lt;SubBackward0&gt;)
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True),
 tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x_train_tensor</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y_train_tensor</span> <span class="o">-</span> <span class="n">yhat</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="c1"># Lets print a and b before the backward pass.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch number =&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b before backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># We will not do this now like we did in Numpy notebook </span>
    <span class="c1"># - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb</span>
    <span class="c1"># a_grad = -2 * error.mean()</span>
    <span class="c1"># b_grad = -2 * (x_tensor * error).mean()</span>
    
    <span class="c1"># We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. </span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Let&#39;s check the computed gradients...</span>
    <span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="c1"># Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function)</span>
  
    <span class="n">a</span><span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>
    <span class="n">b</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b after backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch number = 0
a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-3.3881], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.9439], device=&#39;cuda:0&#39;)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">RuntimeError</span>                              Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-51-83082fa8169f&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-intense-fg ansi-bold">     22</span>     <span class="ansi-red-fg"># Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function)</span>
<span class="ansi-green-intense-fg ansi-bold">     23</span> 
<span class="ansi-green-fg">---&gt; 24</span><span class="ansi-red-fg">     </span>a<span class="ansi-blue-fg">-=</span> lr <span class="ansi-blue-fg">*</span> a<span class="ansi-blue-fg">.</span>grad
<span class="ansi-green-intense-fg ansi-bold">     25</span>     b<span class="ansi-blue-fg">.</span>sub_<span class="ansi-blue-fg">(</span>lr <span class="ansi-blue-fg">*</span> b<span class="ansi-blue-fg">.</span>grad<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>     print<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;a and b after backward pass&#34;</span> <span class="ansi-blue-fg">,</span> a<span class="ansi-blue-fg">,</span>b<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">RuntimeError</span>: a leaf Variable that requires grad has been used in an in-place operation.</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We used the inplace value substitution instead of assigning to a and b. There are two ways to do it and both are shown above i.e. a-= or b.sub_</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It does not even finish epoch 0. It fails when assigning the value. This is because of Pytorch's Dynamic Computational Graph
Basically since required_grad is set to True for a and b, pytorch is calculating gradients and storing them for each change to a and b.</p>
<p>Since we are trying to change the same object using its own value, it throws an error. 
We need to basically somehow tell Pytorch not to calculate gradient for a and b when we are doing this parameter update step.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True),
 tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Lets-try-something-else-now.-We-will-tell-Pytorch-not-to-compute-gradients-when-we-are-doing-the-parameter-update.">Lets try something else now. We will tell Pytorch not to compute gradients when we are doing the parameter update.<a class="anchor-link" href="#Lets-try-something-else-now.-We-will-tell-Pytorch-not-to-compute-gradients-when-we-are-doing-the-parameter-update."> </a></h1>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x_train_tensor</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y_train_tensor</span> <span class="o">-</span> <span class="n">yhat</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="c1"># Lets print a and b before the backward pass for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch number =&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b before backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># We will not do this now like we did in Numpy notebook </span>
    <span class="c1"># - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb</span>
    <span class="c1"># a_grad = -2 * error.mean()</span>
    <span class="c1"># b_grad = -2 * (x_tensor * error).mean()</span>
    
    <span class="c1"># We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. </span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Let&#39;s check the computed gradients...</span>
    <span class="c1"># Lets print the gradients for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> 
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="c1"># Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function)</span>
    <span class="c1"># We are telling Pytorch not to calculate gradients for steps within the &quot;with&quot;</span>
    <span class="c1"># torch.no_grad() allows us to perform regular Python operations on tensors, independent of PyTorch’s Dynamic computation graph.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">a</span><span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>
      <span class="n">b</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="c1"># Lets print a and b after the backward pass for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b after backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch number = 0
a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-3.3881], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.9439], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 100
a and b before backward pass tensor([0.2229], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1810], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-4.1505], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.8718], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.6379], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3682], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 200
a and b before backward pass tensor([0.2240], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3068], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-4.8702], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.7955], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.7110], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.4864], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 300
a and b before backward pass tensor([0.2004], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.5092], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-5.5295], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.7378], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.7533], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.6830], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 400
a and b before backward pass tensor([0.1578], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.7758], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-6.1127], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.7185], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.7690], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.9476], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 500
a and b before backward pass tensor([0.1038], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.0900], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-6.6066], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.7537], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.7645], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.2654], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 600
a and b before backward pass tensor([0.0476], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.4321], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-7.0014], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.8537], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.7478], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.6175], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 700
a and b before backward pass tensor([-0.0010], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7809], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-7.2911], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-2.0226], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.7281], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9832], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 800
a and b before backward pass tensor([-0.0321], device=&#39;cuda:0&#39;, requires_grad=True) tensor([2.1147], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-7.4737], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-2.2580], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.7153], device=&#39;cuda:0&#39;, requires_grad=True) tensor([2.3405], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 900
a and b before backward pass tensor([-0.0366], device=&#39;cuda:0&#39;, requires_grad=True) tensor([2.4134], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-7.5511], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-2.5508], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.7185], device=&#39;cuda:0&#39;, requires_grad=True) tensor([2.6685], device=&#39;cuda:0&#39;, requires_grad=True)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This ran successfully. But the values of a and b are wrong at 900th epoch and is getting worse. 
Our results from numpy exercise were very close to actual values after 1000 epochs. 
This is another issue due to Pytorch's computational graph. It keeps accumulating the gradient values.
We need to tell it to let it go after an epoch.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True),
 tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Lets-try-something-else-now.-We-will-reset-the-grads-to-zero-after-each-epoch">Lets try something else now. We will reset the grads to zero after each epoch<a class="anchor-link" href="#Lets-try-something-else-now.-We-will-reset-the-grads-to-zero-after-each-epoch"> </a></h1>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x_train_tensor</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y_train_tensor</span> <span class="o">-</span> <span class="n">yhat</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="c1"># Lets print a and b before the backward pass for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch number =&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b before backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># We will not do this now like we did in Numpy notebook </span>
    <span class="c1"># - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb</span>
    <span class="c1"># a_grad = -2 * error.mean()</span>
    <span class="c1"># b_grad = -2 * (x_tensor * error).mean()</span>
    
    <span class="c1"># We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. </span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Let&#39;s check the computed gradients...</span>
    <span class="c1"># Lets print the gradients for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> 
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="c1"># Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function)</span>
    <span class="c1"># We are telling Pytorch not to calculate gradients for steps within the &quot;with&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">a</span><span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>
        <span class="n">b</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="c1"># Lets print a and b after the backward pass for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b after backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># We reset it to zero after each epoch</span>
    <span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="c1"># Lets print a and b after the backward pass and after reset to zero for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b after backward pass after resetting grad to zero&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch number = 0
a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-3.3881], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.9439], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 100
a and b before backward pass tensor([1.1479], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7257], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0188], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0367], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 200
a and b before backward pass tensor([1.0507], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9159], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0041], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0080], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 300
a and b before backward pass tensor([1.0295], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9574], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0009], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0018], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 400
a and b before backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9664], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0002], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0004], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 500
a and b before backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([4.2574e-05], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-8.3295e-05], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 600
a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([9.3249e-06], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.8163e-05], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 700
a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([1.9097e-06], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-4.1103e-06], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 800
a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([5.1083e-07], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-8.8313e-07], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 900
a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([5.4762e-07], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-5.7358e-07], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This looks good. Compare the gradient values for each epoch for this loop vs the one before. 
The gradients keep improving here and the values of a and b keep coming closer to actual values.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True),
 tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The predicted values are close to actual values.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Dynamic-Computational-Graph">Dynamic Computational Graph<a class="anchor-link" href="#Dynamic-Computational-Graph"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now a and b are not messed up. But I am resetting them to check the Dynamic Computational Graph</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True),
 tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets manually calculate the prediction, error and loss for epoch 0</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x_train_tensor</span>
<span class="n">error</span> <span class="o">=</span> <span class="n">y_train_tensor</span> <span class="o">-</span> <span class="n">yhat</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">make_dot</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
&lt;!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"&gt;
<!-- Generated by graphviz version 2.40.1 (20161225.0304)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="172pt" height="171pt" viewBox="0.00 0.00 171.50 171.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 167)">
<title>%3</title>
<polygon fill="#ffffff" stroke="transparent" points="-4,4 -4,-167 167.5,-167 167.5,4 -4,4" />
<!-- 139624134240632 -->
<g id="node1" class="node">
<title>139624134240632</title>
<polygon fill="#caff70" stroke="#000000" points="118,-21 26,-21 26,0 118,0 118,-21" />
<text text-anchor="middle" x="72" y="-7.4" font-family="Times,serif" font-size="12.00" fill="#000000">AddBackward0</text>
</g>
<!-- 139624134241080 -->
<g id="node2" class="node">
<title>139624134241080</title>
<polygon fill="#add8e6" stroke="#000000" points="54,-92 0,-92 0,-57 54,-57 54,-92" />
<text text-anchor="middle" x="27" y="-64.4" font-family="Times,serif" font-size="12.00" fill="#000000"> (1)</text>
</g>
<!-- 139624134241080&#45;&gt;139624134240632 -->
<g id="edge1" class="edge">
<title>139624134241080&#45;&gt;139624134240632</title>
<path fill="none" stroke="#000000" d="M39.535,-56.6724C45.4798,-48.2176 52.5878,-38.1085 58.6352,-29.5078" />
<polygon fill="#000000" stroke="#000000" points="61.5714,-31.4169 64.4601,-21.2234 55.8452,-27.3906 61.5714,-31.4169" />
</g>
<!-- 139624134239400 -->
<g id="node3" class="node">
<title>139624134239400</title>
<polygon fill="#d3d3d3" stroke="#000000" points="163.5,-85 72.5,-85 72.5,-64 163.5,-64 163.5,-85" />
<text text-anchor="middle" x="118" y="-71.4" font-family="Times,serif" font-size="12.00" fill="#000000">MulBackward0</text>
</g>
<!-- 139624134239400&#45;&gt;139624134240632 -->
<g id="edge2" class="edge">
<title>139624134239400&#45;&gt;139624134240632</title>
<path fill="none" stroke="#000000" d="M110.404,-63.9317C103.7191,-54.6309 93.821,-40.8597 85.7479,-29.6276" />
<polygon fill="#000000" stroke="#000000" points="88.4395,-27.3753 79.761,-21.2979 82.7553,-31.4608 88.4395,-27.3753" />
</g>
<!-- 139624134240968 -->
<g id="node4" class="node">
<title>139624134240968</title>
<polygon fill="#add8e6" stroke="#000000" points="145,-163 91,-163 91,-128 145,-128 145,-163" />
<text text-anchor="middle" x="118" y="-135.4" font-family="Times,serif" font-size="12.00" fill="#000000"> (1)</text>
</g>
<!-- 139624134240968&#45;&gt;139624134239400 -->
<g id="edge3" class="edge">
<title>139624134240968&#45;&gt;139624134239400</title>
<path fill="none" stroke="#000000" d="M118,-127.9494C118,-118.058 118,-105.6435 118,-95.2693" />
<polygon fill="#000000" stroke="#000000" points="121.5001,-95.0288 118,-85.0288 114.5001,-95.0289 121.5001,-95.0288" />
</g>
</g>
</svg>

</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">make_dot</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
&lt;!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"&gt;
<!-- Generated by graphviz version 2.40.1 (20161225.0304)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="172pt" height="228pt" viewBox="0.00 0.00 171.50 228.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 224)">
<title>%3</title>
<polygon fill="#ffffff" stroke="transparent" points="-4,4 -4,-224 167.5,-224 167.5,4 -4,4" />
<!-- 139624134240408 -->
<g id="node1" class="node">
<title>139624134240408</title>
<polygon fill="#caff70" stroke="#000000" points="117,-21 27,-21 27,0 117,0 117,-21" />
<text text-anchor="middle" x="72" y="-7.4" font-family="Times,serif" font-size="12.00" fill="#000000">SubBackward0</text>
</g>
<!-- 139624134240632 -->
<g id="node2" class="node">
<title>139624134240632</title>
<polygon fill="#d3d3d3" stroke="#000000" points="118,-78 26,-78 26,-57 118,-57 118,-78" />
<text text-anchor="middle" x="72" y="-64.4" font-family="Times,serif" font-size="12.00" fill="#000000">AddBackward0</text>
</g>
<!-- 139624134240632&#45;&gt;139624134240408 -->
<g id="edge1" class="edge">
<title>139624134240632&#45;&gt;139624134240408</title>
<path fill="none" stroke="#000000" d="M72,-56.7787C72,-49.6134 72,-39.9517 72,-31.3097" />
<polygon fill="#000000" stroke="#000000" points="75.5001,-31.1732 72,-21.1732 68.5001,-31.1732 75.5001,-31.1732" />
</g>
<!-- 139624134241080 -->
<g id="node3" class="node">
<title>139624134241080</title>
<polygon fill="#add8e6" stroke="#000000" points="54,-149 0,-149 0,-114 54,-114 54,-149" />
<text text-anchor="middle" x="27" y="-121.4" font-family="Times,serif" font-size="12.00" fill="#000000"> (1)</text>
</g>
<!-- 139624134241080&#45;&gt;139624134240632 -->
<g id="edge2" class="edge">
<title>139624134241080&#45;&gt;139624134240632</title>
<path fill="none" stroke="#000000" d="M39.535,-113.6724C45.4798,-105.2176 52.5878,-95.1085 58.6352,-86.5078" />
<polygon fill="#000000" stroke="#000000" points="61.5714,-88.4169 64.4601,-78.2234 55.8452,-84.3906 61.5714,-88.4169" />
</g>
<!-- 139624134239400 -->
<g id="node4" class="node">
<title>139624134239400</title>
<polygon fill="#d3d3d3" stroke="#000000" points="163.5,-142 72.5,-142 72.5,-121 163.5,-121 163.5,-142" />
<text text-anchor="middle" x="118" y="-128.4" font-family="Times,serif" font-size="12.00" fill="#000000">MulBackward0</text>
</g>
<!-- 139624134239400&#45;&gt;139624134240632 -->
<g id="edge3" class="edge">
<title>139624134239400&#45;&gt;139624134240632</title>
<path fill="none" stroke="#000000" d="M110.404,-120.9317C103.7191,-111.6309 93.821,-97.8597 85.7479,-86.6276" />
<polygon fill="#000000" stroke="#000000" points="88.4395,-84.3753 79.761,-78.2979 82.7553,-88.4608 88.4395,-84.3753" />
</g>
<!-- 139624134240968 -->
<g id="node5" class="node">
<title>139624134240968</title>
<polygon fill="#add8e6" stroke="#000000" points="145,-220 91,-220 91,-185 145,-185 145,-220" />
<text text-anchor="middle" x="118" y="-192.4" font-family="Times,serif" font-size="12.00" fill="#000000"> (1)</text>
</g>
<!-- 139624134240968&#45;&gt;139624134239400 -->
<g id="edge4" class="edge">
<title>139624134240968&#45;&gt;139624134239400</title>
<path fill="none" stroke="#000000" d="M118,-184.9494C118,-175.058 118,-162.6435 118,-152.2693" />
<polygon fill="#000000" stroke="#000000" points="121.5001,-152.0288 118,-142.0288 114.5001,-152.0289 121.5001,-152.0288" />
</g>
</g>
</svg>

</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">make_dot</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
&lt;!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"&gt;
<!-- Generated by graphviz version 2.40.1 (20161225.0304)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="172pt" height="342pt" viewBox="0.00 0.00 171.50 342.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 338)">
<title>%3</title>
<polygon fill="#ffffff" stroke="transparent" points="-4,4 -4,-338 167.5,-338 167.5,4 -4,4" />
<!-- 139624133776552 -->
<g id="node1" class="node">
<title>139624133776552</title>
<polygon fill="#caff70" stroke="#000000" points="121,-21 23,-21 23,0 121,0 121,-21" />
<text text-anchor="middle" x="72" y="-7.4" font-family="Times,serif" font-size="12.00" fill="#000000">MeanBackward0</text>
</g>
<!-- 139624133776160 -->
<g id="node2" class="node">
<title>139624133776160</title>
<polygon fill="#d3d3d3" stroke="#000000" points="118.5,-78 25.5,-78 25.5,-57 118.5,-57 118.5,-78" />
<text text-anchor="middle" x="72" y="-64.4" font-family="Times,serif" font-size="12.00" fill="#000000">PowBackward0</text>
</g>
<!-- 139624133776160&#45;&gt;139624133776552 -->
<g id="edge1" class="edge">
<title>139624133776160&#45;&gt;139624133776552</title>
<path fill="none" stroke="#000000" d="M72,-56.7787C72,-49.6134 72,-39.9517 72,-31.3097" />
<polygon fill="#000000" stroke="#000000" points="75.5001,-31.1732 72,-21.1732 68.5001,-31.1732 75.5001,-31.1732" />
</g>
<!-- 139624133775656 -->
<g id="node3" class="node">
<title>139624133775656</title>
<polygon fill="#d3d3d3" stroke="#000000" points="117,-135 27,-135 27,-114 117,-114 117,-135" />
<text text-anchor="middle" x="72" y="-121.4" font-family="Times,serif" font-size="12.00" fill="#000000">SubBackward0</text>
</g>
<!-- 139624133775656&#45;&gt;139624133776160 -->
<g id="edge2" class="edge">
<title>139624133775656&#45;&gt;139624133776160</title>
<path fill="none" stroke="#000000" d="M72,-113.7787C72,-106.6134 72,-96.9517 72,-88.3097" />
<polygon fill="#000000" stroke="#000000" points="75.5001,-88.1732 72,-78.1732 68.5001,-88.1732 75.5001,-88.1732" />
</g>
<!-- 139624133776440 -->
<g id="node4" class="node">
<title>139624133776440</title>
<polygon fill="#d3d3d3" stroke="#000000" points="118,-192 26,-192 26,-171 118,-171 118,-192" />
<text text-anchor="middle" x="72" y="-178.4" font-family="Times,serif" font-size="12.00" fill="#000000">AddBackward0</text>
</g>
<!-- 139624133776440&#45;&gt;139624133775656 -->
<g id="edge3" class="edge">
<title>139624133776440&#45;&gt;139624133775656</title>
<path fill="none" stroke="#000000" d="M72,-170.7787C72,-163.6134 72,-153.9517 72,-145.3097" />
<polygon fill="#000000" stroke="#000000" points="75.5001,-145.1732 72,-135.1732 68.5001,-145.1732 75.5001,-145.1732" />
</g>
<!-- 139624133777056 -->
<g id="node5" class="node">
<title>139624133777056</title>
<polygon fill="#add8e6" stroke="#000000" points="54,-263 0,-263 0,-228 54,-228 54,-263" />
<text text-anchor="middle" x="27" y="-235.4" font-family="Times,serif" font-size="12.00" fill="#000000"> (1)</text>
</g>
<!-- 139624133777056&#45;&gt;139624133776440 -->
<g id="edge4" class="edge">
<title>139624133777056&#45;&gt;139624133776440</title>
<path fill="none" stroke="#000000" d="M39.535,-227.6724C45.4798,-219.2176 52.5878,-209.1085 58.6352,-200.5078" />
<polygon fill="#000000" stroke="#000000" points="61.5714,-202.4169 64.4601,-192.2234 55.8452,-198.3906 61.5714,-202.4169" />
</g>
<!-- 139624133777112 -->
<g id="node6" class="node">
<title>139624133777112</title>
<polygon fill="#d3d3d3" stroke="#000000" points="163.5,-256 72.5,-256 72.5,-235 163.5,-235 163.5,-256" />
<text text-anchor="middle" x="118" y="-242.4" font-family="Times,serif" font-size="12.00" fill="#000000">MulBackward0</text>
</g>
<!-- 139624133777112&#45;&gt;139624133776440 -->
<g id="edge5" class="edge">
<title>139624133777112&#45;&gt;139624133776440</title>
<path fill="none" stroke="#000000" d="M110.404,-234.9317C103.7191,-225.6309 93.821,-211.8597 85.7479,-200.6276" />
<polygon fill="#000000" stroke="#000000" points="88.4395,-198.3753 79.761,-192.2979 82.7553,-202.4608 88.4395,-198.3753" />
</g>
<!-- 139624133777280 -->
<g id="node7" class="node">
<title>139624133777280</title>
<polygon fill="#add8e6" stroke="#000000" points="145,-334 91,-334 91,-299 145,-299 145,-334" />
<text text-anchor="middle" x="118" y="-306.4" font-family="Times,serif" font-size="12.00" fill="#000000"> (1)</text>
</g>
<!-- 139624133777280&#45;&gt;139624133777112 -->
<g id="edge6" class="edge">
<title>139624133777280&#45;&gt;139624133777112</title>
<path fill="none" stroke="#000000" d="M118,-298.9494C118,-289.058 118,-276.6435 118,-266.2693" />
<polygon fill="#000000" stroke="#000000" points="121.5001,-266.0288 118,-256.0288 114.5001,-266.0289 121.5001,-266.0288" />
</g>
</g>
</svg>

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Blue box</strong> - This is for the tensors that are used as parameters i.e. a and b and we have set requires_grad = True for them.</p>
<p><strong>Grey box</strong> - This indicates some operation that involves a gradient-computing (like a or b) tensor and/or its dependencies.</p>
<p><strong>Green box</strong> - This indicates the starting point of the gradient calculation for the backward pass. This and grey boxes are the places where gradients are calculated.</p>
<p>Note that there are no special mentions for x in the forward pass of the graph because it is <strong>not</strong> a tensor for which we compute gradients.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Lets-torch-it-more..-We-used-pytorch-to-calculate-the-gradients-before-using-the-.backward-function.">Lets torch it more.. We used pytorch to calculate the gradients before using the .backward function.<a class="anchor-link" href="#Lets-torch-it-more..-We-used-pytorch-to-calculate-the-gradients-before-using-the-.backward-function."> </a></h1><h1 id="Lets-use-pytorch-now-to-do-the-parameters-=-parameters---learning-rate-*-gradients-which-is-updating-the-weights-or-parameters.">Lets use pytorch now to do the parameters = parameters - learning rate * gradients which is updating the weights or parameters.<a class="anchor-link" href="#Lets-use-pytorch-now-to-do-the-parameters-=-parameters---learning-rate-*-gradients-which-is-updating-the-weights-or-parameters."> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Generally we do not update the weights/parameters manually since there will be a large number of them. We have to use the optim class of Pytorch.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In below code, we are basically using the optim code to update parameters through SGD (in this case since entire batch of data is used it works as Batch Gradient Descent). In real life, this would actually work as a Mini Batch Gradient Descent since mini batches of data are sent at a time in each epoch.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Define a SGD optimizer to update the parameters</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x_train_tensor</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y_train_tensor</span> <span class="o">-</span> <span class="n">yhat</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">error</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="c1"># Lets print a and b before the backward pass for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch number =&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b before backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># We will not do this now like we did in Numpy notebook </span>
    <span class="c1"># - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb</span>
    <span class="c1"># a_grad = -2 * error.mean()</span>
    <span class="c1"># b_grad = -2 * (x_tensor * error).mean()</span>
    
    <span class="c1"># We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. </span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Let&#39;s check the computed gradients...</span>
    <span class="c1"># Lets print the gradients for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> 
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="c1"># Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function)</span>
    <span class="c1"># We are telling Pytorch not to calculate gradients for steps within the &quot;with&quot;</span>
    <span class="c1">#with torch.no_grad():</span>
    <span class="c1">#    a-= lr * a.grad</span>
    <span class="c1">#   b.sub_(lr * b.grad)</span>

    <span class="c1"># We will use optimizer to update the parameters. </span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="c1"># Lets print a and b after the backward pass for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b after backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># We reset it to zero after each epoch</span>
    <span class="c1">#a.grad.zero_()</span>
    <span class="c1">#b.grad.zero_()</span>

    <span class="c1"># We will use optimizer to reset the gradients to zero. </span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Lets print a and b after the backward pass and after reset to zero for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b after backward pass after resetting grad to zero&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch number = 0
a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-3.3881], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.9439], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 100
a and b before backward pass tensor([1.1479], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7257], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0188], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0367], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 200
a and b before backward pass tensor([1.0507], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9159], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0041], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0080], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 300
a and b before backward pass tensor([1.0295], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9574], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0009], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0018], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 400
a and b before backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9664], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0002], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0004], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 500
a and b before backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([4.2574e-05], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-8.3295e-05], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 600
a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([9.3249e-06], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.8163e-05], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 700
a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([1.9097e-06], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-4.1103e-06], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 800
a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([5.1083e-07], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-8.8313e-07], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 900
a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([5.4762e-07], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-5.7358e-07], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward pass after resetting grad to zero tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The output looks good and we used Pytorch's optim to update parameters and also reset the gradient after each epoch. Good going..!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="What-else-is-our-handwritten-code-?-Prediction-expression,-Error-and-Loss-calculations.">What else is our handwritten code ? Prediction expression, Error and Loss calculations.<a class="anchor-link" href="#What-else-is-our-handwritten-code-?-Prediction-expression,-Error-and-Loss-calculations."> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets work on Loss and Error first.</p>
<p>Pytorch has many Loss functions and we have to use the appropriate ones based on the need.</p>
<p>In this example, we are using MSE and we can use nn.MSELoss class for creating the loss function.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Define a SGD optimizer to update the parameters</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="c1"># reduction = &#39;mean&#39; basically says you aggregate the losses by mean. </span>
<span class="c1"># reduction can also be sum in which case it is SSE. Sum Squared Error.</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">x_train_tensor</span>
    <span class="c1"># error = y_train_tensor - yhat</span>
    <span class="c1">#loss = (error ** 2).mean()</span>
    <span class="c1"># We commented the loss calculation and error calculation since </span>
    <span class="c1"># both will be taken care of by loss_func defined above. </span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">y_train_tensor</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
    
    <span class="c1"># Lets print a and b before the backward pass for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch number =&quot;</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b before backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># We will not do this now like we did in Numpy notebook </span>
    <span class="c1"># - https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb</span>
    <span class="c1"># a_grad = -2 * error.mean()</span>
    <span class="c1"># b_grad = -2 * (x_tensor * error).mean()</span>
    
    <span class="c1"># We will tell PyTorch to do the back progagation to the beginning from the specified loss which is at the end. </span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Let&#39;s check the computed gradients...</span>
    <span class="c1"># Lets print the gradients for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> 
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b&#39;s gradient = &quot;</span> <span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    
    <span class="c1"># Let&#39;s try to update the parameters with the formula we know - parameters = parameters - learning rate* gradient(parameters wrto Loss function)</span>
    <span class="c1"># We are telling Pytorch not to calculate gradients for steps within the &quot;with&quot;</span>
    <span class="c1">#with torch.no_grad():</span>
    <span class="c1">#    a-= lr * a.grad</span>
    <span class="c1">#   b.sub_(lr * b.grad)</span>

    <span class="c1"># We will use optimizer to update the parameters. </span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="c1"># Lets print a and b after the backward pass for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b after backward pass&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># We reset it to zero after each epoch</span>
    <span class="c1">#a.grad.zero_()</span>
    <span class="c1">#b.grad.zero_()</span>

    <span class="c1"># We will use optimizer to reset the gradients to zero. </span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Lets print a and b after the backward pass and after reset to zero for certain epochs only.</span>
    <span class="k">if</span><span class="p">(</span><span class="n">epoch</span><span class="o">%</span><span class="k">100</span>==0):
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;a and b after backward and zeroing grad&quot;</span> <span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch number = 0
a and b before backward pass tensor([0.1940], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.1391], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([-3.3881], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.9439], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([0.5328], device=&#39;cuda:0&#39;, requires_grad=True) tensor([0.3335], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 100
a and b before backward pass tensor([1.1479], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7257], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0188], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0367], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([1.1460], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.7294], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 200
a and b before backward pass tensor([1.0507], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9159], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0041], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0080], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([1.0503], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9167], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 300
a and b before backward pass tensor([1.0295], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9574], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0009], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0018], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([1.0294], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9576], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 400
a and b before backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9664], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([0.0002], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-0.0004], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([1.0248], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9665], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 500
a and b before backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([4.2574e-05], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-8.3295e-05], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([1.0238], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9684], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 600
a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([9.3249e-06], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-1.8163e-05], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9688], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 700
a and b before backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([1.9097e-06], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-4.1103e-06], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([1.0236], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9689], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 800
a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([5.1083e-07], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-8.8313e-07], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
epoch number = 900
a and b before backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a&#39;s gradient =  tensor([5.4762e-07], device=&#39;cuda:0&#39;)
b&#39;s gradient =  tensor([-5.7358e-07], device=&#39;cuda:0&#39;)
a and b after backward pass tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
a and b after backward and zeroing grad tensor([1.0235], device=&#39;cuda:0&#39;, requires_grad=True) tensor([1.9690], device=&#39;cuda:0&#39;, requires_grad=True)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Alright..-Loss-calculated-using-Pytorch.-What-next-?">Alright.. Loss calculated using Pytorch. What next ?<a class="anchor-link" href="#Alright..-Loss-calculated-using-Pytorch.-What-next-?"> </a></h1><h1 id="Lets-tackle-the-remaining-manually-written-expression.">Lets tackle the remaining manually written expression.<a class="anchor-link" href="#Lets-tackle-the-remaining-manually-written-expression."> </a></h1><h1 id="The-Prediction-logic-or-the-Model"><strong><em>The Prediction logic or the Model</em></strong><a class="anchor-link" href="#The-Prediction-logic-or-the-Model"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the model, we need to use the Module Class of Pytorch. We should create a new class which inherits from the Module class.</p>
<p>It needs to have two mandatory methods apart from any other methods as required for the problem at hand -<br />
<strong>init</strong>(self) - As the name says this is for the initialization. Setting up of the parameters. In this case of LR, we have a and b</p>
<p>forward(self, x) - The method which does the actual prediction logic.</p>
<p>Pytorch Module class is defined such that the forward method works as the core method of the class.</p>
<p>When the class is called like a function, the forward method is invoked.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">MyLRModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># To make &quot;a&quot; and &quot;b&quot; real parameters of the model, we need to wrap them in nn.Parameter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Computes the predictions and in this case it is a + bx</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#Keeping the seed same as before. This has to be done before defining the model. </span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Now we can create a model and send it to the device. Model should be on the same device as the parameters.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLRModel</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<span class="c1"># To print the values of a and b , we use state_dict() method.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-1</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Define a SGD optimizer to update the parameters. Note that parameters are now passed via model.parameters() and not manually.</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="c1"># reduction = &#39;mean&#39; basically says you aggregate the losses by mean. </span>
<span class="c1"># reduction can also be sum in which case it is SSE. Sum Squared Error.</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="c1"># This is to tell Pytorch that model should be in training mode at this time and not evaluation mode. </span>
    <span class="c1"># The model behavior might change in different modes. </span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> 

    <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">y_train_tensor</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>

    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># To print the values of a and b after all epochs, we use state_dict() method.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>OrderedDict([(&#39;a&#39;, tensor([0.3367], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([0.1288], device=&#39;cuda:0&#39;))])
OrderedDict([(&#39;a&#39;, tensor([1.0235], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([1.9690], device=&#39;cuda:0&#39;))])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Output same as before. We have 5 statements in the loop.</p>

<pre><code>yhat = model(x_train_tensor)

loss = loss_func(y_train_tensor, yhat)

loss.backward()

optimizer.step()

optimizer.zero_grad()

</code></pre>
<p>Can we refactor these ?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets refactor the training steps. Here, we just put all the code that kept repeating in the epoch loop in a method</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">make_train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="c1"># Builds function that performs a step in the train loop</span>
    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># Sets model to training mode</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="c1"># Makes predictions</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Computes loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
        <span class="c1"># Computes gradients</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># Updates parameters and zeroes gradients</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># Returns the loss</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    
    <span class="c1"># Returns the function that will be called inside the train loop</span>
    <span class="k">return</span> <span class="n">train_step</span>


<span class="c1"># Creates the train_step function for our model, loss function and optimizer</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">make_train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># For each epoch...</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="c1"># Performs one train step and returns the corresponding loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">,</span> <span class="n">y_train_tensor</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
<span class="c1"># Checks model&#39;s parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>OrderedDict([(&#39;a&#39;, tensor([1.0235], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([1.9690], device=&#39;cuda:0&#39;))])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In real life, data will not be so simple.</p>
<p>Therefore we pass data as (features, labels) to Pytorch</p>
<p>features are inputs and labels are outputs.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this we use the Dataset class of Pytorch. Below we see a custom Dataset class that we defined by inheriting the Pytorch's Dataset class as well as TensorDataset class of Pytorch</p>
<p>Any dataset returns three things, (x,y) , a way to index through x,y and length(x)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">TensorDataset</span>

<span class="k">class</span> <span class="nc">CustomDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_tensor</span><span class="p">,</span> <span class="n">y_tensor</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x_tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y_tensor</span>
        
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>


<span class="n">x_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">y_train_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">CustomDataset</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">,</span> <span class="n">y_train_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">type</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(tensor([0.7713]), tensor([2.4745]))
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>__main__.CustomDataset</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">,</span> <span class="n">y_train_tensor</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">type</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(tensor([0.7713]), tensor([2.4745]))
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.utils.data.dataset.TensorDataset</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The outputs of our Custom Dataset class and TensorDataset class are the same. 
This is because we are doing exactly what the TensorDataset will do although our class will not have other bells and whistles which TensorDataset class will have.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Okay.. What next ?</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">)</span> <span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_train_tensor</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(80, torch.Tensor, 80, torch.utils.data.dataset.TensorDataset)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">x_val</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(20, numpy.ndarray)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_valid_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">y_valid_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_val</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">valid_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_valid_tensor</span><span class="p">,</span> <span class="n">y_valid_tensor</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">x_valid_tensor</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_valid_tensor</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_data</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">valid_data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(20, torch.Tensor, 20, torch.utils.data.dataset.TensorDataset)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the above cells, we created Train and Valid Datasets using Pytorch TensorDataset class</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Also note that these Datasets are on cpu and not the GPU. We will get to know the reason for this once we understand Dataloaders</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Before that lets try to push the dataset to GPU..</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AttributeError</span>                            Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-82-9b29ccbff29d&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>train_data<span class="ansi-blue-fg">.</span>to<span class="ansi-blue-fg">(</span>device<span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">AttributeError</span>: &#39;TensorDataset&#39; object has no attribute &#39;to&#39;</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Ahh... It fails..</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lets see what these Dataloaders are. Generally data is sent to GPU and to the training loop in mini batches. Dataloader is a class which can store the dataset and allows to fetch mini batches of data via iteration.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Below gives a mini batch of data from train_data dataset.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[tensor([[0.9507],
         [0.5427],
         [0.1409],
         [0.3745],
         [0.1987],
         [0.8948],
         [0.7722],
         [0.7852],
         [0.5248],
         [0.2809],
         [0.1159],
         [0.0740],
         [0.1849],
         [0.4561],
         [0.7608],
         [0.1560]]), tensor([[2.8715],
         [2.2161],
         [1.1211],
         [1.7578],
         [1.2654],
         [2.7393],
         [2.4208],
         [2.5283],
         [2.0167],
         [1.5846],
         [1.1603],
         [1.1713],
         [1.5888],
         [1.7706],
         [2.4970],
         [1.2901]])]</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="o">.</span><span class="n">batch_sampler</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>16</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above cell show the size of one iteration of the Dataloader. It is 16 i.e the size we specified.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_loader</span><span class="o">.</span><span class="n">batch_size</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>16</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([0.7713]), tensor([2.4745]))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([0.7713]), tensor([2.4745]))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above cells show the value of the index 0 in dataset from within the Loader as well as direclty from the dataset</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">make_train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="c1"># The dataset, dataloader and its mini-batches all resides in the CPU for now.</span>
        <span class="c1"># Lets send those mini-batches to the GPU in below steps</span>
        <span class="c1"># Model is already in GPU</span>
        <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>OrderedDict([(&#39;a&#39;, tensor([1.0242], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([1.9682], device=&#39;cuda:0&#39;))])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Well, we added an extra loop and it took slightly higher time to finish than our previous trials.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, this is required in real life, because the data will be huge and cannot be loaded into GPU at once.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you remember, we split the data 80:20 as train and valid in the beginning of this notebook.</p>
<p>However, we generally use Pytorch's feature to split the data into train and valid (random split) or we have data split as training and validation data using some functional rule beforehand.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have trained the model, but we need to validate it.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For that we use the validation dataloader-&gt; dataset-&gt; data</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">valid_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">valid_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">make_train_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="ow">in</span> <span class="n">valid_loader</span><span class="p">:</span>
            <span class="n">x_val</span> <span class="o">=</span> <span class="n">x_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">y_val</span> <span class="o">=</span> <span class="n">y_val</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            
            <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

            <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_val</span><span class="p">)</span>
            <span class="n">val_loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
            <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>OrderedDict([(&#39;a&#39;, tensor([1.0290], device=&#39;cuda:0&#39;)), (&#39;b&#39;, tensor([1.9720], device=&#39;cuda:0&#39;))])
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">val_losses</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2000</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li><p>How did this number come ?
valid data length (see first few lines of notebook) which is 20 / 16 batch size = 2 (approx)</p>
<p>2 * number of epochs(1000) = 2000</p>
</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">val_losses</span><span class="p">[</span><span class="mi">1990</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[0.00672027375549078,
 0.017072057351469994,
 0.010100197046995163,
 0.004003560170531273,
 0.008860822767019272,
 0.007289723493158817,
 0.007129967212677002,
 0.015293894335627556,
 0.0099174864590168,
 0.00720137357711792]</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>5000</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>How did this number come ?
train data length (see first few lines of notebook) which is 80 / 16 batch size = 5</li>
</ol>
<p>5 * number of epochs(1000) = 5000</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">losses</span><span class="p">[</span><span class="mi">4990</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[0.01098373532295227,
 0.007446237839758396,
 0.004498552531003952,
 0.011742398142814636,
 0.006587368436157703,
 0.006659486331045628,
 0.008902094326913357,
 0.006174780428409576,
 0.010141816921532154,
 0.008895618841052055]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can see the final losses of train and valid data, training loss is slightly higher than validation loss.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>OrderedDict([('a', tensor([1.0290], device='cuda:0')), ('b', tensor([1.9720], device='cuda:0'))])</p>
<h1 id="This-is-the-final-prediction-of-the-model-which-is-pretty-accurate,-slightly-more-accurate-than-our-numpy-model----https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb">This is the final prediction of the model which is pretty accurate, slightly more accurate than our numpy model -  <a href="https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb">https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb</a><a class="anchor-link" href="#This-is-the-final-prediction-of-the-model-which-is-pretty-accurate,-slightly-more-accurate-than-our-numpy-model----https://github.com/manjunathrgithub/Simple-LR-Model-with-Numpy/blob/master/LR_Using_Numpy.ipynb"> </a></h1>
</div>
</div>
</div>
&lt;/div&gt;
 

</SubBackward0></SubBackward0></p></div></div></div></div>


  </div><a class="u-url" href="/blog/2019/12/31/Linear-Regression-Using-Pytorch.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An ML/DL/AI enthusiast&#39;s thoughts and understandings.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/mrg-ai" title="mrg-ai"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ParamsAndActivs" title="ParamsAndActivs"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
