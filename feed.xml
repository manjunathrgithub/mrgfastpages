<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://mrg-ai.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mrg-ai.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-09-22T05:59:12-05:00</updated><id>https://mrg-ai.github.io/blog/feed.xml</id><title type="html">ParamsAndActivs</title><subtitle>An ML/DL/AI enthusiast's thoughts and understandings.</subtitle><entry><title type="html">Machinelearning Endtoend Flow Summary</title><link href="https://mrg-ai.github.io/blog/2020/09/07/MachineLearning-EndtoEnd-Flow-Summary.html" rel="alternate" type="text/html" title="Machinelearning Endtoend Flow Summary" /><published>2020-09-07T00:00:00-05:00</published><updated>2020-09-07T00:00:00-05:00</updated><id>https://mrg-ai.github.io/blog/2020/09/07/MachineLearning-EndtoEnd-Flow-Summary</id><content type="html" xml:base="https://mrg-ai.github.io/blog/2020/09/07/MachineLearning-EndtoEnd-Flow-Summary.html">&lt;p&gt;The blog assumes the reader knows some basic Machine Learning terms.&lt;/p&gt;

&lt;p&gt;This Summary post should be read along with the below blog post&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mrg-ai.github.io/blog/2020/09/20/ML_EndToEnd_usingCAHousingDataset.html&quot;&gt;https://mrg-ai.github.io/blog/2020/09/20/ML_EndToEnd_usingCAHousingDataset.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;get-the-data&quot;&gt;Get the data&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This could be via files or from a database. Try to get the data into a pandas dataframe.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;That is the best format to do further data exploration.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use head(), info(), describe() methods to look at the data and column/column type information&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using matplotlib visualize the data via histograms for numerical columns&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For Categorical columns, find the unique values using value_counts()&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;split-the-data-into-training-and-test&quot;&gt;Split the data into Training and Test&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Splitting the data can be random using a random seed using sklearns test_train_split class&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Get the labels into a different dataframe for using during prediction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, in reality we should split using some unique column in the data (if available)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We can create such column if that is possible by using some combination of existing columns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sometimes, we may also have to do a stratified data split i.e. data should be taken from the different “strata” of the data. For example – Low Income Grp, High Income Grp etc.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;StratifiedShuffleSplit class can be used along with some column which indicates the “strata”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;visualize-and-data-cleanup-of-the-training-data&quot;&gt;Visualize and Data Cleanup of the training data&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Use matplotlib and look at the training data in more detail.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create new attributes (new dataframe columns) from existing columns if possible&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Date columns can definitely be used to create new Day, Month, Quarter, Year etc (DatePart function)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Clean the data for NULLs, Blanks etc through any of the below methods. Each method has its own implications and should be considered appropriately.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Drop such rows&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Use sklearn SimpleImputer to impute such NULLs with Median or Mean values&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;NOTE –&lt;/strong&gt; SimpleImputer works only on Numerical columns and CategoricalEncoders work on Categorical columns. We generally create different dataframes for Numericals and Categoricals. There is another Class which can handle both together and we will look at it in next section.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Handle the Categorical values i.e. convert them to Numbers&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Ordinal Encoder Class for Ordinal (they have a relationship between them) Categories like &lt;em&gt;Low, Medium, High&lt;/em&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;One Hot Encoder Class for categories which are Nominal (unrelated to one another) like List of State Names (CA, AZ, NY etc.)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;We can also create Custom Transformers to do some custom transformations. An example below. Using BaseEstimator and TransformerMixin classes we can get many existing sklearn methods in the Custom Transformer.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;from sklearn.base import BaseEstimator&lt;strong&gt;,&lt;/strong&gt; TransformerMixin&lt;/p&gt;

  &lt;p&gt;rooms_ix&lt;strong&gt;,&lt;/strong&gt; bedrooms_ix&lt;strong&gt;,&lt;/strong&gt; population_ix&lt;strong&gt;,&lt;/strong&gt; households_ix &lt;strong&gt;=&lt;/strong&gt; 3&lt;strong&gt;,&lt;/strong&gt; 4&lt;strong&gt;,&lt;/strong&gt; 5&lt;strong&gt;,&lt;/strong&gt; 6&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;class&lt;/strong&gt; &lt;strong&gt;CombinedAttributesAdder(&lt;/strong&gt;BaseEstimator&lt;strong&gt;,&lt;/strong&gt; TransformerMixin&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;def&lt;/strong&gt; __init__&lt;strong&gt;(&lt;/strong&gt;&lt;em&gt;self&lt;/em&gt;&lt;strong&gt;,&lt;/strong&gt; add_bedrooms_per_room&lt;strong&gt;=True):&lt;/strong&gt; &lt;em&gt;# no *args or **kargs&lt;/em&gt;&lt;/p&gt;

  &lt;p&gt;&lt;em&gt;self&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;add_bedrooms_per_room &lt;strong&gt;=&lt;/strong&gt; add_bedrooms_per_room&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;def&lt;/strong&gt; &lt;strong&gt;fit(&lt;/strong&gt;&lt;em&gt;self&lt;/em&gt;&lt;strong&gt;,&lt;/strong&gt; X&lt;strong&gt;,&lt;/strong&gt; y&lt;strong&gt;=None):&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;return&lt;/strong&gt; &lt;em&gt;self&lt;/em&gt; &lt;em&gt;# nothing else to do&lt;/em&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;def&lt;/strong&gt; &lt;strong&gt;transform(&lt;/strong&gt;&lt;em&gt;self&lt;/em&gt;&lt;strong&gt;,&lt;/strong&gt; X&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;rooms_per_household &lt;strong&gt;=&lt;/strong&gt; X&lt;strong&gt;[:,&lt;/strong&gt; rooms_ix&lt;strong&gt;]&lt;/strong&gt; &lt;strong&gt;/&lt;/strong&gt; X&lt;strong&gt;[:,&lt;/strong&gt; households_ix&lt;strong&gt;]&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;population_per_household &lt;strong&gt;=&lt;/strong&gt; X&lt;strong&gt;[:,&lt;/strong&gt; population_ix&lt;strong&gt;]&lt;/strong&gt; &lt;strong&gt;/&lt;/strong&gt; X&lt;strong&gt;[:,&lt;/strong&gt; households_ix&lt;strong&gt;]&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;if&lt;/strong&gt; &lt;em&gt;self&lt;/em&gt;&lt;strong&gt;.&lt;/strong&gt;add_bedrooms_per_room&lt;strong&gt;:&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;bedrooms_per_room &lt;strong&gt;=&lt;/strong&gt; X&lt;strong&gt;[:,&lt;/strong&gt; bedrooms_ix&lt;strong&gt;]&lt;/strong&gt; &lt;strong&gt;/&lt;/strong&gt; X&lt;strong&gt;[:,&lt;/strong&gt; rooms_ix&lt;strong&gt;]&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;return&lt;/strong&gt; np&lt;strong&gt;.&lt;/strong&gt;c_&lt;strong&gt;[&lt;/strong&gt;X&lt;strong&gt;,&lt;/strong&gt; rooms_per_household&lt;strong&gt;,&lt;/strong&gt; population_per_household&lt;strong&gt;,&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;bedrooms_per_room&lt;strong&gt;]&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;else:&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;return&lt;/strong&gt; np&lt;strong&gt;.&lt;/strong&gt;c_&lt;strong&gt;[&lt;/strong&gt;X&lt;strong&gt;,&lt;/strong&gt; rooms_per_household&lt;strong&gt;,&lt;/strong&gt; population_per_household&lt;strong&gt;]&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;attr_adder &lt;strong&gt;=&lt;/strong&gt; CombinedAttributesAdder&lt;strong&gt;(&lt;/strong&gt;add_bedrooms_per_room&lt;strong&gt;=False)&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;housing_extra_attribs &lt;strong&gt;=&lt;/strong&gt; attr_adder&lt;strong&gt;.&lt;/strong&gt;transform&lt;strong&gt;(&lt;/strong&gt;housing&lt;strong&gt;.&lt;/strong&gt;values&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;feature-scaling&quot;&gt;Feature Scaling&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This is required because ML algorithms require the different numerical features to be on similar scales.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Therefore, it is always a good practice to Scale the Numerical features.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Min-Max Scaling (Normalization) can be achieved with MinMaxScaler Class. This will cause issues when the data has many outliers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Standardization can be achieved through StandardScaler Class. Less affected by outliers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;pipelines&quot;&gt;Pipelines&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Since there are multiple steps we do as part of Data preprocessing, we should create a pipeline to do these transformations one after the another.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The output of one becomes input to another and so on.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A pipeline for numerical attributes can look like below&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;num_pipeline &lt;strong&gt;=&lt;/strong&gt; Pipeline&lt;strong&gt;([&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;‘imputer’&lt;strong&gt;,&lt;/strong&gt; SimpleImputer&lt;strong&gt;(&lt;/strong&gt;strategy&lt;strong&gt;=&lt;/strong&gt;“median”&lt;strong&gt;)),&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;‘attribs_adder’&lt;strong&gt;,&lt;/strong&gt; CombinedAttributesAdder&lt;strong&gt;()),&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;‘std_scaler’&lt;strong&gt;,&lt;/strong&gt; StandardScaler&lt;strong&gt;()),&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;])&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We can get the different transformations done for numerical columns as below using the pipeline defined above&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Dataframe_Transformed = num_pipeline.fit_transform(Dataframe[numerical attribute list])&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We can create a Full Pipeline for all attributes at once also. This would be ideal instead of having separate pipelines for Numericals and Categoricals.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Below is such an example. We can use ColumnTransformer Class. Remainder keyword is to tell that any columns not covered in the num pipeline or cat pipeline should be passed through.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;from sklearn.compose import ColumnTransformer&lt;/p&gt;

  &lt;p&gt;num_attribs &lt;strong&gt;=&lt;/strong&gt; &lt;em&gt;list&lt;/em&gt;&lt;strong&gt;(&lt;/strong&gt;dataframe_num&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;cat_attribs &lt;strong&gt;=&lt;/strong&gt; &lt;em&gt;list&lt;/em&gt;&lt;strong&gt;(&lt;/strong&gt;dataframe_cat&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;num_attribs&lt;/p&gt;

  &lt;p&gt;[‘List of Numeric Attributes’]&lt;/p&gt;

  &lt;p&gt;cat_attribs&lt;/p&gt;

  &lt;p&gt;[‘List of Categorical Attributes’]&lt;/p&gt;

  &lt;p&gt;full_pipeline &lt;strong&gt;=&lt;/strong&gt; ColumnTransformer&lt;strong&gt;([&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;“num”&lt;strong&gt;,&lt;/strong&gt; num_pipeline&lt;strong&gt;,&lt;/strong&gt; num_attribs&lt;strong&gt;),&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;“cat”&lt;strong&gt;,&lt;/strong&gt; OneHotEncoder&lt;strong&gt;(),&lt;/strong&gt; cat_attribs&lt;strong&gt;),&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;],&lt;/strong&gt; remainder&lt;strong&gt;=&lt;/strong&gt;‘passthrough’&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dataframe_AllCols_Prepared = full_pipeline.fit_transform(Dataframe_AllCols)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The data is now ready to be trained.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;evaluate-different-models&quot;&gt;Evaluate Different Models&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Evaluate the different models&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Calculate the cross validations score using K Fold Cross Validation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pick the model which best suits the data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Save the picked model using joblib.dump(model, ‘&amp;lt;model_name&amp;gt;.pkl’) as a pkl file.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This can be later reloaded back using joblib.load()&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;finetune-the-model&quot;&gt;Finetune the Model&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Finetune the models hyperparameters to finetune the model&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Using GridSearchCV or RandomizedSearchCV classes, we can get the best hyperparameters for the model.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;deploy-the-model&quot;&gt;Deploy the Model&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The model can be deployed on Cloud or can be exposed through a REST API and the model’s predict function can be used to evaluate the output by giving the necessary inputs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Note that the test input or any new input has to be transformed using the same data preprocessing pipeline before the model can predict the output using that input.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;X_test_prepared &lt;strong&gt;=&lt;/strong&gt; full_pipeline&lt;strong&gt;.&lt;/strong&gt;transform&lt;strong&gt;(&lt;/strong&gt;X_test&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;final_predictions &lt;strong&gt;=&lt;/strong&gt; final_model&lt;strong&gt;.&lt;/strong&gt;predict&lt;strong&gt;(&lt;/strong&gt;X_test_prepared&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">The blog assumes the reader knows some basic Machine Learning terms.</summary></entry><entry><title type="html">Ml Intro</title><link href="https://mrg-ai.github.io/blog/2020/05/05/ML-Intro.html" rel="alternate" type="text/html" title="Ml Intro" /><published>2020-05-05T00:00:00-05:00</published><updated>2020-05-05T00:00:00-05:00</updated><id>https://mrg-ai.github.io/blog/2020/05/05/ML-Intro</id><content type="html" xml:base="https://mrg-ai.github.io/blog/2020/05/05/ML-Intro.html">&lt;p&gt;This post does not explain what ML is and the usual stuff. Rather it mentions some basic details about ML. This is based on Aurelion’s ML book.&lt;/p&gt;

&lt;h1 id=&quot;types-of-ml&quot;&gt;Types of ML&lt;/h1&gt;

&lt;p&gt;Based on how they are classified, below are some classifications&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Whether or not they are trained with human supervision&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Supervised&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Unsupervised&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Semi supervised&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Reinforcement Learning&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Whether or not they can learn incrementally on the fly&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Online&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;batch learning&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Whether they work by simply comparing new data points to known data points, or instead by detecting patterns in the training data and building a predictive model, much like scientists do&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;instance-based&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;model-based learning&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;common-supervised-learning-algorithms&quot;&gt;Common Supervised Learning Algorithms&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;k-Nearest Neighbors&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linear Regression&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Logistic Regression&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Support Vector Machines (SVMs)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Decision Trees and Random Forests&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Neural networks&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;common-unsupervised-learning-algorithms&quot;&gt;Common Unsupervised Learning Algorithms&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Clustering&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;K-Means&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;DBSCAN&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Hierarchical Cluster Analysis (HCA)&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anomaly detection and novelty detection&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;One-class SVM&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Isolation Forest&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Visualization and dimensionality reduction&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Principal Component Analysis (PCA)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Kernel PCA&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Locally Linear Embedding (LLE)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;t-Distributed Stochastic Neighbor Embedding (t-SNE)&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Association rule learning&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Apriori&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Eclat&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;most-semi-supervised-learning-algorithms-are-combinations-of-unsupervised-and-supervised-algorithms&quot;&gt;Most semi supervised learning algorithms are combinations of unsupervised and supervised algorithms&lt;/h2&gt;

&lt;p&gt;For example, &lt;em&gt;deep belief networks&lt;/em&gt; (DBNs) are based on unsupervised components called &lt;em&gt;restricted Boltzmann machines&lt;/em&gt; (RBMs) stacked on top of one another. RBMs are trained sequentially in an unsupervised manner, and then the whole system is fine-tuned using supervised learning techniques.&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning-algorithms&quot;&gt;Reinforcement Learning Algorithms&lt;/h2&gt;

&lt;p&gt;There are no common algorithms. These work on Reward and Penalties and the learning happens over time by running it on multiple real-life examples&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Algorithms which play Chess or Go are an example&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Programs used in Robots are another example&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;batch-and-online-learning-algorithms&quot;&gt;Batch and Online Learning algorithms&lt;/h2&gt;

&lt;p&gt;As the name says there are algorithms which have to be trained offline i.e. Batch Algorithms and algorithms which can learn on the fly i.e. Online algorithms&lt;/p&gt;

&lt;h2 id=&quot;instance-vs-model-algorithms&quot;&gt;Instance vs Model algorithms&lt;/h2&gt;

&lt;p&gt;Depending on whether the algorithm uses learned instances to predict for new inputs like say Classification Algorithms (k-NearestNeighbors for example) or uses a Model like say Regression Algorithms where you have a line/plane.&lt;/p&gt;

&lt;h1 id=&quot;challenges-in-ml&quot;&gt;Challenges in ML&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bad Data&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bad Model&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-to-do-with-the-bad-data-or-bad-model&quot;&gt;What to do with the Bad data or Bad model?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Feature Selection&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Feature Extraction&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Regularization&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hyperparameters&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-load&quot;&gt;Data Load&lt;/h2&gt;

&lt;p&gt;Below function will be useful to get data from online datasets.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;import&lt;/strong&gt; &lt;strong&gt;os&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;import&lt;/strong&gt; &lt;strong&gt;tarfile&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;import&lt;/strong&gt; &lt;strong&gt;urllib&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;DOWNLOAD_ROOT = “https://raw.githubusercontent.com/ageron/handson-ml2/master/”&lt;/p&gt;

  &lt;p&gt;HOUSING_PATH = os.path.join(“datasets”, “housing”)&lt;/p&gt;

  &lt;p&gt;HOUSING_URL = DOWNLOAD_ROOT + “datasets/housing/housing.tgz”&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;def&lt;/strong&gt; fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):&lt;/p&gt;

  &lt;p&gt;os.makedirs(housing_path, exist_ok=True)&lt;/p&gt;

  &lt;p&gt;tgz_path = os.path.join(housing_path, “housing.tgz”)&lt;/p&gt;

  &lt;p&gt;urllib.request.urlretrieve(housing_url, tgz_path)&lt;/p&gt;

  &lt;p&gt;housing_tgz = tarfile.open(tgz_path)&lt;/p&gt;

  &lt;p&gt;housing_tgz.extractall(path=housing_path)&lt;/p&gt;

  &lt;p&gt;housing_tgz.close()&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Look at the next few posts on ML End to End process for more information.&lt;/p&gt;</content><author><name></name></author><summary type="html">This post does not explain what ML is and the usual stuff. Rather it mentions some basic details about ML. This is based on Aurelion’s ML book.</summary></entry></feed>